{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Polars Comprehensive Quiz & Challenge Notebook\n",
    "\n",
    "## Welcome to the Ultimate Polars Assessment!\n",
    "\n",
    "This notebook is designed to **solidify your Polars learning** after completing the tutorial series. It covers **ALL major Polars features** and concepts from the course.\n",
    "\n",
    "### ðŸ“‹ What's Covered:\n",
    "- âœ… Basic DataFrames & Expressions\n",
    "- âœ… Lazy Evaluation & Optimization\n",
    "- âœ… All 7 Join Types\n",
    "- âœ… GroupBy & Aggregations\n",
    "- âœ… Window Functions\n",
    "- âœ… String & DateTime Operations\n",
    "- âœ… Pivoting & Reshaping\n",
    "- âœ… Time Series Analysis\n",
    "- âœ… Nested Data Structures\n",
    "- âœ… Data Quality & Validation\n",
    "- âœ… Performance Optimization\n",
    "- âœ… Real-World Scenarios\n",
    "\n",
    "### ðŸŽ“ Difficulty Levels:\n",
    "- ðŸŸ¢ **Beginner** - Foundation concepts\n",
    "- ðŸŸ¡ **Intermediate** - Practical applications\n",
    "- ðŸ”´ **Advanced** - Complex scenarios\n",
    "- ðŸŸ£ **Expert** - Production-ready challenges\n",
    "\n",
    "### ðŸ“ Instructions:\n",
    "1. Read each question carefully\n",
    "2. Try solving without looking at hints first\n",
    "3. Use hints if stuck (expand the hint cell)\n",
    "4. Test your solution with the provided assertions\n",
    "5. Compare your approach with the solution (available at the end)\n",
    "\n",
    "### ðŸš€ Let's Begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import polars.selectors as cs\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"\\nâœ… Setup complete! Let's start the quiz!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“š Section 1: Foundations - DataFrames & Expressions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¢ Q1: Basic DataFrame Creation & Selection\n",
    "\n",
    "**Task:** Create a DataFrame with the following data and perform operations:\n",
    "- Create a DataFrame with columns: `employee_id`, `name`, `department`, `salary`, `years_experience`\n",
    "- Select only employees in \"Engineering\" department\n",
    "- Add a new column `salary_per_year` calculated as salary divided by years_experience\n",
    "- Select columns: name, salary, salary_per_year\n",
    "\n",
    "**Data:**\n",
    "```\n",
    "employee_id: [101, 102, 103, 104, 105, 106]\n",
    "name: ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank']\n",
    "department: ['Engineering', 'Marketing', 'Engineering', 'Sales', 'Engineering', 'Marketing']\n",
    "salary: [95000, 65000, 105000, 70000, 88000, 72000]\n",
    "years_experience: [5, 3, 8, 4, 6, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use pl.DataFrame() to create the DataFrame\n",
    "# Hint 2: Use .filter() with pl.col('department') == 'Engineering'\n",
    "# Hint 3: Use .with_columns() to add the new column\n",
    "# Hint 4: Use .select() to choose specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "df_employees = # YOUR CODE\n",
    "\n",
    "result_q1 = # YOUR CODE\n",
    "\n",
    "result_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q1.shape == (3, 3), f\"Expected shape (3, 3), got {result_q1.shape}\"\n",
    "assert 'salary_per_year' in result_q1.columns, \"Missing salary_per_year column\"\n",
    "assert result_q1['name'].to_list() == ['Alice', 'Charlie', 'Eve'], \"Incorrect filtering\"\n",
    "print(\"âœ… Q1 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q2: Expression Chaining & Conditional Logic\n",
    "\n",
    "**Task:** Using the employees DataFrame from Q1:\n",
    "- Create a new column `performance_category` with the following logic:\n",
    "  - \"High Performer\" if salary_per_year > 15000\n",
    "  - \"Mid Performer\" if salary_per_year between 12000 and 15000\n",
    "  - \"Developing\" otherwise\n",
    "- Create another column `bonus` that is 10% of salary for High Performers, 5% for Mid Performers, 2% for Developing\n",
    "- Round the bonus to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use pl.when().then().when().then().otherwise() for conditional logic\n",
    "# Hint 2: Chain multiple .with_columns() or put multiple expressions in one\n",
    "# Hint 3: Use .round(2) to round values\n",
    "# Hint 4: You can reference the performance_category column in the bonus calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "result_q2 = # YOUR CODE\n",
    "\n",
    "result_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert 'performance_category' in result_q2.columns, \"Missing performance_category column\"\n",
    "assert 'bonus' in result_q2.columns, \"Missing bonus column\"\n",
    "assert result_q2.filter(pl.col('name') == 'Alice')['bonus'].item() == 9500.0, \"Incorrect bonus calculation\"\n",
    "print(\"âœ… Q2 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¢ Q3: Selectors for Type-Based Operations\n",
    "\n",
    "**Task:** Create a mixed-type DataFrame and use selectors:\n",
    "- Create a DataFrame with numeric, string, and boolean columns\n",
    "- Use selectors to:\n",
    "  - Multiply all numeric columns by 2\n",
    "  - Convert all string columns to uppercase\n",
    "  - Invert all boolean columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Import polars.selectors as cs\n",
    "# Hint 2: Use cs.numeric(), cs.string(), cs.boolean()\n",
    "# Hint 3: Use .str.to_uppercase() for strings\n",
    "# Hint 4: Use ~ (NOT operator) or .not_() to invert booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sample DataFrame\n",
    "df_mixed = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['apple', 'banana', 'cherry', 'date', 'elderberry'],\n",
    "    'price': [1.2, 0.5, 2.3, 3.4, 1.8],\n",
    "    'quantity': [10, 20, 15, 8, 12],\n",
    "    'category': ['fruit', 'fruit', 'fruit', 'fruit', 'fruit'],\n",
    "    'in_stock': [True, True, False, True, False],\n",
    "    'featured': [False, True, True, False, True]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q3 = # YOUR CODE\n",
    "\n",
    "result_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q3['id'].to_list() == [2, 4, 6, 8, 10], \"Numeric columns not doubled\"\n",
    "assert result_q3['name'].to_list()[0] == 'APPLE', \"String columns not uppercase\"\n",
    "assert result_q3['in_stock'].to_list() == [False, False, True, False, True], \"Boolean not inverted\"\n",
    "print(\"âœ… Q3 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”„ Section 2: Lazy Evaluation & Optimization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q4: Understanding Query Plans\n",
    "\n",
    "**Task:** \n",
    "- Create a LazyFrame from a DataFrame\n",
    "- Perform the following operations in lazy mode:\n",
    "  - Filter rows where price > 1.0\n",
    "  - Select only 'name', 'price', 'quantity' columns\n",
    "  - Add a column 'total_value' = price * quantity\n",
    "  - Sort by total_value descending\n",
    "- Examine the optimized query plan using .explain()\n",
    "- Collect the results\n",
    "\n",
    "**Question:** What optimizations did Polars apply? Look for projection pushdown and predicate pushdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .lazy() to convert DataFrame to LazyFrame\n",
    "# Hint 2: Chain operations: .filter().select().with_columns().sort()\n",
    "# Hint 3: Use .explain(optimized=True) to see the optimized plan\n",
    "# Hint 4: Use .collect() to execute and get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "lf_q4 = # YOUR CODE (LazyFrame)\n",
    "\n",
    "# Print the optimized plan\n",
    "print(\"Optimized Query Plan:\")\n",
    "print(lf_q4.explain(optimized=True))\n",
    "\n",
    "# Collect results\n",
    "result_q4 = # YOUR CODE\n",
    "\n",
    "result_q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q4.shape[1] == 4, \"Should have 4 columns\"\n",
    "assert 'total_value' in result_q4.columns, \"Missing total_value column\"\n",
    "assert result_q4['total_value'][0] > result_q4['total_value'][-1], \"Not sorted descending\"\n",
    "print(\"âœ… Q4 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”— Section 3: Joins - All 7 Types\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q5: Inner, Left, and Anti Joins\n",
    "\n",
    "**Task:** You have two DataFrames:\n",
    "- `customers`: customer_id, name, email\n",
    "- `orders`: order_id, customer_id, amount, order_date\n",
    "\n",
    "Perform the following:\n",
    "1. **Inner join**: Get all orders with customer information\n",
    "2. **Left join**: Get all customers with their orders (including customers with no orders)\n",
    "3. **Anti join**: Get customers who have NEVER placed an order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .join(other, on='key', how='inner/left/anti')\n",
    "# Hint 2: Inner join returns only matching rows from both\n",
    "# Hint 3: Left join keeps all left rows, fills nulls for non-matches\n",
    "# Hint 4: Anti join returns left rows WITHOUT matches in right (efficient filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "customers = pl.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'charlie@email.com', \n",
    "              'diana@email.com', 'eve@email.com', 'frank@email.com']\n",
    "})\n",
    "\n",
    "orders = pl.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105, 106, 107],\n",
    "    'customer_id': [1, 2, 1, 3, 2, 1, 4],\n",
    "    'amount': [150.0, 200.0, 75.0, 300.0, 120.0, 90.0, 250.0],\n",
    "    'order_date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', \n",
    "                   '2024-01-19', '2024-01-20', '2024-01-21']\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "inner_join_result = # YOUR CODE\n",
    "left_join_result = # YOUR CODE\n",
    "anti_join_result = # YOUR CODE\n",
    "\n",
    "print(\"Inner Join (Orders with Customer Info):\")\n",
    "print(inner_join_result)\n",
    "print(\"\\nLeft Join (All Customers with Orders):\")\n",
    "print(left_join_result)\n",
    "print(\"\\nAnti Join (Customers with NO Orders):\")\n",
    "print(anti_join_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert inner_join_result.shape[0] == 7, \"Inner join should have 7 rows\"\n",
    "assert left_join_result.shape[0] == 11, \"Left join should have 11 rows (customers * orders + unmatched)\"\n",
    "assert anti_join_result.shape[0] == 2, \"Anti join should have 2 customers (Eve and Frank)\"\n",
    "assert set(anti_join_result['name'].to_list()) == {'Eve', 'Frank'}, \"Wrong customers in anti join\"\n",
    "print(\"âœ… Q5 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q6: Semi Join and Cross Join\n",
    "\n",
    "**Task:** \n",
    "1. **Semi join**: Get all customers who HAVE placed at least one order (but don't duplicate customer rows)\n",
    "2. **Cross join**: Create all possible combinations of product categories and shipping methods for a catalog\n",
    "\n",
    "**Question:** What's the difference between semi join and inner join when there are multiple matches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Semi join returns unique rows from left that have matches in right\n",
    "# Hint 2: Cross join creates Cartesian product (all combinations)\n",
    "# Hint 3: Semi join is more efficient than inner join + unique when you only need left table data\n",
    "# Hint 4: Cross join doesn't need an 'on' parameter - it combines everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi join example\n",
    "semi_join_result = # YOUR CODE\n",
    "\n",
    "# Cross join example\n",
    "categories = pl.DataFrame({'category': ['Electronics', 'Clothing', 'Books']})\n",
    "shipping_methods = pl.DataFrame({'shipping': ['Standard', 'Express', 'Overnight']})\n",
    "\n",
    "cross_join_result = # YOUR CODE\n",
    "\n",
    "print(\"Semi Join (Customers who ordered):\")\n",
    "print(semi_join_result)\n",
    "print(\"\\nCross Join (All Category-Shipping Combinations):\")\n",
    "print(cross_join_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert semi_join_result.shape[0] == 4, \"Semi join should have 4 unique customers\"\n",
    "assert cross_join_result.shape[0] == 9, \"Cross join should have 9 combinations (3x3)\"\n",
    "print(\"âœ… Q6 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š Section 4: GroupBy & Aggregations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q7: Multiple Aggregations\n",
    "\n",
    "**Task:** Using the orders DataFrame, calculate per customer:\n",
    "- Total number of orders\n",
    "- Total amount spent\n",
    "- Average order amount\n",
    "- Minimum and maximum order amounts\n",
    "- First and last order dates\n",
    "\n",
    "Sort by total amount spent descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .group_by('customer_id').agg([...])\n",
    "# Hint 2: Aggregation functions: .len(), .sum(), .mean(), .min(), .max(), .first(), .last()\n",
    "# Hint 3: Use .alias() to name your aggregated columns\n",
    "# Hint 4: Chain .sort() after aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n",
    "result_q7 = # YOUR CODE\n",
    "\n",
    "result_q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q7.shape[1] == 7, \"Should have 7 columns (customer_id + 6 aggregations)\"\n",
    "assert result_q7['customer_id'][0] == 2, \"Should be sorted by total spent, customer 2 first\"\n",
    "print(\"âœ… Q7 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q8: Conditional Aggregations\n",
    "\n",
    "**Task:** Create a sales DataFrame and calculate:\n",
    "- Total revenue per region\n",
    "- Count of high-value orders (amount > 500) per region\n",
    "- Average of ONLY high-value orders per region\n",
    "- Percentage of orders that are high-value per region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .filter() inside aggregation for conditional aggregations\n",
    "# Hint 2: pl.col('amount').filter(pl.col('amount') > 500).len()\n",
    "# Hint 3: For percentage: (high_value_count / total_count) * 100\n",
    "# Hint 4: You can calculate multiple aggregations in a single .agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "sales = pl.DataFrame({\n",
    "    'order_id': range(1, 21),\n",
    "    'region': ['North', 'South', 'North', 'East', 'West', 'South', 'North', 'East',\n",
    "               'West', 'South', 'North', 'East', 'West', 'South', 'North', 'East',\n",
    "               'West', 'South', 'North', 'East'],\n",
    "    'amount': [350, 600, 450, 750, 420, 890, 320, 560, 680, 410,\n",
    "               590, 720, 380, 650, 490, 830, 520, 770, 440, 910]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q8 = # YOUR CODE\n",
    "\n",
    "result_q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert 'total_revenue' in result_q8.columns, \"Missing total_revenue column\"\n",
    "assert 'high_value_count' in result_q8.columns, \"Missing high_value_count column\"\n",
    "assert 'high_value_percentage' in result_q8.columns, \"Missing high_value_percentage column\"\n",
    "print(\"âœ… Q8 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸªŸ Section 5: Window Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q9: Ranking and Top-N per Group\n",
    "\n",
    "**Task:** Given a product sales DataFrame:\n",
    "- Rank products by revenue within each category (highest revenue = rank 1)\n",
    "- Calculate each product's percentage contribution to category total revenue\n",
    "- Add a cumulative sum of revenue within each category (ordered by revenue)\n",
    "- Filter to get only the top 2 products per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .over('category') to partition window functions\n",
    "# Hint 2: .rank(descending=True) for ranking\n",
    "# Hint 3: For percentage: (value / sum over partition) * 100\n",
    "# Hint 4: .cum_sum().over() for cumulative sum within groups\n",
    "# Hint 5: Filter where rank <= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "product_sales = pl.DataFrame({\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam',\n",
    "                'Shirt', 'Pants', 'Shoes', 'Hat', 'Jacket',\n",
    "                'Novel', 'Textbook', 'Comic', 'Magazine', 'Dictionary'],\n",
    "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Electronics',\n",
    "                 'Clothing', 'Clothing', 'Clothing', 'Clothing', 'Clothing',\n",
    "                 'Books', 'Books', 'Books', 'Books', 'Books'],\n",
    "    'revenue': [50000, 5000, 8000, 30000, 7000,\n",
    "                15000, 20000, 25000, 5000, 30000,\n",
    "                8000, 15000, 3000, 2000, 6000]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q9 = # YOUR CODE\n",
    "\n",
    "result_q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q9.shape[0] == 6, \"Should have 6 rows (top 2 per 3 categories)\"\n",
    "assert 'rank' in result_q9.columns, \"Missing rank column\"\n",
    "assert 'revenue_percentage' in result_q9.columns, \"Missing revenue_percentage column\"\n",
    "assert all(result_q9['rank'] <= 2), \"Should only have ranks 1 and 2\"\n",
    "print(\"âœ… Q9 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q10: Lag/Lead and Running Calculations\n",
    "\n",
    "**Task:** Given daily stock prices:\n",
    "- Calculate the previous day's close price (lag)\n",
    "- Calculate the next day's close price (lead)\n",
    "- Calculate daily return: (close - previous_close) / previous_close * 100\n",
    "- Calculate 7-day rolling average of close price\n",
    "- Calculate cumulative maximum price up to that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .shift(1) for lag (previous value), .shift(-1) for lead (next value)\n",
    "# Hint 2: Daily return formula: ((close - lag_close) / lag_close) * 100\n",
    "# Hint 3: Use .rolling_mean(window_size=7) for rolling average\n",
    "# Hint 4: Use .cum_max() for cumulative maximum\n",
    "# Hint 5: Be careful with null values from shift operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "np.random.seed(42)\n",
    "dates = pl.date_range(datetime(2024, 1, 1), datetime(2024, 1, 30), interval='1d', eager=True)\n",
    "stock_prices = pl.DataFrame({\n",
    "    'date': dates,\n",
    "    'close': 100 + np.cumsum(np.random.randn(30) * 2)\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q10 = # YOUR CODE\n",
    "\n",
    "result_q10.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert 'previous_close' in result_q10.columns, \"Missing previous_close column\"\n",
    "assert 'next_close' in result_q10.columns, \"Missing next_close column\"\n",
    "assert 'daily_return' in result_q10.columns, \"Missing daily_return column\"\n",
    "assert 'rolling_avg_7d' in result_q10.columns, \"Missing rolling_avg_7d column\"\n",
    "assert 'cum_max' in result_q10.columns, \"Missing cum_max column\"\n",
    "print(\"âœ… Q10 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ Section 6: String Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q11: String Cleaning and Extraction\n",
    "\n",
    "**Task:** Given a messy email list:\n",
    "- Extract the username (part before @)\n",
    "- Extract the domain (part after @)\n",
    "- Check if email is from a .com domain\n",
    "- Clean and standardize email format (lowercase, trim whitespace)\n",
    "- Extract the domain extension (.com, .org, .edu, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .str.split('@') to split email, then .list.get(0) and .list.get(1)\n",
    "# Hint 2: Use .str.to_lowercase() and .str.strip_chars() for cleaning\n",
    "# Hint 3: Use .str.ends_with('.com') to check domain\n",
    "# Hint 4: Use .str.extract(r'\\.(\\w+)$') to extract extension with regex\n",
    "# Hint 5: Chain multiple string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "emails_df = pl.DataFrame({\n",
    "    'email': [\n",
    "        '  Alice.Smith@COMPANY.com  ',\n",
    "        'bob_jones@university.EDU',\n",
    "        'charlie@startup.io',\n",
    "        '  diana-lee@nonprofit.ORG',\n",
    "        'eve.wilson@techcorp.COM  ',\n",
    "        'frank@research.ac.uk'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q11 = # YOUR CODE\n",
    "\n",
    "result_q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert 'username' in result_q11.columns, \"Missing username column\"\n",
    "assert 'domain' in result_q11.columns, \"Missing domain column\"\n",
    "assert 'is_com' in result_q11.columns, \"Missing is_com column\"\n",
    "assert 'extension' in result_q11.columns, \"Missing extension column\"\n",
    "assert result_q11['username'][0] == 'alice.smith', \"Incorrect username extraction or cleaning\"\n",
    "print(\"âœ… Q11 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q12: Advanced Regex and Pattern Matching\n",
    "\n",
    "**Task:** Given customer feedback text:\n",
    "- Extract phone numbers using regex (format: XXX-XXX-XXXX or (XXX) XXX-XXXX)\n",
    "- Check if feedback contains specific keywords: 'excellent', 'poor', 'average'\n",
    "- Calculate sentiment score: +1 for 'excellent', -1 for 'poor', 0 for 'average', 0 if none\n",
    "- Extract all price mentions (format: $XX.XX or $XXX.XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .str.extract() for single match, .str.extract_all() for multiple matches\n",
    "# Hint 2: Phone regex: r'\\(?\\d{3}\\)?[\\s-]?\\d{3}-\\d{4}'\n",
    "# Hint 3: Use .str.contains() for keyword detection (case_insensitive=True)\n",
    "# Hint 4: Use when/then/otherwise for sentiment score based on contains\n",
    "# Hint 5: Price regex: r'\\$\\d+\\.\\d{2}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "feedback_df = pl.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'feedback': [\n",
    "        'Excellent service! Call me at 555-123-4567. Worth the $99.99 price.',\n",
    "        'Average experience. Contact: (555) 987-6543. Product was $45.50.',\n",
    "        'Poor quality for $129.99. Please call 555-456-7890 for refund.',\n",
    "        'The $75.00 item was excellent! Reach me at (555) 111-2222.',\n",
    "        'Nothing special, just average. Price of $55.25 seems fair.'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q12 = # YOUR CODE\n",
    "\n",
    "result_q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert 'phone' in result_q12.columns, \"Missing phone column\"\n",
    "assert 'sentiment_score' in result_q12.columns, \"Missing sentiment_score column\"\n",
    "assert result_q12['sentiment_score'].to_list() == [1, 0, -1, 1, 0], \"Incorrect sentiment scores\"\n",
    "print(\"âœ… Q12 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“… Section 7: DateTime Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q13: DateTime Components and Filtering\n",
    "\n",
    "**Task:** Given a transaction log:\n",
    "- Parse the datetime strings to datetime type\n",
    "- Extract: year, month, day, weekday name, hour, quarter\n",
    "- Filter transactions that occurred:\n",
    "  - On weekends (Saturday or Sunday)\n",
    "  - In Q4 (October, November, December)\n",
    "  - After 6 PM (hour >= 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .str.to_datetime() to parse strings\n",
    "# Hint 2: Use .dt.year(), .dt.month(), .dt.day(), .dt.weekday(), .dt.hour(), .dt.quarter()\n",
    "# Hint 3: Weekday: 6=Sunday, 5=Saturday (use .is_in([5, 6]))\n",
    "# Hint 4: Q4 months: 10, 11, 12\n",
    "# Hint 5: Combine filters with & operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "transactions = pl.DataFrame({\n",
    "    'transaction_id': range(1, 11),\n",
    "    'timestamp': [\n",
    "        '2024-10-15 14:30:00', '2024-11-20 19:45:00', '2024-09-05 10:15:00',\n",
    "        '2024-12-25 20:00:00', '2024-10-07 18:30:00', '2024-11-03 21:15:00',\n",
    "        '2024-08-14 09:00:00', '2024-12-31 23:59:00', '2024-10-28 17:45:00',\n",
    "        '2024-11-10 19:30:00'\n",
    "    ],\n",
    "    'amount': [100, 250, 75, 500, 150, 300, 80, 450, 120, 200]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q13 = # YOUR CODE (add all requested columns and filters)\n",
    "\n",
    "result_q13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q13['timestamp'].dtype == pl.Datetime, \"timestamp should be Datetime type\"\n",
    "assert 'year' in result_q13.columns, \"Missing year column\"\n",
    "assert 'weekday' in result_q13.columns, \"Missing weekday column\"\n",
    "assert 'quarter' in result_q13.columns, \"Missing quarter column\"\n",
    "print(\"âœ… Q13 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q14: DateTime Arithmetic and Duration\n",
    "\n",
    "**Task:** Given subscription data:\n",
    "- Calculate subscription duration in days\n",
    "- Determine if subscription is still active (end_date is null or in future)\n",
    "- Calculate days until renewal (or days since expired)\n",
    "- Categorize subscriptions: 'New' (<30 days), 'Regular' (30-365 days), 'Long-term' (>365 days)\n",
    "- Find the anniversary date (1 year from start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use (end_date - start_date).dt.total_days() for duration\n",
    "# Hint 2: Use pl.col('end_date').is_null() or end_date > today for active check\n",
    "# Hint 3: Use pl.lit(datetime.now()) or pl.datetime('today') for current date\n",
    "# Hint 4: Use when/then/otherwise for categorization based on duration\n",
    "# Hint 5: Add pl.duration(years=1) to start_date for anniversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "subscriptions = pl.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4, 5, 6],\n",
    "    'start_date': [\n",
    "        datetime(2024, 1, 15),\n",
    "        datetime(2023, 6, 20),\n",
    "        datetime(2024, 10, 1),\n",
    "        datetime(2022, 3, 10),\n",
    "        datetime(2024, 9, 15),\n",
    "        datetime(2021, 12, 5)\n",
    "    ],\n",
    "    'end_date': [\n",
    "        datetime(2025, 1, 15),  # Active\n",
    "        datetime(2024, 6, 20),  # Expired\n",
    "        None,                    # Active (no end date)\n",
    "        datetime(2025, 3, 10),  # Active\n",
    "        datetime(2024, 10, 15), # Expired\n",
    "        None                     # Active (no end date)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "result_q14 = # YOUR CODE\n",
    "\n",
    "result_q14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert 'duration_days' in result_q14.columns, \"Missing duration_days column\"\n",
    "assert 'is_active' in result_q14.columns, \"Missing is_active column\"\n",
    "assert 'subscription_category' in result_q14.columns, \"Missing subscription_category column\"\n",
    "assert 'anniversary_date' in result_q14.columns, \"Missing anniversary_date column\"\n",
    "print(\"âœ… Q14 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”„ Section 8: Pivoting & Reshaping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q15: Pivot Tables\n",
    "\n",
    "**Task:** Given monthly sales data in long format:\n",
    "- Create a pivot table with months as rows and products as columns, values as sales\n",
    "- Then unpivot (melt) it back to long format\n",
    "- Calculate total sales per month and per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .pivot(index='month', columns='product', values='sales')\n",
    "# Hint 2: Use .unpivot(index='month') or .melt(id_vars='month') to go back to long format\n",
    "# Hint 3: For totals, use group_by and sum\n",
    "# Hint 4: You might want to use aggregate_function='sum' in pivot if there are duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "monthly_sales = pl.DataFrame({\n",
    "    'month': ['Jan', 'Jan', 'Jan', 'Feb', 'Feb', 'Feb', 'Mar', 'Mar', 'Mar'],\n",
    "    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],\n",
    "    'sales': [100, 150, 200, 120, 180, 210, 140, 160, 190]\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "pivoted_q15 = # YOUR CODE (pivot)\n",
    "print(\"Pivoted (Wide Format):\")\n",
    "print(pivoted_q15)\n",
    "\n",
    "unpivoted_q15 = # YOUR CODE (unpivot back to long)\n",
    "print(\"\\nUnpivoted (Long Format):\")\n",
    "print(unpivoted_q15)\n",
    "\n",
    "monthly_totals = # YOUR CODE (total per month)\n",
    "product_totals = # YOUR CODE (total per product)\n",
    "print(\"\\nMonthly Totals:\")\n",
    "print(monthly_totals)\n",
    "print(\"\\nProduct Totals:\")\n",
    "print(product_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert pivoted_q15.shape == (3, 4), \"Pivoted should be 3 months x 4 columns (month + 3 products)\"\n",
    "assert unpivoted_q15.shape[0] == 9, \"Unpivoted should have 9 rows\"\n",
    "assert monthly_totals.shape[0] == 3, \"Should have 3 months\"\n",
    "assert product_totals.shape[0] == 3, \"Should have 3 products\"\n",
    "print(\"âœ… Q15 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ• Section 9: Time Series & group_by_dynamic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q16: Dynamic Temporal Grouping\n",
    "\n",
    "**Task:** Given hourly sensor readings:\n",
    "- Aggregate to daily intervals (mean, min, max temperature)\n",
    "- Aggregate to weekly intervals (mean temperature)\n",
    "- Aggregate to 6-hour windows (mean temperature, count of readings)\n",
    "- Handle the irregular timestamps properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .group_by_dynamic('timestamp', every='1d') for daily aggregation\n",
    "# Hint 2: Change 'every' parameter: '1w' for weekly, '6h' for 6-hour windows\n",
    "# Hint 3: Use .agg() with multiple aggregations like in group_by\n",
    "# Hint 4: Make sure timestamp column is datetime type\n",
    "# Hint 5: Sort by timestamp before group_by_dynamic for best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data - hourly sensor readings\n",
    "np.random.seed(42)\n",
    "timestamps = pl.datetime_range(\n",
    "    datetime(2024, 1, 1, 0, 0),\n",
    "    datetime(2024, 1, 7, 23, 0),\n",
    "    interval='1h',\n",
    "    eager=True\n",
    ")\n",
    "sensor_data = pl.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'temperature': 20 + np.random.randn(len(timestamps)) * 5,\n",
    "    'humidity': 60 + np.random.randn(len(timestamps)) * 10\n",
    "})\n",
    "\n",
    "# Your solution here:\n",
    "daily_agg = # YOUR CODE\n",
    "weekly_agg = # YOUR CODE\n",
    "six_hour_agg = # YOUR CODE\n",
    "\n",
    "print(\"Daily Aggregation:\")\n",
    "print(daily_agg.head())\n",
    "print(\"\\nWeekly Aggregation:\")\n",
    "print(weekly_agg)\n",
    "print(\"\\n6-Hour Aggregation:\")\n",
    "print(six_hour_agg.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert daily_agg.shape[0] == 8, \"Should have 8 days (7 full + 1 partial)\"\n",
    "assert weekly_agg.shape[0] >= 1, \"Should have at least 1 week\"\n",
    "assert six_hour_agg.shape[0] >= 28, \"Should have many 6-hour windows\"\n",
    "print(\"âœ… Q16 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ Section 10: Nested Data Structures\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q17: Working with Lists and Structs\n",
    "\n",
    "**Task:** Given customer purchase history:\n",
    "- Parse the nested purchase data (lists of structs)\n",
    "- Explode the purchases to get one row per purchase\n",
    "- Extract product name and price from the struct\n",
    "- Calculate total spent per customer\n",
    "- Find the most expensive item purchased by each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .explode('purchases') to expand list column to rows\n",
    "# Hint 2: Use .struct.field('field_name') or direct access with ['field'] to extract from struct\n",
    "# Hint 3: After exploding, use group_by('customer_id') for aggregations\n",
    "# Hint 4: Use .unnest() to flatten struct columns into separate columns\n",
    "# Hint 5: For max item, use window function or group_by with max aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with nested structures\n",
    "customers_nested = pl.DataFrame({\n",
    "    'customer_id': [1, 2, 3],\n",
    "    'customer_name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'purchases': [\n",
    "        [{'product': 'Laptop', 'price': 1200}, {'product': 'Mouse', 'price': 25}],\n",
    "        [{'product': 'Phone', 'price': 800}, {'product': 'Case', 'price': 15}, {'product': 'Charger', 'price': 30}],\n",
    "        [{'product': 'Tablet', 'price': 500}]\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Original nested data:\")\n",
    "print(customers_nested)\n",
    "print(\"\\nSchema:\")\n",
    "print(customers_nested.schema)\n",
    "\n",
    "# Your solution here:\n",
    "exploded_q17 = # YOUR CODE (explode and unnest)\n",
    "print(\"\\nExploded and flattened:\")\n",
    "print(exploded_q17)\n",
    "\n",
    "total_spent = # YOUR CODE (total per customer)\n",
    "print(\"\\nTotal spent per customer:\")\n",
    "print(total_spent)\n",
    "\n",
    "most_expensive = # YOUR CODE (most expensive item per customer)\n",
    "print(\"\\nMost expensive item per customer:\")\n",
    "print(most_expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert exploded_q17.shape[0] == 6, \"Should have 6 rows after exploding\"\n",
    "assert 'product' in exploded_q17.columns and 'price' in exploded_q17.columns, \"Should have unnested struct fields\"\n",
    "assert total_spent.filter(pl.col('customer_id') == 1)['total_spent'].item() == 1225, \"Incorrect total for customer 1\"\n",
    "print(\"âœ… Q17 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§¹ Section 11: Data Quality & Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q18: Comprehensive Missing Data Handling\n",
    "\n",
    "**Task:** Given a dataset with various missing values:\n",
    "- Identify columns with missing values and count nulls per column\n",
    "- Fill missing prices with the median price\n",
    "- Fill missing categories with 'Unknown'\n",
    "- Forward fill missing dates (use previous valid date)\n",
    "- Drop rows where quantity is missing (critical field)\n",
    "- Create a data quality report showing null percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .null_count() to count nulls per column\n",
    "# Hint 2: Use .fill_null(strategy='forward') for forward fill\n",
    "# Hint 3: Use .fill_null(value) or .fill_null(pl.median('column')) for specific fills\n",
    "# Hint 4: Use .drop_nulls(subset=['column']) to drop rows with null in specific column\n",
    "# Hint 5: Calculate null percentage: (null_count / total_rows) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with missing values\n",
    "messy_data = pl.DataFrame({\n",
    "    'date': ['2024-01-01', '2024-01-02', None, '2024-01-04', None, '2024-01-06'],\n",
    "    'product': ['A', 'B', 'C', 'D', 'E', 'F'],\n",
    "    'category': ['Electronics', None, 'Electronics', 'Clothing', None, 'Books'],\n",
    "    'price': [100.0, None, 150.0, None, 80.0, 120.0],\n",
    "    'quantity': [5, 10, None, 8, 12, 6]\n",
    "})\n",
    "\n",
    "print(\"Original messy data:\")\n",
    "print(messy_data)\n",
    "\n",
    "# Your solution here:\n",
    "# First, create quality report\n",
    "quality_report = # YOUR CODE (show nulls and percentages per column)\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(quality_report)\n",
    "\n",
    "# Then clean the data\n",
    "cleaned_q18 = # YOUR CODE\n",
    "print(\"\\nCleaned data:\")\n",
    "print(cleaned_q18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert cleaned_q18.null_count().sum_horizontal()[0] == 0, \"Should have no nulls after cleaning\"\n",
    "assert cleaned_q18.shape[0] == 5, \"Should have 5 rows after dropping row with null quantity\"\n",
    "print(\"âœ… Q18 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ¡ Q19: Duplicate Detection and Handling\n",
    "\n",
    "**Task:** Given customer data with duplicates:\n",
    "- Identify duplicate rows (all columns)\n",
    "- Identify duplicates based on email only\n",
    "- Keep the first occurrence of each email\n",
    "- Mark duplicate emails with a flag before deduplication\n",
    "- Create a report of duplicate emails with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Use .is_duplicated() to mark duplicate rows\n",
    "# Hint 2: Use .unique(subset=['email'], keep='first') for deduplication\n",
    "# Hint 3: Use group_by('email').agg(pl.len()) to count duplicates\n",
    "# Hint 4: Use .is_duplicated() before deduplication to add a flag column\n",
    "# Hint 5: Filter to show only emails with count > 1 for the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with duplicates\n",
    "customers_dupes = pl.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'name': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'Diana', 'Alice', 'Eve'],\n",
    "    'email': ['alice@email.com', 'bob@email.com', 'alice@email.com', \n",
    "              'charlie@email.com', 'bob@email.com', 'diana@email.com',\n",
    "              'alice@email.com', 'eve@email.com'],\n",
    "    'signup_date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',\n",
    "                    '2024-01-05', '2024-01-06', '2024-01-07', '2024-01-08']\n",
    "})\n",
    "\n",
    "print(\"Original data with duplicates:\")\n",
    "print(customers_dupes)\n",
    "\n",
    "# Your solution here:\n",
    "duplicate_report = # YOUR CODE (report of duplicate emails with counts)\n",
    "print(\"\\nDuplicate Email Report:\")\n",
    "print(duplicate_report)\n",
    "\n",
    "marked_duplicates = # YOUR CODE (add is_duplicate flag)\n",
    "print(\"\\nData with duplicate flag:\")\n",
    "print(marked_duplicates)\n",
    "\n",
    "deduplicated_q19 = # YOUR CODE (keep first occurrence)\n",
    "print(\"\\nDeduplicated data (kept first):\")\n",
    "print(deduplicated_q19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert deduplicated_q19.shape[0] == 5, \"Should have 5 unique emails\"\n",
    "assert 'is_duplicate' in marked_duplicates.columns, \"Should have is_duplicate flag\"\n",
    "assert marked_duplicates['is_duplicate'].sum() == 3, \"Should have 3 duplicate rows marked\"\n",
    "print(\"âœ… Q19 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸš€ Section 12: Performance & Optimization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”´ Q20: Query Optimization Challenge\n",
    "\n",
    "**Task:** Given an inefficient query, optimize it:\n",
    "- Read a large dataset (simulated)\n",
    "- Apply multiple transformations\n",
    "- Use lazy evaluation to optimize the query\n",
    "- Compare the query plans (naive vs optimized)\n",
    "- Identify which optimizations were applied\n",
    "\n",
    "**Initial Query (inefficient):**\n",
    "```python\n",
    "result = (df\n",
    "    .filter(pl.col('amount') > 100)\n",
    "    .filter(pl.col('region') == 'North')\n",
    "    .with_columns([\n",
    "        (pl.col('amount') * 1.1).alias('amount_with_tax'),\n",
    "        (pl.col('amount') * 0.9).alias('amount_with_discount')\n",
    "    ])\n",
    "    .select(['region', 'amount_with_tax', 'amount_with_discount'])\n",
    ")\n",
    "```\n",
    "\n",
    "**Optimize this query using lazy evaluation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1: Convert to lazy with .lazy() at the start\n",
    "# Hint 2: Use .explain() to see what optimizations Polars applies\n",
    "# Hint 3: Projection pushdown will select columns early\n",
    "# Hint 4: Predicate pushdown will apply filters early\n",
    "# Hint 5: Common subexpression elimination may optimize repeated calculations\n",
    "# Hint 6: Combine filters when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample large dataset\n",
    "np.random.seed(42)\n",
    "large_df = pl.DataFrame({\n",
    "    'order_id': range(10000),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 10000),\n",
    "    'amount': np.random.uniform(50, 500, 10000),\n",
    "    'customer_id': np.random.randint(1, 1000, 10000),\n",
    "    'product_id': np.random.randint(1, 100, 10000),\n",
    "    'date': np.random.choice(pl.date_range(datetime(2024, 1, 1), datetime(2024, 12, 31), interval='1d', eager=True), 10000)\n",
    "})\n",
    "\n",
    "# Your optimized solution here:\n",
    "optimized_q20 = # YOUR CODE (use lazy evaluation)\n",
    "\n",
    "print(\"Optimized Query Plan:\")\n",
    "print(optimized_q20.explain(optimized=True))\n",
    "\n",
    "result_q20 = optimized_q20.collect()\n",
    "print(\"\\nResult:\")\n",
    "print(result_q20.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your solution\n",
    "assert result_q20.shape[1] == 3, \"Should have 3 columns after projection\"\n",
    "assert all(result_q20['region'] == 'North'), \"Should only have North region\"\n",
    "print(\"âœ… Q20 Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ Section 13: Real-World Integration Scenarios\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ£ Q21: E-Commerce Analytics Pipeline (MEGA CHALLENGE)\n",
    "\n",
    "**Scenario:** You're analyzing e-commerce data with multiple related tables.\n",
    "\n",
    "**Given:**\n",
    "- `customers`: customer_id, name, email, signup_date, region\n",
    "- `orders`: order_id, customer_id, order_date, status\n",
    "- `order_items`: order_id, product_id, quantity, price\n",
    "- `products`: product_id, product_name, category\n",
    "\n",
    "**Tasks:**\n",
    "1. **Customer Lifetime Value (CLV):**\n",
    "   - Calculate total revenue per customer\n",
    "   - Calculate number of orders per customer\n",
    "   - Calculate average order value per customer\n",
    "   - Calculate days since first order\n",
    "\n",
    "2. **Product Analysis:**\n",
    "   - Top 5 products by revenue\n",
    "   - Top 3 categories by number of orders\n",
    "   - Products that have never been ordered\n",
    "\n",
    "3. **Time-Based Analysis:**\n",
    "   - Monthly revenue trend\n",
    "   - Month-over-month growth rate\n",
    "   - Best performing month\n",
    "\n",
    "4. **Customer Segmentation:**\n",
    "   - Segment customers into: 'High Value' (CLV > $1000), 'Medium Value' ($500-$1000), 'Low Value' (< $500)\n",
    "   - Count customers per segment per region\n",
    "\n",
    "5. **Advanced Metrics:**\n",
    "   - Customer retention: customers who ordered in multiple months\n",
    "   - Average time between orders per customer\n",
    "   - Product bundling: most common product pairs in orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a complex challenge - break it down into steps!\n",
    "# Hint 1: Start by joining tables to create a comprehensive dataset\n",
    "# Hint 2: Use lazy evaluation for the entire pipeline\n",
    "# Hint 3: Use window functions for ranking and cumulative calculations\n",
    "# Hint 4: Use group_by_dynamic for time-based aggregations\n",
    "# Hint 5: Use when/then/otherwise for segmentation\n",
    "# Hint 6: For product pairs, you might need a cross join within each order\n",
    "# Hint 7: Break this into multiple smaller queries and combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for e-commerce scenario\n",
    "np.random.seed(42)\n",
    "\n",
    "# Customers\n",
    "customers_ecom = pl.DataFrame({\n",
    "    'customer_id': range(1, 51),\n",
    "    'name': [f'Customer_{i}' for i in range(1, 51)],\n",
    "    'email': [f'customer{i}@email.com' for i in range(1, 51)],\n",
    "    'signup_date': np.random.choice(\n",
    "        pl.date_range(datetime(2023, 1, 1), datetime(2024, 1, 1), interval='1d', eager=True), \n",
    "        50\n",
    "    ),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 50)\n",
    "})\n",
    "\n",
    "# Products\n",
    "products_ecom = pl.DataFrame({\n",
    "    'product_id': range(1, 21),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 21)],\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports'], 20)\n",
    "})\n",
    "\n",
    "# Orders (200 orders)\n",
    "orders_ecom = pl.DataFrame({\n",
    "    'order_id': range(1, 201),\n",
    "    'customer_id': np.random.randint(1, 51, 200),\n",
    "    'order_date': np.random.choice(\n",
    "        pl.date_range(datetime(2023, 1, 1), datetime(2024, 11, 1), interval='1d', eager=True),\n",
    "        200\n",
    "    ),\n",
    "    'status': np.random.choice(['completed', 'pending', 'cancelled'], 200, p=[0.8, 0.15, 0.05])\n",
    "})\n",
    "\n",
    "# Order Items (300 items across orders)\n",
    "order_items_ecom = pl.DataFrame({\n",
    "    'order_id': np.random.choice(range(1, 201), 300),\n",
    "    'product_id': np.random.randint(1, 21, 300),\n",
    "    'quantity': np.random.randint(1, 5, 300),\n",
    "    'price': np.random.uniform(10, 200, 300).round(2)\n",
    "})\n",
    "\n",
    "print(\"Sample data created!\")\n",
    "print(f\"Customers: {customers_ecom.shape[0]}\")\n",
    "print(f\"Products: {products_ecom.shape[0]}\")\n",
    "print(f\"Orders: {orders_ecom.shape[0]}\")\n",
    "print(f\"Order Items: {order_items_ecom.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE - Task 1: Customer Lifetime Value\n",
    "clv_analysis = # YOUR CODE\n",
    "\n",
    "print(\"Customer Lifetime Value Analysis:\")\n",
    "print(clv_analysis.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE - Task 2: Product Analysis\n",
    "top_products = # YOUR CODE (top 5 by revenue)\n",
    "top_categories = # YOUR CODE (top 3 by order count)\n",
    "never_ordered = # YOUR CODE (products never ordered)\n",
    "\n",
    "print(\"Top 5 Products by Revenue:\")\n",
    "print(top_products)\n",
    "print(\"\\nTop 3 Categories by Orders:\")\n",
    "print(top_categories)\n",
    "print(\"\\nProducts Never Ordered:\")\n",
    "print(never_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE - Task 3: Time-Based Analysis\n",
    "monthly_revenue = # YOUR CODE\n",
    "monthly_growth = # YOUR CODE (with month-over-month growth rate)\n",
    "\n",
    "print(\"Monthly Revenue Trend:\")\n",
    "print(monthly_revenue)\n",
    "print(\"\\nMonth-over-Month Growth:\")\n",
    "print(monthly_growth.tail(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE - Task 4: Customer Segmentation\n",
    "customer_segments = # YOUR CODE\n",
    "segment_by_region = # YOUR CODE\n",
    "\n",
    "print(\"Customer Segmentation:\")\n",
    "print(customer_segments.head(10))\n",
    "print(\"\\nSegments by Region:\")\n",
    "print(segment_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE - Task 5: Advanced Metrics\n",
    "retention_analysis = # YOUR CODE (customers with orders in multiple months)\n",
    "avg_time_between_orders = # YOUR CODE\n",
    "product_pairs = # YOUR CODE (most common product combinations)\n",
    "\n",
    "print(\"Customer Retention (multi-month orders):\")\n",
    "print(retention_analysis.head(10))\n",
    "print(\"\\nAverage Time Between Orders:\")\n",
    "print(avg_time_between_orders.head(10))\n",
    "print(\"\\nTop Product Pairs:\")\n",
    "print(product_pairs.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test for Q21\n",
    "assert 'total_revenue' in clv_analysis.columns, \"CLV analysis missing total_revenue\"\n",
    "assert top_products.shape[0] == 5, \"Should have top 5 products\"\n",
    "assert 'month' in monthly_revenue.columns, \"Monthly revenue should have month column\"\n",
    "assert 'segment' in customer_segments.columns, \"Customer segments should have segment column\"\n",
    "print(\"âœ… Q21 Passed! Congratulations on completing the MEGA CHALLENGE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ† Final Challenge: Build Your Own Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŸ£ Q22: Create Your Own Real-World Analysis\n",
    "\n",
    "**Task:** Choose a domain that interests you (finance, healthcare, sports, social media, etc.) and:\n",
    "\n",
    "1. **Generate or find sample data** (at least 3 related tables)\n",
    "2. **Formulate 5 business questions** you want to answer\n",
    "3. **Use Polars to answer them**, demonstrating:\n",
    "   - Multiple join types\n",
    "   - Complex aggregations\n",
    "   - Window functions\n",
    "   - Time-based analysis (if applicable)\n",
    "   - At least one advanced technique (nested data, group_by_dynamic, etc.)\n",
    "4. **Optimize your queries** using lazy evaluation\n",
    "5. **Document your findings** with markdown cells\n",
    "\n",
    "**This is your chance to show everything you've learned!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CUSTOM ANALYSIS HERE\n",
    "# Feel free to use as many cells as you need!\n",
    "\n",
    "# Example structure:\n",
    "# 1. Data Generation/Loading\n",
    "# 2. Data Exploration\n",
    "# 3. Analysis Question 1\n",
    "# 4. Analysis Question 2\n",
    "# 5. Analysis Question 3\n",
    "# 6. Analysis Question 4\n",
    "# 7. Analysis Question 5\n",
    "# 8. Summary and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ“ Conclusion & Next Steps\n",
    "---\n",
    "\n",
    "### ðŸŽ‰ Congratulations!\n",
    "\n",
    "If you've completed this quiz notebook, you have demonstrated proficiency in:\n",
    "\n",
    "âœ… **Core Polars Concepts:**\n",
    "- DataFrames and LazyFrames\n",
    "- Expression system and chaining\n",
    "- Selectors for type-based operations\n",
    "\n",
    "âœ… **Data Manipulation:**\n",
    "- All 7 join types and when to use them\n",
    "- GroupBy aggregations with complex logic\n",
    "- Window functions for advanced analytics\n",
    "- Pivoting and reshaping data\n",
    "\n",
    "âœ… **Specialized Operations:**\n",
    "- String manipulation and regex\n",
    "- DateTime operations and temporal analysis\n",
    "- Time series with group_by_dynamic\n",
    "- Nested data structures (lists and structs)\n",
    "\n",
    "âœ… **Data Quality:**\n",
    "- Missing value detection and handling\n",
    "- Duplicate identification and deduplication\n",
    "- Data validation patterns\n",
    "\n",
    "âœ… **Performance:**\n",
    "- Lazy evaluation and query optimization\n",
    "- Reading query plans\n",
    "- Efficient data processing patterns\n",
    "\n",
    "âœ… **Real-World Application:**\n",
    "- Complex multi-table analysis\n",
    "- Business metrics and KPIs\n",
    "- End-to-end data pipelines\n",
    "\n",
    "### ðŸ“š Continue Learning:\n",
    "\n",
    "1. **Practice with real datasets**: Kaggle, public APIs, your own data\n",
    "2. **Explore advanced topics**: Custom functions, SQL interface, cloud I/O\n",
    "3. **Integrate with other tools**: Pandas, DuckDB, visualization libraries\n",
    "4. **Build production pipelines**: Apply Polars to real-world problems\n",
    "5. **Join the community**: Polars Discord, GitHub discussions\n",
    "\n",
    "### ðŸŒŸ You're now ready to use Polars in production!\n",
    "\n",
    "**Keep practicing, keep learning, and happy data wrangling! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
