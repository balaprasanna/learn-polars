{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Missing Data and Duplicate Handling - Comprehensive Guide\n",
    "\n",
    "This notebook covers handling missing values (nulls) and duplicate data in Polars.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Comprehensive null/missing value handling strategies\n",
    "- Forward fill, backward fill, and interpolation\n",
    "- Null-safe operations and propagation\n",
    "- Identifying and analyzing duplicates\n",
    "- Removing duplicates with different strategies\n",
    "- Missing data patterns and visualization\n",
    "- Best practices for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Understanding Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with null values\n",
    "df_nulls = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'name': ['Alice', 'Bob', None, 'Diana', 'Eve', None, 'Frank', 'Grace'],\n",
    "    'age': [25, None, 35, None, 28, 42, None, 31],\n",
    "    'score': [85.5, 92.3, None, 88.9, None, 95.2, 78.1, None],\n",
    "    'date': [\n",
    "        date(2024, 1, 1), date(2024, 1, 2), None, \n",
    "        date(2024, 1, 4), date(2024, 1, 5), None,\n",
    "        date(2024, 1, 7), date(2024, 1, 8)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"DataFrame with null values:\")\n",
    "print(df_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"Null count per column:\")\n",
    "print(df_nulls.null_count())\n",
    "\n",
    "print(\"\\nNull percentage per column:\")\n",
    "null_pct = (df_nulls.null_count() / len(df_nulls) * 100)\n",
    "print(null_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with any null\n",
    "has_any_null = df_nulls.select(\n",
    "    pl.any_horizontal(pl.all().is_null()).alias('has_null')\n",
    ")\n",
    "\n",
    "df_with_null_flag = df_nulls.with_columns(has_any_null)\n",
    "print(\"Rows with any null value:\")\n",
    "print(df_with_null_flag.filter(pl.col('has_null')))\n",
    "print(f\"\\nTotal rows with nulls: {df_with_null_flag['has_null'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Filling Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Fill with Literal Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nulls with specific values\n",
    "df_filled_literal = df_nulls.with_columns([\n",
    "    pl.col('name').fill_null('Unknown').alias('name_filled'),\n",
    "    pl.col('age').fill_null(0).alias('age_filled'),\n",
    "    pl.col('score').fill_null(0.0).alias('score_filled')\n",
    "])\n",
    "\n",
    "print(\"Fill nulls with literals:\")\n",
    "print(df_filled_literal.select(['name', 'name_filled', 'age', 'age_filled', 'score', 'score_filled']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fill with Statistical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with mean, median, min, max\n",
    "df_filled_stats = df_nulls.with_columns([\n",
    "    pl.col('age').fill_null(pl.col('age').mean()).alias('age_mean'),\n",
    "    pl.col('age').fill_null(pl.col('age').median()).alias('age_median'),\n",
    "    pl.col('score').fill_null(pl.col('score').mean()).alias('score_mean')\n",
    "])\n",
    "\n",
    "print(\"Fill nulls with statistics:\")\n",
    "print(df_filled_stats.select([\n",
    "    'id', 'age', 'age_mean', 'age_median', 'score', 'score_mean'\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Forward Fill and Backward Fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill (propagate last valid value forward)\n",
    "df_forward_fill = df_nulls.with_columns([\n",
    "    pl.col('name').fill_null(strategy='forward').alias('name_ffill'),\n",
    "    pl.col('age').fill_null(strategy='forward').alias('age_ffill'),\n",
    "    pl.col('score').fill_null(strategy='forward').alias('score_ffill')\n",
    "])\n",
    "\n",
    "print(\"Forward fill:\")\n",
    "print(df_forward_fill.select(['id', 'name', 'name_ffill', 'age', 'age_ffill']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill (propagate next valid value backward)\n",
    "df_backward_fill = df_nulls.with_columns([\n",
    "    pl.col('name').fill_null(strategy='backward').alias('name_bfill'),\n",
    "    pl.col('age').fill_null(strategy='backward').alias('age_bfill'),\n",
    "    pl.col('score').fill_null(strategy='backward').alias('score_bfill')\n",
    "])\n",
    "\n",
    "print(\"Backward fill:\")\n",
    "print(df_backward_fill.select(['id', 'name', 'name_bfill', 'age', 'age_bfill']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpolation (for numeric/temporal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation for numeric columns\n",
    "df_interpolated = df_nulls.with_columns([\n",
    "    pl.col('age').interpolate().alias('age_interpolated'),\n",
    "    pl.col('score').interpolate().alias('score_interpolated')\n",
    "])\n",
    "\n",
    "print(\"Linear interpolation:\")\n",
    "print(df_interpolated.select(['id', 'age', 'age_interpolated', 'score', 'score_interpolated']))\n",
    "print(\"\\n💡 Interpolation estimates values between known points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Conditional Fill (Coalesce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce: Use first non-null value from multiple columns\n",
    "df_backup = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'primary_email': ['alice@example.com', None, 'charlie@example.com', None],\n",
    "    'secondary_email': [None, 'bob_backup@example.com', None, 'diana_backup@example.com'],\n",
    "    'default_email': ['default@example.com'] * 4\n",
    "})\n",
    "\n",
    "df_coalesced = df_backup.with_columns(\n",
    "    pl.coalesce(['primary_email', 'secondary_email', 'default_email']).alias('email')\n",
    ")\n",
    "\n",
    "print(\"Coalesce (first non-null):\")\n",
    "print(df_coalesced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Dropping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with ANY null value\n",
    "df_dropped_any = df_nulls.drop_nulls()\n",
    "\n",
    "print(\"Drop rows with any null:\")\n",
    "print(df_dropped_any)\n",
    "print(f\"\\nRows: {len(df_nulls)} -> {len(df_dropped_any)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null in specific columns\n",
    "df_dropped_subset = df_nulls.drop_nulls(subset=['name', 'age'])\n",
    "\n",
    "print(\"Drop rows with null in 'name' or 'age':\")\n",
    "print(df_dropped_subset)\n",
    "print(f\"\\nRows: {len(df_nulls)} -> {len(df_dropped_subset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Null-Safe Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Null Propagation in Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulls propagate through arithmetic operations\n",
    "df_arithmetic = pl.DataFrame({\n",
    "    'a': [1, 2, None, 4, 5],\n",
    "    'b': [10, None, 30, 40, None]\n",
    "})\n",
    "\n",
    "df_arithmetic_result = df_arithmetic.with_columns([\n",
    "    (pl.col('a') + pl.col('b')).alias('sum'),\n",
    "    (pl.col('a') * pl.col('b')).alias('product'),\n",
    "    (pl.col('a') / pl.col('b')).alias('division')\n",
    "])\n",
    "\n",
    "print(\"Null propagation in arithmetic:\")\n",
    "print(df_arithmetic_result)\n",
    "print(\"\\n💡 Any operation with null results in null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Nulls in Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations typically ignore nulls\n",
    "df_agg = pl.DataFrame({\n",
    "    'group': ['A', 'A', 'A', 'B', 'B', 'B'],\n",
    "    'value': [10, None, 30, 40, 50, None]\n",
    "})\n",
    "\n",
    "result = df_agg.group_by('group').agg([\n",
    "    pl.col('value').sum().alias('sum'),\n",
    "    pl.col('value').mean().alias('mean'),\n",
    "    pl.col('value').count().alias('count'),\n",
    "    pl.col('value').null_count().alias('null_count')\n",
    "])\n",
    "\n",
    "print(\"Aggregations ignore nulls:\")\n",
    "print(result)\n",
    "print(\"\\n💡 sum, mean, count ignore nulls by default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Nulls in Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null comparisons always return null (not True or False)\n",
    "df_compare = pl.DataFrame({\n",
    "    'value': [1, 2, None, 4, None]\n",
    "})\n",
    "\n",
    "df_compare_result = df_compare.with_columns([\n",
    "    (pl.col('value') > 2).alias('greater_than_2'),\n",
    "    (pl.col('value') == None).alias('equals_none'),  # Always null!\n",
    "    pl.col('value').is_null().alias('is_null'),  # Correct way\n",
    "    pl.col('value').is_not_null().alias('is_not_null')\n",
    "])\n",
    "\n",
    "print(\"Null comparisons:\")\n",
    "print(df_compare_result)\n",
    "print(\"\\n⚠️ Use .is_null() instead of == None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Nulls in Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulls don't match in joins (by default)\n",
    "df_left = pl.DataFrame({\n",
    "    'key': [1, 2, None, 4],\n",
    "    'value_left': ['A', 'B', 'C', 'D']\n",
    "})\n",
    "\n",
    "df_right = pl.DataFrame({\n",
    "    'key': [1, None, 3, 4],\n",
    "    'value_right': ['W', 'X', 'Y', 'Z']\n",
    "})\n",
    "\n",
    "result = df_left.join(df_right, on='key', how='inner')\n",
    "\n",
    "print(\"Join with null keys:\")\n",
    "print(result)\n",
    "print(\"\\n💡 Null keys don't match (row with key=None excluded)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Missing Data Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic dataset with missing patterns\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "\n",
    "df_pattern = pl.DataFrame({\n",
    "    'id': range(n),\n",
    "    'age': [x if np.random.random() > 0.1 else None for x in np.random.randint(18, 80, n)],\n",
    "    'income': [x if np.random.random() > 0.15 else None for x in np.random.randint(20000, 150000, n)],\n",
    "    'education': [x if np.random.random() > 0.05 else None \n",
    "                  for x in np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n)]\n",
    "})\n",
    "\n",
    "print(\"Missing data summary:\")\n",
    "print(df_pattern.null_count())\n",
    "print(f\"\\nMissing percentages:\")\n",
    "print((df_pattern.null_count() / len(df_pattern) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing patterns\n",
    "missing_pattern = df_pattern.select([\n",
    "    pl.col('age').is_null().alias('age_missing'),\n",
    "    pl.col('income').is_null().alias('income_missing'),\n",
    "    pl.col('education').is_null().alias('education_missing')\n",
    "])\n",
    "\n",
    "# Count combinations of missing values\n",
    "pattern_counts = missing_pattern.group_by(['age_missing', 'income_missing', 'education_missing']).agg(\n",
    "    pl.count().alias('count')\n",
    ").sort('count', descending=True)\n",
    "\n",
    "print(\"Missing value patterns:\")\n",
    "print(pattern_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Duplicate Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Identifying Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with duplicates\n",
    "df_dupes = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 2, 4, 3, 5, 1],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Bob', 'Diana', 'Charlie', 'Eve', 'Alice'],\n",
    "    'value': [100, 200, 300, 200, 400, 300, 500, 150]\n",
    "})\n",
    "\n",
    "print(\"DataFrame with duplicates:\")\n",
    "print(df_dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_duplicated(): Mark duplicate rows (True if duplicate exists)\n",
    "df_with_dupes = df_dupes.with_columns([\n",
    "    pl.col('id').is_duplicated().alias('id_is_dup'),\n",
    "    pl.col('name').is_duplicated().alias('name_is_dup')\n",
    "])\n",
    "\n",
    "print(\"Mark duplicates:\")\n",
    "print(df_with_dupes)\n",
    "print(\"\\n💡 is_duplicated() marks ALL occurrences (including first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_unique(): Mark unique values (opposite of is_duplicated)\n",
    "df_with_unique = df_dupes.with_columns([\n",
    "    pl.col('id').is_unique().alias('id_is_unique'),\n",
    "    pl.col('name').is_unique().alias('name_is_unique')\n",
    "])\n",
    "\n",
    "print(\"Mark unique values:\")\n",
    "print(df_with_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate rows based on subset of columns\n",
    "df_subset_dupes = df_dupes.with_columns(\n",
    "    pl.struct(['id', 'name']).is_duplicated().alias('row_is_duplicate')\n",
    ")\n",
    "\n",
    "print(\"Duplicates based on 'id' AND 'name':\")\n",
    "print(df_subset_dupes)\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(df_subset_dupes.filter(pl.col('row_is_duplicate')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Counting Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates per group\n",
    "duplicate_counts = df_dupes.group_by(['id', 'name']).agg(\n",
    "    pl.count().alias('count')\n",
    ").filter(\n",
    "    pl.col('count') > 1\n",
    ").sort('count', descending=True)\n",
    "\n",
    "print(\"Duplicate groups:\")\n",
    "print(duplicate_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values\n",
    "print(\"Unique value counts:\")\n",
    "print(f\"Unique ids: {df_dupes['id'].n_unique()}\")\n",
    "print(f\"Unique names: {df_dupes['name'].n_unique()}\")\n",
    "print(f\"Total rows: {len(df_dupes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique(): Keep only unique rows (all columns)\n",
    "df_unique_all = df_dupes.unique()\n",
    "\n",
    "print(\"Unique (all columns):\")\n",
    "print(df_unique_all)\n",
    "print(f\"\\nRows: {len(df_dupes)} -> {len(df_unique_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique() on specific columns\n",
    "df_unique_subset = df_dupes.unique(subset=['id', 'name'])\n",
    "\n",
    "print(\"Unique based on 'id' and 'name':\")\n",
    "print(df_unique_subset)\n",
    "print(f\"\\nRows: {len(df_dupes)} -> {len(df_unique_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique() with keep parameter\n",
    "# keep='first' (default), 'last', 'none', 'any'\n",
    "\n",
    "print(\"Keep first occurrence:\")\n",
    "df_keep_first = df_dupes.unique(subset=['id'], keep='first')\n",
    "print(df_keep_first)\n",
    "\n",
    "print(\"\\nKeep last occurrence:\")\n",
    "df_keep_last = df_dupes.unique(subset=['id'], keep='last')\n",
    "print(df_keep_last)\n",
    "\n",
    "print(\"\\nKeep none (remove all duplicates):\")\n",
    "df_keep_none = df_dupes.unique(subset=['id'], keep='none')\n",
    "print(df_keep_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Advanced Duplicate Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep duplicate with highest value\n",
    "# Example: Keep the record with highest 'value' for each 'id'\n",
    "\n",
    "df_best = (\n",
    "    df_dupes\n",
    "    .sort('value', descending=True)  # Sort by value (highest first)\n",
    "    .unique(subset=['id'], keep='first')  # Keep first (= highest value)\n",
    "    .sort('id')  # Resort by id for readability\n",
    ")\n",
    "\n",
    "print(\"Keep duplicate with highest value:\")\n",
    "print(df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark first/last occurrence\n",
    "df_marked = df_dupes.with_columns([\n",
    "    (pl.col('id').cum_count().over('id') == 1).alias('is_first'),\n",
    "    (pl.col('id').cum_count().over('id') == pl.col('id').count().over('id')).alias('is_last')\n",
    "])\n",
    "\n",
    "print(\"Mark first and last occurrences:\")\n",
    "print(df_marked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Real-World Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic messy dataset\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "df_messy = pl.DataFrame({\n",
    "    'customer_id': list(range(1, 901)) + list(np.random.choice(range(1, 901), 100)),  # Duplicates\n",
    "    'name': [f'Customer_{i}' if np.random.random() > 0.05 else None for i in range(n)],\n",
    "    'age': [np.random.randint(18, 80) if np.random.random() > 0.1 else None for _ in range(n)],\n",
    "    'email': [f'user{i}@example.com' if np.random.random() > 0.08 else None for i in range(n)],\n",
    "    'purchase_amount': [round(np.random.uniform(10, 1000), 2) if np.random.random() > 0.12 else None for _ in range(n)],\n",
    "    'signup_date': [\n",
    "        date(2024, 1, 1) + timedelta(days=int(np.random.randint(0, 365))) \n",
    "        if np.random.random() > 0.05 else None \n",
    "        for _ in range(n)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Messy dataset statistics:\")\n",
    "print(f\"Total rows: {len(df_messy)}\")\n",
    "print(f\"\\nNull counts:\")\n",
    "print(df_messy.null_count())\n",
    "print(f\"\\nDuplicate customer_ids: {df_messy['customer_id'].is_duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning pipeline\n",
    "df_cleaned = (\n",
    "    df_messy\n",
    "    # 1. Remove complete duplicates (all columns identical)\n",
    "    .unique()\n",
    "    \n",
    "    # 2. For duplicate customer_ids, keep the one with most recent signup\n",
    "    .sort('signup_date', descending=True, nulls_last=True)\n",
    "    .unique(subset=['customer_id'], keep='first')\n",
    "    \n",
    "    # 3. Fill missing values\n",
    "    .with_columns([\n",
    "        pl.col('name').fill_null('Unknown'),\n",
    "        pl.col('age').fill_null(pl.col('age').median()),\n",
    "        pl.col('email').fill_null(pl.concat_str([pl.lit('unknown_'), pl.col('customer_id'), pl.lit('@example.com')])),\n",
    "        pl.col('purchase_amount').fill_null(0.0),\n",
    "        pl.col('signup_date').fill_null(strategy='forward')\n",
    "    ])\n",
    "    \n",
    "    # 4. Remove rows still missing critical data\n",
    "    .drop_nulls(subset=['customer_id'])\n",
    "    \n",
    "    # 5. Sort for readability\n",
    "    .sort('customer_id')\n",
    ")\n",
    "\n",
    "print(\"Cleaned dataset:\")\n",
    "print(f\"Rows: {len(df_messy)} -> {len(df_cleaned)}\")\n",
    "print(f\"\\nNull counts after cleaning:\")\n",
    "print(df_cleaned.null_count())\n",
    "print(f\"\\nDuplicate customer_ids: {df_cleaned['customer_id'].is_duplicated().sum()}\")\n",
    "print(\"\\n✅ Data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Missing Data Strategies:\n",
    "\n",
    "| Method | Use Case | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **drop_nulls()** | Critical columns only | Simple, no assumptions | Loses data |\n",
    "| **fill_null(value)** | Default values known | Fast, deterministic | May bias results |\n",
    "| **fill_null(mean)** | Numeric, symmetric data | Preserves mean | Reduces variance |\n",
    "| **fill_null('forward')** | Time series | Preserves trends | Propagates errors |\n",
    "| **fill_null('backward')** | Time series | Good for forecasts | Looks into future |\n",
    "| **interpolate()** | Numeric time series | Smooth estimates | Assumes linearity |\n",
    "| **coalesce()** | Multiple fallback sources | Flexible | Complex logic |\n",
    "\n",
    "## Duplicate Handling:\n",
    "\n",
    "| Method | Effect | Keep Parameter |\n",
    "|--------|--------|----------------|\n",
    "| **unique()** | Remove duplicates | 'first', 'last', 'none', 'any' |\n",
    "| **is_duplicated()** | Mark duplicates | Marks ALL occurrences |\n",
    "| **is_unique()** | Mark unique | Opposite of is_duplicated |\n",
    "\n",
    "## Best Practices:\n",
    "\n",
    "1. ✅ **Investigate** missing patterns before filling\n",
    "2. ✅ **Document** your cleaning decisions\n",
    "3. ✅ **Validate** data quality after cleaning\n",
    "4. ✅ **Use** forward fill for time series\n",
    "5. ✅ **Prefer** dropping over imputing for critical columns\n",
    "6. ✅ **Keep** duplicates with best data quality\n",
    "7. ❌ **Don't** use `== None` (use `.is_null()` instead)\n",
    "8. ❌ **Don't** blindly fill all nulls with mean\n",
    "\n",
    "## Common Patterns:\n",
    "```python\n",
    "# Fill nulls\n",
    "df.with_columns(pl.col('col').fill_null(strategy='forward'))\n",
    "\n",
    "# Drop nulls in critical columns\n",
    "df.drop_nulls(subset=['id', 'date'])\n",
    "\n",
    "# Remove duplicates, keep first\n",
    "df.unique(subset=['id'], keep='first')\n",
    "\n",
    "# Find duplicates\n",
    "df.filter(pl.col('id').is_duplicated())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Fill nulls with different strategies\n",
    "df_ex1 = pl.DataFrame({\n",
    "    'date': [date(2024, 1, i) for i in range(1, 11)],\n",
    "    'temperature': [20, None, 22, None, None, 25, 26, None, 28, 29]\n",
    "})\n",
    "\n",
    "# TODO: Fill temperature nulls using:\n",
    "# a) Forward fill\n",
    "# b) Interpolation\n",
    "# c) Mean\n",
    "# Compare results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Find and remove duplicates\n",
    "df_ex2 = pl.DataFrame({\n",
    "    'user_id': [1, 2, 3, 2, 4, 3, 5],\n",
    "    'score': [100, 200, 300, 250, 400, 300, 500],\n",
    "    'timestamp': ['2024-01-01', '2024-01-02', '2024-01-03', \n",
    "                  '2024-01-04', '2024-01-05', '2024-01-06', '2024-01-07']\n",
    "})\n",
    "\n",
    "# TODO: Keep the duplicate with the highest score for each user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build a data cleaning pipeline\n",
    "# TODO: Create a messy dataset and clean it following best practices\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
