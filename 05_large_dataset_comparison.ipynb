{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars vs Pandas: Large Dataset Performance Comparison\n",
    "\n",
    "This notebook compares Polars and Pandas performance on real-world datasets with millions of rows.\n",
    "\n",
    "## Datasets Used\n",
    "\n",
    "### 1. NYC Taxi Trip Data (~1-2M rows per month)\n",
    "- **Source**: NYC Taxi & Limousine Commission via AWS Open Data\n",
    "- **Format**: Parquet (original CSV also available)\n",
    "- **URL**: https://registry.opendata.aws/nyc-tlc-trip-records-pds/\n",
    "- **Data**: Yellow taxi trip records including pickup/dropoff times, locations, fares, etc.\n",
    "\n",
    "### 2. US Airline On-Time Performance (10M+ rows)\n",
    "- **Source**: Bureau of Transportation Statistics (BTS)\n",
    "- **Format**: CSV/Parquet\n",
    "- **URL**: https://www.transtats.bts.gov/\n",
    "- **Data**: Flight on-time performance including delays, cancellations, carrier info\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# For downloading data\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage_mb():\n",
    "    \"\"\"Get current process memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def benchmark(func, name, *args, **kwargs):\n",
    "    \"\"\"Benchmark a function execution\"\"\"\n",
    "    mem_before = get_memory_usage_mb()\n",
    "    start = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    duration = time.time() - start\n",
    "    mem_after = get_memory_usage_mb()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Time: {duration:.3f}s\")\n",
    "    print(f\"  Memory change: {mem_after - mem_before:.2f} MB\")\n",
    "    print(f\"  Total memory: {mem_after:.2f} MB\")\n",
    "    \n",
    "    return result, duration, mem_after - mem_before\n",
    "\n",
    "def compare_operations(polars_func, pandas_func, description):\n",
    "    \"\"\"Compare Polars vs Pandas for a given operation\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Operation: {description}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Polars\n",
    "    pl_result, pl_time, pl_mem = benchmark(polars_func, \"Polars\")\n",
    "    \n",
    "    # Pandas\n",
    "    pd_result, pd_time, pd_mem = benchmark(pandas_func, \"Pandas\")\n",
    "    \n",
    "    # Comparison\n",
    "    speedup = pd_time / pl_time if pl_time > 0 else float('inf')\n",
    "    mem_ratio = pd_mem / pl_mem if pl_mem > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Results:\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x (Polars is {speedup:.2f}x faster)\")\n",
    "    print(f\"  Memory: Pandas uses {mem_ratio:.2f}x more memory\")\n",
    "    \n",
    "    return {\n",
    "        'operation': description,\n",
    "        'polars_time': pl_time,\n",
    "        'pandas_time': pd_time,\n",
    "        'speedup': speedup,\n",
    "        'polars_mem': pl_mem,\n",
    "        'pandas_mem': pd_mem\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: NYC Taxi Data (~1-2M rows)\n",
    "\n",
    "### Download Instructions\n",
    "\n",
    "The NYC Taxi data is available in Parquet format from AWS S3. We'll download one month of data.\n",
    "\n",
    "**Direct download URLs** (no authentication required):\n",
    "- Yellow taxi 2024: `https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet`\n",
    "- Green taxi 2024: `https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-01.parquet`\n",
    "\n",
    "You can also browse all available files at:\n",
    "- https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# NYC Taxi data URL (January 2024 - ~2.9M rows)\n",
    "nyc_taxi_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet'\n",
    "nyc_taxi_file = data_dir / 'nyc_taxi_2024_01.parquet'\n",
    "\n",
    "# Download if not exists\n",
    "if not nyc_taxi_file.exists():\n",
    "    print(f\"Downloading NYC Taxi data (~300MB)...\")\n",
    "    response = requests.get(nyc_taxi_url)\n",
    "    with open(nyc_taxi_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"âœ“ Downloaded to {nyc_taxi_file}\")\n",
    "else:\n",
    "    print(f\"âœ“ File already exists: {nyc_taxi_file}\")\n",
    "\n",
    "# Check file size\n",
    "file_size_mb = nyc_taxi_file.stat().st_size / 1024 / 1024\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data: Polars vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with Polars\n",
    "df_pl, pl_load_time, pl_load_mem = benchmark(\n",
    "    pl.read_parquet,\n",
    "    \"Polars - Load Parquet\",\n",
    "    nyc_taxi_file\n",
    ")\n",
    "\n",
    "print(f\"\\nShape: {df_pl.shape}\")\n",
    "print(f\"\\nFirst few columns:\")\n",
    "print(df_pl.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with Pandas\n",
    "df_pd, pd_load_time, pd_load_mem = benchmark(\n",
    "    pd.read_parquet,\n",
    "    \"Pandas - Load Parquet\",\n",
    "    nyc_taxi_file\n",
    ")\n",
    "\n",
    "print(f\"\\nShape: {df_pd.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_pd.head())\n",
    "\n",
    "# Compare loading performance\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Loading Performance Comparison\")\n",
    "print('='*60)\n",
    "print(f\"Polars: {pl_load_time:.3f}s, Memory: {pl_load_mem:.2f} MB\")\n",
    "print(f\"Pandas: {pd_load_time:.3f}s, Memory: {pd_load_mem:.2f} MB\")\n",
    "print(f\"Speedup: {pd_load_time/pl_load_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars exploration\n",
    "print(\"Polars DataFrame Info:\")\n",
    "print(df_pl.describe())\n",
    "print(f\"\\nColumns: {df_pl.columns}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_pl.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarks\n",
    "\n",
    "#### 1. Filtering Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Filter for trips > $50 with more than 2 passengers\n",
    "result1 = compare_operations(\n",
    "    lambda: df_pl.filter(\n",
    "        (pl.col('total_amount') > 50) & \n",
    "        (pl.col('passenger_count') > 2)\n",
    "    ),\n",
    "    lambda: df_pd[\n",
    "        (df_pd['total_amount'] > 50) & \n",
    "        (df_pd['passenger_count'] > 2)\n",
    "    ],\n",
    "    \"Filter: total_amount > $50 AND passenger_count > 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Group by passenger count and calculate average fare\n",
    "result2 = compare_operations(\n",
    "    lambda: df_pl.group_by('passenger_count').agg([\n",
    "        pl.col('total_amount').mean().alias('avg_fare'),\n",
    "        pl.col('trip_distance').mean().alias('avg_distance'),\n",
    "        pl.col('VendorID').count().alias('trip_count')\n",
    "    ]).sort('passenger_count'),\n",
    "    lambda: df_pd.groupby('passenger_count').agg({\n",
    "        'total_amount': 'mean',\n",
    "        'trip_distance': 'mean',\n",
    "        'VendorID': 'count'\n",
    "    }).rename(columns={\n",
    "        'total_amount': 'avg_fare',\n",
    "        'trip_distance': 'avg_distance',\n",
    "        'VendorID': 'trip_count'\n",
    "    }).sort_index(),\n",
    "    \"GroupBy passenger_count with aggregations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Complex Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Multiple group by with complex aggregations\n",
    "result3 = compare_operations(\n",
    "    lambda: df_pl.group_by(['PULocationID', 'DOLocationID']).agg([\n",
    "        pl.col('total_amount').sum().alias('total_revenue'),\n",
    "        pl.col('trip_distance').mean().alias('avg_distance'),\n",
    "        pl.col('VendorID').count().alias('trip_count'),\n",
    "        pl.col('tip_amount').max().alias('max_tip')\n",
    "    ]).filter(pl.col('trip_count') > 100).sort('total_revenue', descending=True).head(20),\n",
    "    lambda: df_pd.groupby(['PULocationID', 'DOLocationID']).agg({\n",
    "        'total_amount': 'sum',\n",
    "        'trip_distance': 'mean',\n",
    "        'VendorID': 'count',\n",
    "        'tip_amount': 'max'\n",
    "    }).rename(columns={\n",
    "        'total_amount': 'total_revenue',\n",
    "        'trip_distance': 'avg_distance',\n",
    "        'VendorID': 'trip_count',\n",
    "        'tip_amount': 'max_tip'\n",
    "    }).query('trip_count > 100').sort_values('total_revenue', ascending=False).head(20),\n",
    "    \"Complex GroupBy: Top 20 routes by revenue (with filters)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime column and extract features\n",
    "result4 = compare_operations(\n",
    "    lambda: df_pl.with_columns([\n",
    "        pl.col('tpep_pickup_datetime').dt.hour().alias('hour'),\n",
    "        pl.col('tpep_pickup_datetime').dt.day().alias('day'),\n",
    "        pl.col('tpep_pickup_datetime').dt.month().alias('month'),\n",
    "        pl.col('tpep_pickup_datetime').dt.weekday().alias('weekday')\n",
    "    ]),\n",
    "    lambda: df_pd.assign(\n",
    "        hour=df_pd['tpep_pickup_datetime'].dt.hour,\n",
    "        day=df_pd['tpep_pickup_datetime'].dt.day,\n",
    "        month=df_pd['tpep_pickup_datetime'].dt.month,\n",
    "        weekday=df_pd['tpep_pickup_datetime'].dt.weekday\n",
    "    ),\n",
    "    \"DateTime extraction: hour, day, month, weekday\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Sorting Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Sort by multiple columns\n",
    "result5 = compare_operations(\n",
    "    lambda: df_pl.sort(['total_amount', 'trip_distance'], descending=[True, False]),\n",
    "    lambda: df_pd.sort_values(['total_amount', 'trip_distance'], ascending=[False, True]),\n",
    "    \"Sort by total_amount (desc) and trip_distance (asc)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Running average using window functions\n",
    "result6 = compare_operations(\n",
    "    lambda: df_pl.with_columns(\n",
    "        pl.col('total_amount').rolling_mean(window_size=100).over('PULocationID').alias('rolling_avg_fare')\n",
    "    ),\n",
    "    lambda: df_pd.assign(\n",
    "        rolling_avg_fare=df_pd.groupby('PULocationID')['total_amount'].transform(\n",
    "            lambda x: x.rolling(window=100, min_periods=1).mean()\n",
    "        )\n",
    "    ),\n",
    "    \"Window function: Rolling average of fare by pickup location\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Create multiple derived columns\n",
    "result7 = compare_operations(\n",
    "    lambda: df_pl.with_columns([\n",
    "        (pl.col('total_amount') / pl.col('trip_distance')).alias('price_per_mile'),\n",
    "        (pl.col('tip_amount') / pl.col('total_amount')).alias('tip_percentage'),\n",
    "        (pl.col('tpep_dropoff_datetime') - pl.col('tpep_pickup_datetime')).dt.total_seconds().alias('duration_seconds')\n",
    "    ]),\n",
    "    lambda: df_pd.assign(\n",
    "        price_per_mile=df_pd['total_amount'] / df_pd['trip_distance'],\n",
    "        tip_percentage=df_pd['tip_amount'] / df_pd['total_amount'],\n",
    "        duration_seconds=(df_pd['tpep_dropoff_datetime'] - df_pd['tpep_pickup_datetime']).dt.total_seconds()\n",
    "    ),\n",
    "    \"Create derived columns: price_per_mile, tip_percentage, duration\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "results = [result1, result2, result3, result4, result5, result6, result7]\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pl.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY - NYC Taxi Dataset (~2.9M rows)\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df)\n",
    "\n",
    "print(f\"\\n\\nðŸ“Š Overall Statistics:\")\n",
    "print(f\"Average Speedup: {summary_df['speedup'].mean():.2f}x\")\n",
    "print(f\"Median Speedup: {summary_df['speedup'].median():.2f}x\")\n",
    "print(f\"Max Speedup: {summary_df['speedup'].max():.2f}x\")\n",
    "print(f\"Min Speedup: {summary_df['speedup'].min():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: Multiple Months for 10M+ Rows\n",
    "\n",
    "For larger datasets (10M+ rows), we can combine multiple months of NYC Taxi data or use the airline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download multiple months to get 10M+ rows\n",
    "months = ['2024-01', '2024-02', '2024-03', '2024-04']\n",
    "files_to_download = []\n",
    "\n",
    "for month in months:\n",
    "    url = f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{month}.parquet'\n",
    "    filepath = data_dir / f'nyc_taxi_{month}.parquet'\n",
    "    \n",
    "    if not filepath.exists():\n",
    "        print(f\"Downloading {month} data...\")\n",
    "        response = requests.get(url)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"âœ“ Downloaded {filepath.name}\")\n",
    "    else:\n",
    "        print(f\"âœ“ Already exists: {filepath.name}\")\n",
    "    \n",
    "    files_to_download.append(str(filepath))\n",
    "\n",
    "print(f\"\\nTotal files: {len(files_to_download)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Combining Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars: Load and concatenate multiple files\n",
    "print(\"Loading with Polars...\")\n",
    "large_df_pl, pl_large_time, pl_large_mem = benchmark(\n",
    "    lambda: pl.concat([pl.read_parquet(f) for f in files_to_download]),\n",
    "    \"Polars - Load and Concatenate 4 months\"\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {large_df_pl.shape}\")\n",
    "print(f\"Total rows: {large_df_pl.shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas: Load and concatenate multiple files\n",
    "print(\"Loading with Pandas...\")\n",
    "large_df_pd, pd_large_time, pd_large_mem = benchmark(\n",
    "    lambda: pd.concat([pd.read_parquet(f) for f in files_to_download], ignore_index=True),\n",
    "    \"Pandas - Load and Concatenate 4 months\"\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shape: {large_df_pd.shape}\")\n",
    "print(f\"Total rows: {large_df_pd.shape[0]:,}\")\n",
    "\n",
    "# Compare\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Large Dataset Loading Comparison\")\n",
    "print('='*60)\n",
    "print(f\"Polars: {pl_large_time:.3f}s, Memory: {pl_large_mem:.2f} MB\")\n",
    "print(f\"Pandas: {pd_large_time:.3f}s, Memory: {pd_large_mem:.2f} MB\")\n",
    "print(f\"Speedup: {pd_large_time/pl_large_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks on Large Dataset (10M+ rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 1: Complex aggregation on large dataset\n",
    "large_result1 = compare_operations(\n",
    "    lambda: large_df_pl.group_by([\n",
    "        pl.col('tpep_pickup_datetime').dt.date().alias('date'),\n",
    "        'PULocationID'\n",
    "    ]).agg([\n",
    "        pl.col('total_amount').sum().alias('daily_revenue'),\n",
    "        pl.col('trip_distance').mean().alias('avg_distance'),\n",
    "        pl.col('VendorID').count().alias('trip_count'),\n",
    "        pl.col('passenger_count').sum().alias('total_passengers')\n",
    "    ]).sort('daily_revenue', descending=True).head(100),\n",
    "    lambda: large_df_pd.assign(\n",
    "        date=large_df_pd['tpep_pickup_datetime'].dt.date\n",
    "    ).groupby(['date', 'PULocationID']).agg({\n",
    "        'total_amount': 'sum',\n",
    "        'trip_distance': 'mean',\n",
    "        'VendorID': 'count',\n",
    "        'passenger_count': 'sum'\n",
    "    }).rename(columns={\n",
    "        'total_amount': 'daily_revenue',\n",
    "        'trip_distance': 'avg_distance',\n",
    "        'VendorID': 'trip_count',\n",
    "        'passenger_count': 'total_passengers'\n",
    "    }).sort_values('daily_revenue', ascending=False).head(100),\n",
    "    \"Large Dataset: Daily revenue by location (top 100)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 2: Filtering on large dataset\n",
    "large_result2 = compare_operations(\n",
    "    lambda: large_df_pl.filter(\n",
    "        (pl.col('trip_distance') > 10) &\n",
    "        (pl.col('total_amount') > 30) &\n",
    "        (pl.col('passenger_count') >= 2)\n",
    "    ).select([\n",
    "        'tpep_pickup_datetime',\n",
    "        'PULocationID',\n",
    "        'DOLocationID',\n",
    "        'trip_distance',\n",
    "        'total_amount'\n",
    "    ]),\n",
    "    lambda: large_df_pd[\n",
    "        (large_df_pd['trip_distance'] > 10) &\n",
    "        (large_df_pd['total_amount'] > 30) &\n",
    "        (large_df_pd['passenger_count'] >= 2)\n",
    "    ][[\n",
    "        'tpep_pickup_datetime',\n",
    "        'PULocationID',\n",
    "        'DOLocationID',\n",
    "        'trip_distance',\n",
    "        'total_amount'\n",
    "    ]],\n",
    "    \"Large Dataset: Filter long expensive trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark 3: Percentile calculations\n",
    "large_result3 = compare_operations(\n",
    "    lambda: large_df_pl.group_by('PULocationID').agg([\n",
    "        pl.col('total_amount').quantile(0.25).alias('p25_fare'),\n",
    "        pl.col('total_amount').quantile(0.50).alias('p50_fare'),\n",
    "        pl.col('total_amount').quantile(0.75).alias('p75_fare'),\n",
    "        pl.col('total_amount').quantile(0.95).alias('p95_fare'),\n",
    "    ]),\n",
    "    lambda: large_df_pd.groupby('PULocationID')['total_amount'].quantile([0.25, 0.50, 0.75, 0.95]).unstack(),\n",
    "    \"Large Dataset: Fare percentiles by location\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large dataset results summary\n",
    "large_results = [large_result1, large_result2, large_result3]\n",
    "large_summary_df = pl.DataFrame(large_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PERFORMANCE SUMMARY - Large Dataset ({large_df_pl.shape[0]:,} rows)\")\n",
    "print(\"=\"*80)\n",
    "print(large_summary_df)\n",
    "\n",
    "print(f\"\\n\\nðŸ“Š Large Dataset Statistics:\")\n",
    "print(f\"Average Speedup: {large_summary_df['speedup'].mean():.2f}x\")\n",
    "print(f\"Median Speedup: {large_summary_df['speedup'].median():.2f}x\")\n",
    "print(f\"Max Speedup: {large_summary_df['speedup'].max():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Evaluation Demo (Polars Only)\n",
    "\n",
    "One of Polars' key advantages is lazy evaluation with query optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy evaluation example\n",
    "print(\"Demonstrating Polars Lazy Evaluation...\\n\")\n",
    "\n",
    "# Create lazy query\n",
    "lazy_query = (\n",
    "    pl.scan_parquet(str(nyc_taxi_file))\n",
    "    .filter(pl.col('total_amount') > 0)\n",
    "    .with_columns([\n",
    "        pl.col('tpep_pickup_datetime').dt.hour().alias('hour'),\n",
    "        (pl.col('tip_amount') / pl.col('total_amount')).alias('tip_ratio')\n",
    "    ])\n",
    "    .group_by('hour')\n",
    "    .agg([\n",
    "        pl.col('total_amount').mean().alias('avg_fare'),\n",
    "        pl.col('tip_ratio').mean().alias('avg_tip_ratio'),\n",
    "        pl.col('VendorID').count().alias('trip_count')\n",
    "    ])\n",
    "    .sort('hour')\n",
    ")\n",
    "\n",
    "print(\"Query plan:\")\n",
    "print(lazy_query.explain())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Executing lazy query...\")\n",
    "start = time.time()\n",
    "result = lazy_query.collect()\n",
    "lazy_time = time.time() - start\n",
    "print(f\"Lazy execution time: {lazy_time:.3f}s\")\n",
    "print(\"\\nResult:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with eager execution\n",
    "print(\"\\nComparing with eager execution...\")\n",
    "start = time.time()\n",
    "eager_result = (\n",
    "    df_pl\n",
    "    .filter(pl.col('total_amount') > 0)\n",
    "    .with_columns([\n",
    "        pl.col('tpep_pickup_datetime').dt.hour().alias('hour'),\n",
    "        (pl.col('tip_amount') / pl.col('total_amount')).alias('tip_ratio')\n",
    "    ])\n",
    "    .group_by('hour')\n",
    "    .agg([\n",
    "        pl.col('total_amount').mean().alias('avg_fare'),\n",
    "        pl.col('tip_ratio').mean().alias('avg_tip_ratio'),\n",
    "        pl.col('VendorID').count().alias('trip_count')\n",
    "    ])\n",
    "    .sort('hour')\n",
    ")\n",
    "eager_time = time.time() - start\n",
    "\n",
    "print(f\"Eager execution time: {eager_time:.3f}s\")\n",
    "print(f\"Lazy is {eager_time/lazy_time:.2f}x faster (with query optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Efficiency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage of DataFrames\n",
    "print(\"Memory Usage Comparison:\\n\")\n",
    "\n",
    "# Polars memory usage\n",
    "pl_memory = df_pl.estimated_size() / 1024 / 1024  # Convert to MB\n",
    "print(f\"Polars DataFrame: {pl_memory:.2f} MB\")\n",
    "\n",
    "# Pandas memory usage\n",
    "pd_memory = df_pd.memory_usage(deep=True).sum() / 1024 / 1024  # Convert to MB\n",
    "print(f\"Pandas DataFrame: {pd_memory:.2f} MB\")\n",
    "\n",
    "print(f\"\\nPandas uses {pd_memory/pl_memory:.2f}x more memory than Polars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Performance Benefits of Polars:\n",
    "1. **Speed**: Typically 2-10x faster than Pandas on large datasets\n",
    "2. **Memory**: More memory-efficient due to Arrow format\n",
    "3. **Lazy Evaluation**: Query optimization leads to better performance\n",
    "4. **Parallel Processing**: Automatic parallelization of operations\n",
    "5. **Type Safety**: Stronger type system prevents errors\n",
    "\n",
    "### When to Use Polars:\n",
    "- Large datasets (1M+ rows)\n",
    "- Complex aggregations and transformations\n",
    "- Performance-critical applications\n",
    "- New projects without Pandas legacy code\n",
    "\n",
    "### When Pandas Might Be Better:\n",
    "- Small datasets (<100k rows) where performance doesn't matter\n",
    "- Existing codebase with heavy Pandas usage\n",
    "- Need for specific Pandas ecosystem libraries\n",
    "- Team familiarity with Pandas API\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "### Datasets:\n",
    "- **NYC Taxi Data**: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "- **US Airline Data**: https://www.transtats.bts.gov/\n",
    "- **More datasets**: https://registry.opendata.aws/\n",
    "\n",
    "### Documentation:\n",
    "- **Polars**: https://pola-rs.github.io/polars/\n",
    "- **Performance Guide**: https://pola-rs.github.io/polars-book/user-guide/performance/\n",
    "- **Migration from Pandas**: https://pola-rs.github.io/polars-book/user-guide/migration/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
