{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Advanced I/O and Cloud Storage - Comprehensive Guide\n",
    "\n",
    "This notebook covers advanced input/output operations in Polars, including cloud storage, databases, and various file formats.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Reading from and writing to cloud storage (AWS S3, Azure Blob, Google Cloud Storage)\n",
    "- Database connections (PostgreSQL, MySQL, SQLite)\n",
    "- Working with partitioned Parquet files (Hive-style partitioning)\n",
    "- Additional file formats (Excel, Feather/Arrow IPC, NDJSON)\n",
    "- Advanced I/O options (compression, URLs, streaming)\n",
    "- Schema management and evolution\n",
    "- Performance optimization for I/O operations\n",
    "\n",
    "## Prerequisites:\n",
    "```bash\n",
    "# Install additional dependencies\n",
    "pip install polars s3fs adlfs gcsfs sqlalchemy psycopg2-binary pymysql openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "import tempfile\n",
    "\n",
    "# Create temporary directory for examples\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Temporary directory: {temp_dir}\")\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Cloud Storage Integration\n",
    "\n",
    "Polars can read from and write to cloud storage providers using storage options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 AWS S3 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 - Reading with credentials\n",
    "\n",
    "# Method 1: Using storage_options with explicit credentials\n",
    "storage_options_s3 = {\n",
    "    'aws_access_key_id': 'YOUR_ACCESS_KEY',\n",
    "    'aws_secret_access_key': 'YOUR_SECRET_KEY',\n",
    "    'aws_region': 'us-east-1'\n",
    "}\n",
    "\n",
    "# Read CSV from S3\n",
    "# df = pl.read_csv(\n",
    "#     's3://my-bucket/path/to/data.csv',\n",
    "#     storage_options=storage_options_s3\n",
    "# )\n",
    "\n",
    "# Read Parquet from S3 (lazy for better performance)\n",
    "# df = pl.scan_parquet(\n",
    "#     's3://my-bucket/path/to/data.parquet',\n",
    "#     storage_options=storage_options_s3\n",
    "# ).collect()\n",
    "\n",
    "print(\"Example: Reading from S3 with explicit credentials\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using AWS credential provider (recommended)\n",
    "\n",
    "# Polars can automatically use AWS credentials from:\n",
    "# - Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "# - AWS config files (~/.aws/credentials)\n",
    "# - IAM roles (when running on EC2/ECS/Lambda)\n",
    "\n",
    "# Using default credentials\n",
    "# df = pl.scan_parquet('s3://my-bucket/data.parquet').collect()\n",
    "\n",
    "# Using specific AWS profile\n",
    "# credential_provider = pl.CredentialProviderAWS(\n",
    "#     profile_name='my-profile'\n",
    "# )\n",
    "\n",
    "print(\"Example: Using AWS credential provider\")\n",
    "print(\"Polars automatically detects AWS credentials from environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to S3\n",
    "\n",
    "# Create sample data\n",
    "df_sample = pl.DataFrame({\n",
    "    'id': range(1000),\n",
    "    'value': range(1000, 2000),\n",
    "    'category': ['A', 'B', 'C'] * 333 + ['A']\n",
    "})\n",
    "\n",
    "# Write to S3 (parquet is recommended)\n",
    "# df_sample.write_parquet(\n",
    "#     's3://my-bucket/output/data.parquet',\n",
    "#     storage_options=storage_options_s3\n",
    "# )\n",
    "\n",
    "# Write CSV to S3\n",
    "# df_sample.write_csv(\n",
    "#     's3://my-bucket/output/data.csv',\n",
    "#     storage_options=storage_options_s3\n",
    "# )\n",
    "\n",
    "print(\"Example: Writing to S3\")\n",
    "print(f\"Sample data shape: {df_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Azure Blob Storage Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Blob Storage\n",
    "\n",
    "storage_options_azure = {\n",
    "    'account_name': 'YOUR_ACCOUNT_NAME',\n",
    "    'account_key': 'YOUR_ACCOUNT_KEY'\n",
    "}\n",
    "\n",
    "# Alternative: Using SAS token\n",
    "storage_options_azure_sas = {\n",
    "    'account_name': 'YOUR_ACCOUNT_NAME',\n",
    "    'sas_token': 'YOUR_SAS_TOKEN'\n",
    "}\n",
    "\n",
    "# Read from Azure Blob Storage\n",
    "# df = pl.read_parquet(\n",
    "#     'az://container-name/path/to/data.parquet',\n",
    "#     storage_options=storage_options_azure\n",
    "# )\n",
    "\n",
    "# Lazy scan for better performance\n",
    "# df = pl.scan_parquet(\n",
    "#     'az://container-name/path/*.parquet',\n",
    "#     storage_options=storage_options_azure\n",
    "# ).collect()\n",
    "\n",
    "print(\"Example: Azure Blob Storage integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Google Cloud Storage (GCS) Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Cloud Storage\n",
    "\n",
    "# Using service account credentials\n",
    "storage_options_gcs = {\n",
    "    'token': '/path/to/service-account-key.json'\n",
    "}\n",
    "\n",
    "# Or using default credentials (if running on GCP)\n",
    "storage_options_gcs_default = {\n",
    "    'token': 'google_default'\n",
    "}\n",
    "\n",
    "# Read from GCS\n",
    "# df = pl.read_parquet(\n",
    "#     'gs://bucket-name/path/to/data.parquet',\n",
    "#     storage_options=storage_options_gcs\n",
    "# )\n",
    "\n",
    "# Scan multiple files with pattern\n",
    "# df = pl.scan_parquet(\n",
    "#     'gs://bucket-name/data/*.parquet',\n",
    "#     storage_options=storage_options_gcs\n",
    "# ).collect()\n",
    "\n",
    "print(\"Example: Google Cloud Storage integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Database Connections\n",
    "\n",
    "Reading from and writing to databases using connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 SQLite (No Server Required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a sample SQLite database\n",
    "db_path = os.path.join(temp_dir, 'example.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create sample data and insert into SQLite\n",
    "df_customers = pl.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', \n",
    "              'diana@example.com', 'eve@example.com'],\n",
    "    'age': [25, 30, 35, 28, 42]\n",
    "})\n",
    "\n",
    "# Write Polars DataFrame to SQLite using pandas bridge\n",
    "df_customers.to_pandas().to_sql('customers', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Created SQLite database with sample data\")\n",
    "print(df_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from SQLite using SQL query\n",
    "query = \"SELECT * FROM customers WHERE age > 28\"\n",
    "df_from_db = pl.read_database(query, connection=f\"sqlite:///{db_path}\")\n",
    "\n",
    "print(\"Data read from SQLite:\")\n",
    "print(df_from_db)\n",
    "\n",
    "# Alternative: Read entire table\n",
    "df_full = pl.read_database(\"SELECT * FROM customers\", connection=f\"sqlite:///{db_path}\")\n",
    "print(f\"\\nFull table shape: {df_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PostgreSQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL connection string\n",
    "# postgresql://username:password@host:port/database\n",
    "\n",
    "postgres_conn = \"postgresql://user:password@localhost:5432/mydb\"\n",
    "\n",
    "# Read from PostgreSQL\n",
    "# df = pl.read_database(\n",
    "#     \"SELECT * FROM sales WHERE date >= '2024-01-01'\",\n",
    "#     connection=postgres_conn\n",
    "# )\n",
    "\n",
    "# Using parameters (safer for user input)\n",
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine(postgres_conn)\n",
    "\n",
    "# df = pl.read_database(\n",
    "#     \"SELECT * FROM sales WHERE region = :region\",\n",
    "#     connection=engine,\n",
    "#     params={'region': 'West'}\n",
    "# )\n",
    "\n",
    "print(\"Example: PostgreSQL connection\")\n",
    "print(\"Use pl.read_database() with PostgreSQL connection string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 MySQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection string\n",
    "# mysql+pymysql://username:password@host:port/database\n",
    "\n",
    "mysql_conn = \"mysql+pymysql://user:password@localhost:3306/mydb\"\n",
    "\n",
    "# Read from MySQL\n",
    "# df = pl.read_database(\n",
    "#     \"SELECT * FROM orders WHERE status = 'completed'\",\n",
    "#     connection=mysql_conn\n",
    "# )\n",
    "\n",
    "# Join tables in database (more efficient than pulling all data)\n",
    "# complex_query = \"\"\"\n",
    "#     SELECT o.*, c.name, c.email\n",
    "#     FROM orders o\n",
    "#     JOIN customers c ON o.customer_id = c.id\n",
    "#     WHERE o.order_date >= '2024-01-01'\n",
    "# \"\"\"\n",
    "# df = pl.read_database(complex_query, connection=mysql_conn)\n",
    "\n",
    "print(\"Example: MySQL connection\")\n",
    "print(\"Use pl.read_database() with MySQL connection string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Writing to Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Polars DataFrames to databases\n",
    "# Currently requires conversion through pandas\n",
    "\n",
    "# Method 1: Using pandas bridge\n",
    "df_to_write = pl.DataFrame({\n",
    "    'product_id': [1, 2, 3],\n",
    "    'name': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'price': [1200, 25, 75]\n",
    "})\n",
    "\n",
    "# Write to SQLite\n",
    "df_to_write.to_pandas().to_sql(\n",
    "    'products', \n",
    "    conn, \n",
    "    if_exists='replace',  # 'append' to add to existing table\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Written to database using pandas bridge\")\n",
    "\n",
    "# Verify write\n",
    "df_verify = pl.read_database(\"SELECT * FROM products\", connection=f\"sqlite:///{db_path}\")\n",
    "print(df_verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using SQLAlchemy engine (more flexible)\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "df_large = pl.DataFrame({\n",
    "    'id': range(1000),\n",
    "    'value': range(1000, 2000)\n",
    ""})\n",
    "\n",
    "# Write in chunks for large datasets\n",
    "df_large.to_pandas().to_sql(\n",
    "    'large_table',\n",
    "    engine,\n",
    "    if_exists='replace',\n",
    "    index=False,\n",
    "    chunksize=100  # Write in batches of 100 rows\n",
    ")\n",
    "\n",
    "print(f\"Written {len(df_large)} rows in chunks\")\n",
    "\n",
    "# Clean up\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Partitioned Parquet Files\n",
    "\n",
    "Hive-style partitioning for efficient data organization and querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Creating Partitioned Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "dates = [date(2024, 1, 1) + timedelta(days=i) for i in range(365)]\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "df_sales = pl.DataFrame({\n",
    "    'date': [random.choice(dates) for _ in range(10000)],\n",
    "    'region': [random.choice(regions) for _ in range(10000)],\n",
    "    'product_id': [random.randint(1, 100) for _ in range(10000)],\n",
    "    'quantity': [random.randint(1, 10) for _ in range(10000)],\n",
    "    'revenue': [round(random.uniform(10, 1000), 2) for _ in range(10000)]\n",
    "})\n",
    "\n",
    "# Add year and month columns for partitioning\n",
    "df_sales = df_sales.with_columns([\n",
    "    pl.col('date').dt.year().alias('year'),\n",
    "    pl.col('date').dt.month().alias('month')\n",
    "])\n",
    "\n",
    "print(\"Sample sales data:\")\n",
    "print(df_sales.head())\n",
    "print(f\"\\nTotal rows: {len(df_sales):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned by year and month (Hive-style)\n",
    "partition_dir = os.path.join(temp_dir, 'partitioned_sales')\n",
    "\n",
    "# Note: Polars doesn't have native partition_by yet, use workaround\n",
    "# Workaround: Write each partition separately\n",
    "for year in df_sales['year'].unique().sort():\n",
    "    for month in df_sales['month'].unique().sort():\n",
    "        partition_data = df_sales.filter(\n",
    "            (pl.col('year') == year) & (pl.col('month') == month)\n",
    "        ).drop(['year', 'month'])  # Don't duplicate in file\n",
    "        \n",
    "        if len(partition_data) > 0:\n",
    "            partition_path = os.path.join(\n",
    "                partition_dir, \n",
    "                f'year={year}',\n",
    "                f'month={month}',\n",
    "                'data.parquet'\n",
    "            )\n",
    "            os.makedirs(os.path.dirname(partition_path), exist_ok=True)\n",
    "            partition_data.write_parquet(partition_path)\n",
    "\n",
    "print(f\"Written partitioned data to: {partition_dir}\")\n",
    "print(f\"Partition structure: year=YYYY/month=MM/data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect partition structure\n",
    "import glob\n",
    "\n",
    "parquet_files = glob.glob(os.path.join(partition_dir, '**/*.parquet'), recursive=True)\n",
    "print(f\"Total partition files: {len(parquet_files)}\")\n",
    "print(\"\\nFirst few partitions:\")\n",
    "for f in parquet_files[:5]:\n",
    "    print(f\"  {f.replace(partition_dir, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Reading Partitioned Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all partitions with automatic partition discovery\n",
    "df_all = pl.scan_parquet(\n",
    "    os.path.join(partition_dir, '**/*.parquet'),\n",
    "    hive_partitioning=True  # Automatically parse year= and month= from paths\n",
    ").collect()\n",
    "\n",
    "print(\"Read all partitions:\")\n",
    "print(df_all.head())\n",
    "print(f\"\\nShape: {df_all.shape}\")\n",
    "print(f\"Columns: {df_all.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition pruning: Only read specific partitions (MUCH faster!)\n",
    "# Read only January 2024 data\n",
    "df_jan = pl.scan_parquet(\n",
    "    os.path.join(partition_dir, 'year=2024/month=1/*.parquet'),\n",
    "    hive_partitioning=True\n",
    ").collect()\n",
    "\n",
    "print(\"Read only January 2024 partition:\")\n",
    "print(f\"Shape: {df_jan.shape}\")\n",
    "print(df_jan.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicate pushdown with partitions\n",
    "# Filter on partition columns - Polars only reads matching partitions!\n",
    "df_q1 = pl.scan_parquet(\n",
    "    os.path.join(partition_dir, '**/*.parquet'),\n",
    "    hive_partitioning=True\n",
    ").filter(\n",
    "    (pl.col('year') == 2024) & (pl.col('month') <= 3)\n",
    ").collect()\n",
    "\n",
    "print(\"Q1 2024 data (only 3 months of partitions read):\")\n",
    "print(f\"Shape: {df_q1.shape}\")\n",
    "print(f\"Months: {df_q1['month'].unique().sort().to_list()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multi-Schema Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parquet files with slightly different schemas\n",
    "schema_dir = os.path.join(temp_dir, 'multi_schema')\n",
    "os.makedirs(schema_dir, exist_ok=True)\n",
    "\n",
    "# File 1: Original schema\n",
    "df1 = pl.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C'],\n",
    "    'value': [10, 20, 30]\n",
    "})\n",
    "df1.write_parquet(os.path.join(schema_dir, 'file1.parquet'))\n",
    "\n",
    "# File 2: Added column\n",
    "df2 = pl.DataFrame({\n",
    "    'id': [4, 5, 6],\n",
    "    'name': ['D', 'E', 'F'],\n",
    "    'value': [40, 50, 60],\n",
    "    'new_column': ['x', 'y', 'z']  # New column!\n",
    "})\n",
    "df2.write_parquet(os.path.join(schema_dir, 'file2.parquet'))\n",
    "\n",
    "# File 3: Different column type\n",
    "df3 = pl.DataFrame({\n",
    "    'id': [7, 8, 9],\n",
    "    'name': ['G', 'H', 'I'],\n",
    "    'value': [70.5, 80.5, 90.5],  # Float instead of int!\n",
    "})\n",
    "df3.write_parquet(os.path.join(schema_dir, 'file3.parquet'))\n",
    "\n",
    "print(\"Created parquet files with different schemas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files with schema evolution\n",
    "# This will handle missing columns and type differences\n",
    "df_combined = pl.read_parquet(\n",
    "    os.path.join(schema_dir, '*.parquet')\n",
    ")\n",
    "\n",
    "print(\"Combined multi-schema files:\")\n",
    "print(df_combined)\n",
    "print(f\"\\nSchema: {df_combined.schema}\")\n",
    "print(\"Note: Missing 'new_column' values are null, 'value' cast to Float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Additional File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to Excel\n",
    "df_excel = pl.DataFrame({\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor'],\n",
    "    'Price': [1200, 25, 75, 300],\n",
    "    'Stock': [15, 100, 50, 30],\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics']\n",
    "})\n",
    "\n",
    "excel_path = os.path.join(temp_dir, 'products.xlsx')\n",
    "df_excel.write_excel(excel_path)\n",
    "\n",
    "print(f\"Written to Excel: {excel_path}\")\n",
    "print(df_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from Excel\n",
    "df_from_excel = pl.read_excel(excel_path)\n",
    "\n",
    "print(\"Read from Excel:\")\n",
    "print(df_from_excel)\n",
    "\n",
    "# Read specific sheet (if multiple sheets)\n",
    "# df = pl.read_excel('file.xlsx', sheet_name='Sheet2')\n",
    "\n",
    "# Read specific range\n",
    "# df = pl.read_excel('file.xlsx', sheet_name='Sheet1', read_csv_options={'skip_rows': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feather/Arrow IPC Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feather format (Arrow IPC) - Very fast, preserves all Polars types\n",
    "feather_path = os.path.join(temp_dir, 'data.feather')\n",
    "\n",
    "# Write to Feather\n",
    "df_sales.write_ipc(feather_path)\n",
    "\n",
    "print(f\"Written to Feather: {feather_path}\")\n",
    "print(f\"File size: {os.path.getsize(feather_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Feather\n",
    "df_from_feather = pl.read_ipc(feather_path)\n",
    "\n",
    "print(\"Read from Feather:\")\n",
    "print(f\"Shape: {df_from_feather.shape}\")\n",
    "print(df_from_feather.head())\n",
    "\n",
    "# Scan for lazy loading (like parquet)\n",
    "df_lazy = pl.scan_ipc(feather_path).filter(pl.col('region') == 'North').collect()\n",
    "print(f\"\\nLazy scan with filter: {len(df_lazy)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 NDJSON (Newline-Delimited JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDJSON: One JSON object per line (common in log files, streaming data)\n",
    "ndjson_path = os.path.join(temp_dir, 'data.ndjson')\n",
    "\n",
    "df_ndjson = pl.DataFrame({\n",
    "    'timestamp': [datetime.now().isoformat() for _ in range(5)],\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'action': ['login', 'click', 'purchase', 'logout', 'login'],\n",
    "    'value': [None, 1.5, 99.99, None, None]\n",
    "})\n",
    "\n",
    "# Write NDJSON\n",
    "df_ndjson.write_ndjson(ndjson_path)\n",
    "\n",
    "print(\"Written NDJSON\")\n",
    "print(\"\\nFile contents:\")\n",
    "with open(ndjson_path, 'r') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        print(f\"Line {i}: {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NDJSON\n",
    "df_from_ndjson = pl.read_ndjson(ndjson_path)\n",
    "\n",
    "print(\"Read from NDJSON:\")\n",
    "print(df_from_ndjson)\n",
    "\n",
    "# Scan for lazy loading\n",
    "df_lazy_ndjson = pl.scan_ndjson(ndjson_path).filter(\n",
    "    pl.col('action') == 'purchase'\n",
    ").collect()\n",
    "print(f\"\\nPurchase actions only: {len(df_lazy_ndjson)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Advanced I/O Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet supports multiple compression algorithms\n",
    "# Available: snappy (default), gzip, brotli, lz4, zstd\n",
    "\n",
    "import time\n",
    "\n",
    "# Create larger dataset for compression comparison\n",
    "df_large = pl.DataFrame({\n",
    "    'id': range(100000),\n",
    "    'text': ['This is sample text data'] * 100000,\n",
    "    'value': range(100000)\n",
    "})\n",
    "\n",
    "compression_types = ['snappy', 'gzip', 'zstd', 'lz4']\n",
    "results = []\n",
    "\n",
    "for compression in compression_types:\n",
    "    path = os.path.join(temp_dir, f'data_{compression}.parquet')\n",
    "    \n",
    "    # Write with compression\n",
    "    start = time.time()\n",
    "    df_large.write_parquet(path, compression=compression)\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    # Get file size\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    \n",
    "    # Read back\n",
    "    start = time.time()\n",
    "    _ = pl.read_parquet(path)\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'compression': compression,\n",
    "        'size_mb': round(size_mb, 2),\n",
    "        'write_time': round(write_time, 3),\n",
    "        'read_time': round(read_time, 3)\n",
    "    })\n",
    "\n",
    "df_compression = pl.DataFrame(results)\n",
    "print(\"Compression comparison:\")\n",
    "print(df_compression)\n",
    "print(\"\\nRecommendation: snappy (default) for balanced performance, zstd for best compression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading compressed CSV files\n",
    "csv_gz_path = os.path.join(temp_dir, 'data.csv.gz')\n",
    "\n",
    "# Write CSV with gzip compression\n",
    "df_sample.write_csv(csv_gz_path)\n",
    "\n",
    "# Read automatically detects gzip compression\n",
    "df_from_gz = pl.read_csv(csv_gz_path)\n",
    "\n",
    "print(\"Read gzip-compressed CSV:\")\n",
    "print(f\"Shape: {df_from_gz.shape}\")\n",
    "print(f\"File size: {os.path.getsize(csv_gz_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Reading from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV directly from URL\n",
    "# Example with public dataset\n",
    "\n",
    "# df = pl.read_csv(\n",
    "#     'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
    "# )\n",
    "\n",
    "# Read Parquet from URL\n",
    "# df = pl.read_parquet(\n",
    "#     'https://example.com/data.parquet'\n",
    "# )\n",
    "\n",
    "print(\"Example: Reading from URLs\")\n",
    "print(\"Polars can read CSV, Parquet, and other formats directly from HTTP(S) URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Streaming Large Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For files larger than RAM, use streaming\n",
    "# Create a large dataset\n",
    "large_path = os.path.join(temp_dir, 'large_data.parquet')\n",
    "df_large = pl.DataFrame({\n",
    "    'id': range(1_000_000),\n",
    "    'value': range(1_000_000)\n",
    "})\n",
    "df_large.write_parquet(large_path)\n",
    "\n",
    "# Streaming query (processes in chunks, low memory)\n",
    "result = (\n",
    "    pl.scan_parquet(large_path)\n",
    "    .filter(pl.col('id') % 1000 == 0)\n",
    "    .select(['id', 'value'])\n",
    "    .collect(streaming=True)  # Streaming mode!\n",
    ")\n",
    "\n",
    "print(\"Streaming query result:\")\n",
    "print(f\"Filtered {len(result):,} rows from {len(df_large):,} total\")\n",
    "print(result.head())\n",
    "print(\"\\nStreaming mode processes data in chunks (low memory usage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly define schema when reading CSV (faster + type control)\n",
    "schema_csv_path = os.path.join(temp_dir, 'typed_data.csv')\n",
    "\n",
    "# Write sample CSV\n",
    "with open(schema_csv_path, 'w') as f:\n",
    "    f.write('id,name,age,salary,active\\n')\n",
    "    f.write('1,Alice,25,50000.50,true\\n')\n",
    "    f.write('2,Bob,30,75000.75,false\\n')\n",
    "    f.write('3,Charlie,35,60000.00,true\\n')\n",
    "\n",
    "# Read with explicit schema\n",
    "schema = {\n",
    "    'id': pl.UInt32,\n",
    "    'name': pl.Utf8,\n",
    "    'age': pl.UInt8,\n",
    "    'salary': pl.Float64,\n",
    "    'active': pl.Boolean\n",
    "}\n",
    "\n",
    "df_typed = pl.read_csv(schema_csv_path, schema=schema)\n",
    "\n",
    "print(\"Read with explicit schema:\")\n",
    "print(df_typed)\n",
    "print(f\"\\nSchema: {df_typed.schema}\")\n",
    "print(\"\\nBenefits: Faster reading, type safety, smaller memory usage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema override for specific columns\n",
    "# Useful when auto-detection gets it wrong\n",
    "df_override = pl.read_csv(\n",
    "    schema_csv_path,\n",
    "    dtypes={'id': pl.Utf8, 'age': pl.UInt16}  # Override just these columns\n",
    ")\n",
    "\n",
    "print(\"Read with dtype overrides:\")\n",
    "print(df_override.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Format Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different file formats\n",
    "import time\n",
    "\n",
    "# Test data\n",
    "df_test = pl.DataFrame({\n",
    "    'id': range(50000),\n",
    "    'text': ['Sample text data'] * 50000,\n",
    "    'value': range(50000),\n",
    "    'category': ['A', 'B', 'C'] * 16666 + ['A', 'B']\n",
    "})\n",
    "\n",
    "formats = {\n",
    "    'CSV': ('csv', df_test.write_csv, pl.read_csv),\n",
    "    'Parquet': ('parquet', df_test.write_parquet, pl.read_parquet),\n",
    "    'Feather': ('feather', df_test.write_ipc, pl.read_ipc),\n",
    "    'JSON': ('json', df_test.write_json, pl.read_json),\n",
    "}\n",
    "\n",
    "comparison = []\n",
    "\n",
    "for name, (ext, write_fn, read_fn) in formats.items():\n",
    "    path = os.path.join(temp_dir, f'test.{ext}')\n",
    "    \n",
    "    # Write\n",
    "    start = time.time()\n",
    "    write_fn(path)\n",
    "    write_time = time.time() - start\n",
    "    \n",
    "    # Size\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    \n",
    "    # Read\n",
    "    start = time.time()\n",
    "    _ = read_fn(path)\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    comparison.append({\n",
    "        'format': name,\n",
    "        'size_mb': round(size_mb, 2),\n",
    "        'write_time': round(write_time, 3),\n",
    "        'read_time': round(read_time, 3)\n",
    "    })\n",
    "\n",
    "df_format_comp = pl.DataFrame(comparison).sort('read_time')\n",
    "print(\"Format comparison (50k rows):\")\n",
    "print(df_format_comp)\n",
    "print(\"\\nðŸ† Recommendation: Parquet for best overall performance and compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 I/O Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "best_practices = pl.DataFrame({\n",
    "    'Scenario': [\n",
    "        'Large datasets',\n",
    "        'Fast read/write',\n",
    "        'Small file size',\n",
    "        'Sharing with Excel users',\n",
    "        'Log files/streaming',\n",
    "        'Cross-language (R, Julia)',\n",
    "        'Long-term archival',\n",
    "        'Cloud storage',\n",
    "        'Database integration'\n",
    "    ],\n",
    "    'Recommended Format': [\n",
    "        'Parquet (with partitioning)',\n",
    "        'Parquet or Feather',\n",
    "        'Parquet with zstd compression',\n",
    "        'Excel (.xlsx)',\n",
    "        'NDJSON',\n",
    "        'Feather/Arrow IPC',\n",
    "        'Parquet',\n",
    "        'Parquet (columnar, supports S3 select)',\n",
    "        'Direct DB connection with pl.read_database()'\n",
    "    ],\n",
    "    'Key Benefit': [\n",
    "        'Partition pruning, predicate pushdown',\n",
    "        'Columnar format, no parsing',\n",
    "        'Best compression ratios',\n",
    "        'Native format',\n",
    "        'Line-by-line processing',\n",
    "        'Language-agnostic Arrow format',\n",
    "        'Stable format, self-documenting schema',\n",
    "        'Works with scan_parquet, lazy loading',\n",
    "        'Query pushdown to database'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"I/O Best Practices:\")\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "### **Cloud Storage:**\n",
    "- Use `storage_options` parameter for S3, Azure Blob, GCS\n",
    "- Polars auto-detects credentials from environment\n",
    "- Works with scan_* methods for lazy loading\n",
    "\n",
    "### **Databases:**\n",
    "- Use `pl.read_database()` with connection strings\n",
    "- Supports PostgreSQL, MySQL, SQLite, and more\n",
    "- Push queries to database for efficiency\n",
    "- Write via pandas bridge: `df.to_pandas().to_sql()`\n",
    "\n",
    "### **Partitioned Parquet:**\n",
    "- Hive-style partitioning: `year=2024/month=01/data.parquet`\n",
    "- Use `hive_partitioning=True` when reading\n",
    "- Partition pruning dramatically speeds up queries\n",
    "- Handles schema evolution gracefully\n",
    "\n",
    "### **File Formats:**\n",
    "| Format | Best For | Speed | Size | Notes |\n",
    "|--------|----------|-------|------|-------|\n",
    "| **Parquet** | Production, archival | â­â­â­â­â­ | â­â­â­â­â­ | Default choice |\n",
    "| **Feather/IPC** | Speed-critical | â­â­â­â­â­ | â­â­â­ | Arrow format |\n",
    "| **CSV** | Human-readable, legacy | â­â­ | â­ | Slow, large |\n",
    "| **Excel** | Excel users | â­â­ | â­â­ | Limited features |\n",
    "| **NDJSON** | Logs, streaming | â­â­â­ | â­â­ | Line-by-line |\n",
    "\n",
    "### **Performance Tips:**\n",
    "1. **Always use `scan_*` for large files** (lazy loading)\n",
    "2. **Use Parquet with partitioning** for datasets > 1GB\n",
    "3. **Specify schema explicitly** for faster CSV parsing\n",
    "4. **Use zstd compression** for archival, snappy for speed\n",
    "5. **Enable streaming** for data larger than RAM\n",
    "6. **Push filters to source** (databases, partitions)\n",
    "\n",
    "### **Common Patterns:**\n",
    "```python\n",
    "# Cloud storage\n",
    "df = pl.scan_parquet('s3://bucket/data/*.parquet').collect()\n",
    "\n",
    "# Partitioned data\n",
    "df = pl.scan_parquet('data/**/*.parquet', hive_partitioning=True)\n",
    "     .filter(pl.col('year') == 2024)\n",
    "     .collect()\n",
    "\n",
    "# Database\n",
    "df = pl.read_database('SELECT * FROM table', connection=conn_str)\n",
    "\n",
    "# Streaming\n",
    "df = pl.scan_parquet('huge_file.parquet').collect(streaming=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a partitioned dataset\n",
    "# TODO: Create sales data partitioned by region and date\n",
    "# Write to partition structure: region=REGION/date=YYYY-MM-DD/data.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Read with partition pruning\n",
    "# TODO: Read only sales from 'West' region in January 2024\n",
    "# Hint: Use filter on partition columns for automatic pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Compare compression methods\n",
    "# TODO: Write same data with different compression, compare size and speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Database integration\n",
    "# TODO: Create SQLite database, write data, read with filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Schema evolution\n",
    "# TODO: Create 3 parquet files with evolving schemas, read all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary directory\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Cleaned up temporary directory: {temp_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
