{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars GroupBy & Aggregations - Comprehensive Guide\n",
    "\n",
    "Master grouping and aggregation operations in Polars.\n",
    "\n",
    "## Topics Covered:\n",
    "- Basic group_by and aggregations\n",
    "- Multiple aggregations per group\n",
    "- Multiple grouping columns\n",
    "- Advanced aggregation functions\n",
    "- Conditional aggregations\n",
    "- Rolling and dynamic group_by\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sales data\n",
    "df = pl.DataFrame({\n",
    "    'date': ['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02', '2023-01-03', '2023-01-03'],\n",
    "    'product': ['A', 'B', 'A', 'B', 'A', 'C'],\n",
    "    'region': ['North', 'North', 'South', 'South', 'North', 'West'],\n",
    "    'sales': [100, 150, 200, 175, 120, 90],\n",
    "    'quantity': [5, 8, 10, 9, 6, 4],\n",
    "    'cost': [60, 90, 120, 105, 72, 54]\n",
    "})\n",
    "\n",
    "print(\"Sample Data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple group_by with single aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product, sum sales\n",
    "result = df.group_by('product').agg(\n",
    "    pl.col('sales').sum()\n",
    ")\n",
    "\n",
    "print(\"Total sales by product:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations per group\n",
    "result = df.group_by('product').agg([\n",
    "    pl.col('sales').sum().alias('total_sales'),\n",
    "    pl.col('sales').mean().alias('avg_sales'),\n",
    "    pl.col('quantity').sum().alias('total_quantity'),\n",
    "    pl.len().alias('num_transactions')\n",
    "])\n",
    "\n",
    "print(\"Multiple aggregations:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product AND region\n",
    "result = df.group_by(['product', 'region']).agg([\n",
    "    pl.col('sales').sum().alias('total_sales'),\n",
    "    pl.col('quantity').sum().alias('total_quantity')\n",
    "]).sort(['product', 'region'])\n",
    "\n",
    "print(\"Group by product and region:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Common Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All common aggregations\n",
    "result = df.group_by('product').agg([\n",
    "    pl.col('sales').sum().alias('sum'),\n",
    "    pl.col('sales').mean().alias('mean'),\n",
    "    pl.col('sales').median().alias('median'),\n",
    "    pl.col('sales').min().alias('min'),\n",
    "    pl.col('sales').max().alias('max'),\n",
    "    pl.col('sales').std().alias('std'),\n",
    "    pl.col('sales').var().alias('variance'),\n",
    "    pl.len().alias('count')\n",
    "])\n",
    "\n",
    "print(\"Common aggregations:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique values and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values and counts\n",
    "result = df.group_by('region').agg([\n",
    "    pl.col('product').n_unique().alias('unique_products'),\n",
    "    pl.col('product').unique().alias('product_list'),\n",
    "    pl.len().alias('num_transactions')\n",
    "])\n",
    "\n",
    "print(\"Unique values:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, last, and nth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first, last values per group\n",
    "result = df.group_by('product').agg([\n",
    "    pl.col('sales').first().alias('first_sale'),\n",
    "    pl.col('sales').last().alias('last_sale'),\n",
    "    pl.col('date').first().alias('first_date'),\n",
    "    pl.col('date').last().alias('last_date')\n",
    "]).sort('product')\n",
    "\n",
    "print(\"First and last values:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate with conditions\n",
    "result = df.group_by('region').agg([\n",
    "    pl.col('sales').sum().alias('total_sales'),\n",
    "    # Count high-value sales (> 150)\n",
    "    pl.col('sales').filter(pl.col('sales') > 150).len().alias('high_value_count'),\n",
    "    # Sum only high-value sales\n",
    "    pl.col('sales').filter(pl.col('sales') > 150).sum().alias('high_value_sum'),\n",
    "    # Average of low-value sales (<= 150)\n",
    "    pl.col('sales').filter(pl.col('sales') <= 150).mean().alias('low_value_avg')\n",
    "])\n",
    "\n",
    "print(\"Conditional aggregations:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating expressions (computed columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate computed values\n",
    "result = df.group_by('product').agg([\n",
    "    # Profit = sales - cost\n",
    "    (pl.col('sales') - pl.col('cost')).sum().alias('total_profit'),\n",
    "    # Average price per unit = sales / quantity\n",
    "    (pl.col('sales') / pl.col('quantity')).mean().alias('avg_price_per_unit'),\n",
    "    # Profit margin = (sales - cost) / sales\n",
    "    ((pl.col('sales') - pl.col('cost')) / pl.col('sales') * 100).mean().alias('avg_profit_margin_pct')\n",
    "])\n",
    "\n",
    "print(\"Aggregating expressions:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles and percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentiles\n",
    "result = df.group_by('region').agg([\n",
    "    pl.col('sales').quantile(0.25).alias('p25'),\n",
    "    pl.col('sales').quantile(0.50).alias('p50_median'),\n",
    "    pl.col('sales').quantile(0.75).alias('p75'),\n",
    "    pl.col('sales').quantile(0.90).alias('p90')\n",
    "])\n",
    "\n",
    "print(\"Percentiles:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List aggregation (collect values into lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values into lists\n",
    "result = df.group_by('region').agg([\n",
    "    pl.col('product').alias('all_products'),  # Creates list of all products\n",
    "    pl.col('sales').alias('all_sales'),       # Creates list of all sales\n",
    "    pl.col('sales').sum().alias('total_sales')\n",
    "])\n",
    "\n",
    "print(\"List aggregation:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Maintaining Row Order with maintain_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without maintain_order (order may change)\n",
    "result1 = df.group_by('product').agg(pl.col('sales').sum())\n",
    "print(\"Without maintain_order:\")\n",
    "print(result1)\n",
    "\n",
    "# With maintain_order (preserves first occurrence order)\n",
    "result2 = df.group_by('product', maintain_order=True).agg(pl.col('sales').sum())\n",
    "print(\"\\nWith maintain_order:\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Multiple Aggregations on Same Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full statistics for sales column\n",
    "result = df.group_by('product').agg([\n",
    "    pl.col('sales').min().alias('min_sales'),\n",
    "    pl.col('sales').quantile(0.25).alias('q1_sales'),\n",
    "    pl.col('sales').median().alias('median_sales'),\n",
    "    pl.col('sales').quantile(0.75).alias('q3_sales'),\n",
    "    pl.col('sales').max().alias('max_sales'),\n",
    "    pl.col('sales').mean().alias('mean_sales'),\n",
    "    pl.col('sales').std().alias('std_sales'),\n",
    "    pl.len().alias('count')\n",
    "])\n",
    "\n",
    "print(\"Full statistics:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Complex Real-World Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more realistic sales data\n",
    "np.random.seed(42)\n",
    "dates = pl.date_range(pl.date(2023, 1, 1), pl.date(2023, 3, 31), '1d', eager=True)\n",
    "\n",
    "sales_data = pl.DataFrame({\n",
    "    'date': np.repeat(dates, 3),\n",
    "    'product': np.tile(['Laptop', 'Mouse', 'Keyboard'], len(dates)),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], len(dates) * 3),\n",
    "    'sales_amount': np.random.uniform(100, 2000, len(dates) * 3),\n",
    "    'quantity': np.random.randint(1, 20, len(dates) * 3),\n",
    "}).with_columns([\n",
    "    pl.col('date').dt.month().alias('month'),\n",
    "    pl.col('date').dt.weekday().alias('weekday')\n",
    "])\n",
    "\n",
    "print(f\"Sales data: {len(sales_data)} rows\")\n",
    "print(sales_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Monthly product performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_performance = sales_data.group_by(['month', 'product']).agg([\n",
    "    pl.col('sales_amount').sum().alias('total_revenue'),\n",
    "    pl.col('quantity').sum().alias('units_sold'),\n",
    "    (pl.col('sales_amount').sum() / pl.col('quantity').sum()).alias('avg_price_per_unit'),\n",
    "    pl.col('sales_amount').mean().alias('avg_transaction'),\n",
    "    pl.len().alias('num_transactions'),\n",
    "    pl.col('region').n_unique().alias('regions_covered')\n",
    "]).sort(['month', 'product'])\n",
    "\n",
    "print(\"Monthly product performance:\")\n",
    "print(monthly_performance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Regional analysis with rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_analysis = (\n",
    "    sales_data\n",
    "    .group_by('region')\n",
    "    .agg([\n",
    "        pl.col('sales_amount').sum().alias('total_revenue'),\n",
    "        pl.col('sales_amount').mean().alias('avg_transaction'),\n",
    "        pl.len().alias('num_transactions'),\n",
    "        pl.col('product').n_unique().alias('unique_products')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col('total_revenue').rank(descending=True).alias('revenue_rank'),\n",
    "        (pl.col('total_revenue') / pl.col('total_revenue').sum() * 100).alias('revenue_share_pct')\n",
    "    ])\n",
    "    .sort('revenue_rank')\n",
    ")\n",
    "\n",
    "print(\"Regional analysis with rankings:\")\n",
    "print(regional_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Weekday vs Weekend analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weekend flag\n",
    "weekday_analysis = (\n",
    "    sales_data\n",
    "    .with_columns([\n",
    "        pl.when(pl.col('weekday').is_in([5, 6]))\n",
    "          .then(pl.lit('Weekend'))\n",
    "          .otherwise(pl.lit('Weekday'))\n",
    "          .alias('day_type')\n",
    "    ])\n",
    "    .group_by(['product', 'day_type'])\n",
    "    .agg([\n",
    "        pl.col('sales_amount').sum().alias('total_sales'),\n",
    "        pl.col('sales_amount').mean().alias('avg_sale'),\n",
    "        pl.len().alias('num_transactions')\n",
    "    ])\n",
    "    .sort(['product', 'day_type'])\n",
    ")\n",
    "\n",
    "print(\"Weekday vs Weekend:\")\n",
    "print(weekday_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Rolling Group By (Time-based Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily sales for one product\n",
    "laptop_sales = (\n",
    "    sales_data\n",
    "    .filter(pl.col('product') == 'Laptop')\n",
    "    .group_by('date')\n",
    "    .agg(pl.col('sales_amount').sum().alias('daily_sales'))\n",
    "    .sort('date')\n",
    ")\n",
    "\n",
    "print(\"Daily laptop sales (first 10 days):\")\n",
    "print(laptop_sales.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling 7-day average\n",
    "rolling_analysis = laptop_sales.with_columns([\n",
    "    pl.col('daily_sales').rolling_mean(window_size=7).alias('7day_avg'),\n",
    "    pl.col('daily_sales').rolling_sum(window_size=7).alias('7day_sum'),\n",
    "    pl.col('daily_sales').rolling_max(window_size=7).alias('7day_max')\n",
    "])\n",
    "\n",
    "print(\"\\nRolling 7-day analysis:\")\n",
    "print(rolling_analysis.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Dynamic Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by time windows (e.g., weekly aggregation)\n",
    "weekly_sales = (\n",
    "    sales_data\n",
    "    .sort('date')\n",
    "    .group_by_dynamic('date', every='1w', by='product')\n",
    "    .agg([\n",
    "        pl.col('sales_amount').sum().alias('weekly_sales'),\n",
    "        pl.col('quantity').sum().alias('weekly_quantity')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"Weekly sales by product:\")\n",
    "print(weekly_sales.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Top N per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 3 sales per region\n",
    "top_sales_per_region = (\n",
    "    sales_data\n",
    "    .sort('sales_amount', descending=True)\n",
    "    .group_by('region', maintain_order=True)\n",
    "    .agg([\n",
    "        pl.col('date').head(3).alias('top_dates'),\n",
    "        pl.col('product').head(3).alias('top_products'),\n",
    "        pl.col('sales_amount').head(3).alias('top_amounts')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"Top 3 sales per region:\")\n",
    "print(top_sales_per_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Ratio to group total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate each transaction's % of regional total\n",
    "with_pct = (\n",
    "    sales_data\n",
    "    .with_columns([\n",
    "        (pl.col('sales_amount') / pl.col('sales_amount').sum().over('region') * 100)\n",
    "        .alias('pct_of_region_total')\n",
    "    ])\n",
    "    .select(['date', 'region', 'product', 'sales_amount', 'pct_of_region_total'])\n",
    "    .sort('pct_of_region_total', descending=True)\n",
    ")\n",
    "\n",
    "print(\"Top transactions by % of regional total:\")\n",
    "print(with_pct.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Multiple groupings with different aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple summary views\n",
    "by_product = sales_data.group_by('product').agg([\n",
    "    pl.col('sales_amount').sum().alias('total')\n",
    "])\n",
    "\n",
    "by_region = sales_data.group_by('region').agg([\n",
    "    pl.col('sales_amount').sum().alias('total')\n",
    "])\n",
    "\n",
    "by_month = sales_data.group_by('month').agg([\n",
    "    pl.col('sales_amount').sum().alias('total')\n",
    "])\n",
    "\n",
    "print(\"By Product:\")\n",
    "print(by_product)\n",
    "print(\"\\nBy Region:\")\n",
    "print(by_region)\n",
    "print(\"\\nBy Month:\")\n",
    "print(by_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 4: Aggregating multiple columns with same function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum multiple numeric columns at once\n",
    "result = sales_data.group_by('product').agg([\n",
    "    pl.col('sales_amount', 'quantity').sum().name.suffix('_sum')\n",
    "])\n",
    "\n",
    "print(\"Sum multiple columns:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Performance Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large dataset\n",
    "large_df = pl.DataFrame({\n",
    "    'group': np.random.choice(['A', 'B', 'C', 'D', 'E'], 1_000_000),\n",
    "    'value1': np.random.randn(1_000_000),\n",
    "    'value2': np.random.randn(1_000_000),\n",
    "    'value3': np.random.randn(1_000_000)\n",
    "})\n",
    "\n",
    "print(f\"Large dataset: {len(large_df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine aggregations in single pass\n",
    "start = time.time()\n",
    "result = large_df.group_by('group').agg([\n",
    "    pl.col('value1').mean(),\n",
    "    pl.col('value2').sum(),\n",
    "    pl.col('value3').std()\n",
    "])\n",
    "time1 = time.time() - start\n",
    "print(f\"Single group_by: {time1:.4f}s\")\n",
    "\n",
    "# Multiple separate group_bys (SLOWER)\n",
    "start = time.time()\n",
    "r1 = large_df.group_by('group').agg(pl.col('value1').mean())\n",
    "r2 = large_df.group_by('group').agg(pl.col('value2').sum())\n",
    "r3 = large_df.group_by('group').agg(pl.col('value3').std())\n",
    "time2 = time.time() - start\n",
    "print(f\"Multiple group_bys: {time2:.4f}s\")\n",
    "print(f\"\\nSingle pass is {time2/time1:.2f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy evaluation with group_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use lazy for complex pipelines\n",
    "start = time.time()\n",
    "lazy_result = (\n",
    "    large_df.lazy()\n",
    "    .filter(pl.col('value1') > 0)\n",
    "    .group_by('group')\n",
    "    .agg([\n",
    "        pl.col('value1').mean(),\n",
    "        pl.col('value2').sum()\n",
    "    ])\n",
    "    .sort('group')\n",
    "    .collect()\n",
    ")\n",
    "lazy_time = time.time() - start\n",
    "\n",
    "print(f\"Lazy execution: {lazy_time:.4f}s\")\n",
    "print(\"Lazy allows filter pushdown before grouping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts:\n",
    "1. **group_by()** splits data into groups for aggregation\n",
    "2. **agg()** applies aggregation expressions to each group\n",
    "3. Multiple aggregations can be performed in a single pass\n",
    "4. **Conditional aggregations** using filter() within agg()\n",
    "5. **Rolling** and **dynamic** group_by for time-series data\n",
    "6. Use **maintain_order** to preserve group order\n",
    "\n",
    "### Common Aggregations:\n",
    "- **Statistical**: sum, mean, median, std, var, min, max\n",
    "- **Counting**: len, n_unique, count\n",
    "- **Positional**: first, last, head, tail\n",
    "- **Quantiles**: quantile()\n",
    "- **Lists**: collect values into lists (default behavior)\n",
    "\n",
    "### Best Practices:\n",
    "- Combine multiple aggregations in single group_by\n",
    "- Use lazy evaluation for complex pipelines\n",
    "- Use conditional aggregations instead of multiple group_bys\n",
    "- Consider rolling/dynamic group_by for time-series\n",
    "- Use over() for window functions (see Window Functions notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
