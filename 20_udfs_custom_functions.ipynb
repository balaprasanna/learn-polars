{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars UDFs & Custom Functions - Comprehensive Workshop\n",
    "\n",
    "Learn how to extend Polars with custom Python functions when built-in expressions aren't enough.\n",
    "\n",
    "## What You'll Learn:\n",
    "- When to use (and avoid) UDFs\n",
    "- map_elements() for row-wise operations\n",
    "- map_batches() for vectorized operations\n",
    "- Custom aggregation functions\n",
    "- Plugin system for performance-critical functions\n",
    "- Performance optimization strategies\n",
    "- Real-world use cases\n",
    "\n",
    "## ⚠️ Important:\n",
    "UDFs are **much slower** than native Polars expressions. Always try to use built-in expressions first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: When to Use UDFs\n",
    "\n",
    "## ✅ Good Use Cases:\n",
    "- Complex business logic not available in Polars\n",
    "- Calling external APIs or libraries\n",
    "- Custom domain-specific calculations\n",
    "- Integrating with existing Python functions\n",
    "\n",
    "## ❌ Avoid UDFs For:\n",
    "- Math operations (use expressions)\n",
    "- String manipulation (use .str namespace)\n",
    "- Date operations (use .dt namespace)\n",
    "- Aggregations (use .agg())\n",
    "- Conditional logic (use when/then/otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "df = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, 34, 28, 42, 31],\n",
    "    'salary': [50000, 75000, 60000, 95000, 68000],\n",
    "    'email': ['alice@example.com', 'bob@test.org', 'charlie@email.com', 'diana@example.org', 'eve@test.com']\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bad Example: DON'T use UDF for simple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ BAD: Using UDF for simple math\n",
    "def double_salary_bad(salary):\n",
    "    return salary * 2\n",
    "\n",
    "result_bad = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('salary').map_elements(double_salary_bad, return_dtype=pl.Int64).alias('doubled_salary_bad')\n",
    "])\n",
    "\n",
    "print(\"❌ Bad approach (UDF):\")\n",
    "print(result_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ GOOD: Use native expression\n",
    "result_good = df.select([\n",
    "    pl.col('name'),\n",
    "    (pl.col('salary') * 2).alias('doubled_salary_good')\n",
    "])\n",
    "\n",
    "print(\"✅ Good approach (native expression):\")\n",
    "print(result_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "large_df = pl.DataFrame({\n",
    "    'value': range(100000)\n",
    "})\n",
    "\n",
    "# UDF approach\n",
    "start = time.time()\n",
    "result1 = large_df.select(pl.col('value').map_elements(lambda x: x * 2, return_dtype=pl.Int64))\n",
    "time_udf = time.time() - start\n",
    "\n",
    "# Expression approach\n",
    "start = time.time()\n",
    "result2 = large_df.select(pl.col('value') * 2)\n",
    "time_expr = time.time() - start\n",
    "\n",
    "print(f\"UDF approach: {time_udf:.4f}s\")\n",
    "print(f\"Expression approach: {time_expr:.4f}s\")\n",
    "print(f\"\\nExpression is {time_udf/time_expr:.0f}x faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: map_elements() - Element-wise Operations\n",
    "\n",
    "Applies a Python function to each element in a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Basic map_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom email validation (complex logic not in Polars)\n",
    "def classify_email_domain(email):\n",
    "    \"\"\"Classify email by domain type\"\"\"\n",
    "    domain = email.split('@')[1]\n",
    "    \n",
    "    if domain.endswith('.com'):\n",
    "        return 'commercial'\n",
    "    elif domain.endswith('.org'):\n",
    "        return 'organization'\n",
    "    elif domain.endswith('.edu'):\n",
    "        return 'education'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "result = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('email'),\n",
    "    pl.col('email').map_elements(classify_email_domain, return_dtype=pl.String).alias('domain_type')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multiple Input Columns with struct.map_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that needs multiple columns\n",
    "def calculate_bonus(row):\n",
    "    \"\"\"Calculate bonus based on age and salary\"\"\"\n",
    "    age = row['age']\n",
    "    salary = row['salary']\n",
    "    \n",
    "    if age > 40:\n",
    "        return salary * 0.15  # 15% bonus for senior employees\n",
    "    elif age > 30:\n",
    "        return salary * 0.10  # 10% bonus\n",
    "    else:\n",
    "        return salary * 0.05  # 5% bonus\n",
    "\n",
    "result = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('age'),\n",
    "    pl.col('salary'),\n",
    "    pl.struct(['age', 'salary']).map_elements(calculate_bonus, return_dtype=pl.Float64).alias('bonus')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Return Complex Types from UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a tuple (will become a struct)\n",
    "def analyze_name(name):\n",
    "    \"\"\"Return multiple values as struct\"\"\"\n",
    "    return {\n",
    "        'length': len(name),\n",
    "        'uppercase': name.upper(),\n",
    "        'has_vowels': any(v in name.lower() for v in 'aeiou'),\n",
    "        'first_letter': name[0]\n",
    "    }\n",
    "\n",
    "result = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('name').map_elements(analyze_name, return_dtype=pl.Struct({\n",
    "        'length': pl.Int64,\n",
    "        'uppercase': pl.String,\n",
    "        'has_vowels': pl.Boolean,\n",
    "        'first_letter': pl.String\n",
    "    })).alias('name_analysis')\n",
    "])\n",
    "\n",
    "print(result)\n",
    "print(\"\\nUnnested:\")\n",
    "print(result.unnest('name_analysis'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Return Lists from UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list\n",
    "def generate_salary_range(salary):\n",
    "    \"\"\"Generate salary range (min, median, max) based on base salary\"\"\"\n",
    "    return [int(salary * 0.8), int(salary), int(salary * 1.2)]\n",
    "\n",
    "result = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('salary'),\n",
    "    pl.col('salary').map_elements(\n",
    "        generate_salary_range, \n",
    "        return_dtype=pl.List(pl.Int64)\n",
    "    ).alias('salary_range')\n",
    "])\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Extract from list\n",
    "result_expanded = result.with_columns([\n",
    "    pl.col('salary_range').list.get(0).alias('min_salary'),\n",
    "    pl.col('salary_range').list.get(1).alias('median_salary'),\n",
    "    pl.col('salary_range').list.get(2).alias('max_salary')\n",
    "])\n",
    "\n",
    "print(\"\\nExpanded:\")\n",
    "print(result_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: map_batches() - Vectorized Operations\n",
    "\n",
    "`map_batches()` is faster than `map_elements()` because it processes entire Series at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Basic map_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that operates on entire Series (using NumPy)\n",
    "def normalize_series(series):\n",
    "    \"\"\"Z-score normalization using NumPy\"\"\"\n",
    "    arr = series.to_numpy()\n",
    "    normalized = (arr - arr.mean()) / arr.std()\n",
    "    return pl.Series(normalized)\n",
    "\n",
    "result = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('salary'),\n",
    "    pl.col('salary').map_batches(normalize_series).alias('normalized_salary')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Performance: map_elements vs map_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset\n",
    "large_df = pl.DataFrame({\n",
    "    'value': np.random.randint(1, 100, 50000)\n",
    "})\n",
    "\n",
    "# Element-wise function\n",
    "def square_root_element(x):\n",
    "    return x ** 0.5\n",
    "\n",
    "# Batch function\n",
    "def square_root_batch(series):\n",
    "    return pl.Series(np.sqrt(series.to_numpy()))\n",
    "\n",
    "# Benchmark map_elements\n",
    "start = time.time()\n",
    "result1 = large_df.select(\n",
    "    pl.col('value').map_elements(square_root_element, return_dtype=pl.Float64)\n",
    ")\n",
    "time_elements = time.time() - start\n",
    "\n",
    "# Benchmark map_batches\n",
    "start = time.time()\n",
    "result2 = large_df.select(\n",
    "    pl.col('value').map_batches(square_root_batch)\n",
    ")\n",
    "time_batches = time.time() - start\n",
    "\n",
    "# Benchmark native expression\n",
    "start = time.time()\n",
    "result3 = large_df.select(\n",
    "    pl.col('value').sqrt()\n",
    ")\n",
    "time_native = time.time() - start\n",
    "\n",
    "print(f\"map_elements: {time_elements:.4f}s\")\n",
    "print(f\"map_batches:  {time_batches:.4f}s\")\n",
    "print(f\"Native expr:  {time_native:.4f}s\")\n",
    "print(f\"\\nmap_batches is {time_elements/time_batches:.1f}x faster than map_elements\")\n",
    "print(f\"Native expr is {time_batches/time_native:.1f}x faster than map_batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using External Libraries with map_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using scipy for advanced statistics\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_percentile_rank(series):\n",
    "    \"\"\"Calculate percentile rank using scipy\"\"\"\n",
    "    arr = series.to_numpy()\n",
    "    ranks = stats.rankdata(arr, method='average')\n",
    "    percentiles = (ranks / len(ranks)) * 100\n",
    "    return pl.Series(percentiles)\n",
    "\n",
    "result = df.select([\n",
    "    pl.col('name'),\n",
    "    pl.col('salary'),\n",
    "    pl.col('salary').map_batches(calculate_percentile_rank).alias('salary_percentile')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Custom Aggregation Functions\n",
    "\n",
    "Use UDFs in group_by aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with groups\n",
    "df_groups = pl.DataFrame({\n",
    "    'department': ['Sales', 'Sales', 'Sales', 'Engineering', 'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "    'employee': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'salary': [50000, 55000, 52000, 75000, 80000, 78000, 45000, 47000],\n",
    "    'years': [2, 5, 3, 7, 10, 8, 3, 4]\n",
    "})\n",
    "\n",
    "print(df_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Custom Aggregation with map_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation: coefficient of variation\n",
    "def coefficient_of_variation(series):\n",
    "    \"\"\"CV = (std / mean) * 100\"\"\"\n",
    "    arr = series.to_numpy()\n",
    "    cv = (arr.std() / arr.mean()) * 100\n",
    "    return cv\n",
    "\n",
    "result = df_groups.group_by('department').agg([\n",
    "    pl.col('salary').mean().alias('avg_salary'),\n",
    "    pl.col('salary').std().alias('std_salary'),\n",
    "    pl.col('salary').map_batches(lambda s: coefficient_of_variation(s), return_dtype=pl.Float64).alias('salary_cv'),\n",
    "    pl.col('employee').count().alias('num_employees')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Complex Multi-Column Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that needs multiple columns in aggregation\n",
    "def calculate_salary_per_year(df_slice):\n",
    "    \"\"\"Calculate average salary per year of experience\"\"\"\n",
    "    total_salary = df_slice['salary'].sum()\n",
    "    total_years = df_slice['years'].sum()\n",
    "    return total_salary / total_years if total_years > 0 else 0\n",
    "\n",
    "result = df_groups.group_by('department').agg([\n",
    "    pl.col('salary').mean().alias('avg_salary'),\n",
    "    pl.col('years').mean().alias('avg_years'),\n",
    "    pl.struct(['salary', 'years']).map_batches(\n",
    "        lambda s: calculate_salary_per_year(s.struct.unnest()),\n",
    "        return_dtype=pl.Float64\n",
    "    ).alias('salary_per_year')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Real-World Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Example: Hash Generation for Data Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash PII data for privacy\n",
    "def hash_string(text):\n",
    "    \"\"\"Create SHA256 hash of string\"\"\"\n",
    "    return hashlib.sha256(text.encode()).hexdigest()[:16]  # First 16 chars\n",
    "\n",
    "df_pii = pl.DataFrame({\n",
    "    'user_id': [1, 2, 3],\n",
    "    'email': ['alice@example.com', 'bob@test.com', 'charlie@email.com'],\n",
    "    'ssn': ['123-45-6789', '987-65-4321', '555-55-5555']\n",
    "})\n",
    "\n",
    "result = df_pii.select([\n",
    "    pl.col('user_id'),\n",
    "    pl.col('email').map_elements(hash_string, return_dtype=pl.String).alias('email_hash'),\n",
    "    pl.col('ssn').map_elements(hash_string, return_dtype=pl.String).alias('ssn_hash')\n",
    "])\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_pii)\n",
    "print(\"\\nHashed:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Example: Geocoding with External API (Mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock geocoding function (in reality, would call an API)\n",
    "def mock_geocode(city):\n",
    "    \"\"\"Mock geocoding - returns fake lat/lon\"\"\"\n",
    "    # In reality: requests.get(f'https://api.geocode.com?city={city}')\n",
    "    geocode_map = {\n",
    "        'NYC': {'lat': 40.7128, 'lon': -74.0060},\n",
    "        'LA': {'lat': 34.0522, 'lon': -118.2437},\n",
    "        'Chicago': {'lat': 41.8781, 'lon': -87.6298},\n",
    "        'Boston': {'lat': 42.3601, 'lon': -71.0589}\n",
    "    }\n",
    "    return geocode_map.get(city, {'lat': 0.0, 'lon': 0.0})\n",
    "\n",
    "df_locations = pl.DataFrame({\n",
    "    'store_id': [1, 2, 3, 4],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'Boston']\n",
    "})\n",
    "\n",
    "result = df_locations.select([\n",
    "    pl.col('store_id'),\n",
    "    pl.col('city'),\n",
    "    pl.col('city').map_elements(\n",
    "        mock_geocode,\n",
    "        return_dtype=pl.Struct({'lat': pl.Float64, 'lon': pl.Float64})\n",
    "    ).alias('coordinates')\n",
    "]).unnest('coordinates')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Example: Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment analysis (mock - in reality use NLTK, spaCy, or transformers)\n",
    "def simple_sentiment(text):\n",
    "    \"\"\"Very simple sentiment analysis based on keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    positive_words = ['good', 'great', 'excellent', 'love', 'amazing', 'best']\n",
    "    negative_words = ['bad', 'terrible', 'hate', 'worst', 'awful', 'poor']\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_words if word in text_lower)\n",
    "    neg_count = sum(1 for word in negative_words if word in text_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 'positive'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df_reviews = pl.DataFrame({\n",
    "    'review_id': [1, 2, 3, 4, 5],\n",
    "    'text': [\n",
    "        'This product is great! I love it.',\n",
    "        'Terrible experience, worst purchase ever.',\n",
    "        'It is okay, nothing special.',\n",
    "        'Amazing quality, best in class.',\n",
    "        'Bad quality, would not recommend.'\n",
    "    ]\n",
    "})\n",
    "\n",
    "result = df_reviews.select([\n",
    "    pl.col('review_id'),\n",
    "    pl.col('text'),\n",
    "    pl.col('text').map_elements(simple_sentiment, return_dtype=pl.String).alias('sentiment')\n",
    "])\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Count sentiments\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(result.group_by('sentiment').agg(pl.len().alias('count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Example: Custom Date Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate business days between dates (excluding weekends)\n",
    "def business_days_until_deadline(row):\n",
    "    \"\"\"Calculate business days from today to deadline\"\"\"\n",
    "    from datetime import timedelta\n",
    "    \n",
    "    today = row['today']\n",
    "    deadline = row['deadline']\n",
    "    \n",
    "    business_days = 0\n",
    "    current_date = today\n",
    "    \n",
    "    while current_date < deadline:\n",
    "        if current_date.weekday() < 5:  # Monday=0, Friday=4\n",
    "            business_days += 1\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return business_days\n",
    "\n",
    "df_deadlines = pl.DataFrame({\n",
    "    'task': ['Report', 'Presentation', 'Review', 'Analysis'],\n",
    "    'today': [date(2024, 1, 15)] * 4,  # Monday\n",
    "    'deadline': [\n",
    "        date(2024, 1, 19),  # Friday\n",
    "        date(2024, 1, 22),  # Monday (next week)\n",
    "        date(2024, 1, 17),  # Wednesday\n",
    "        date(2024, 1, 26)   # Friday (next week)\n",
    "    ]\n",
    "})\n",
    "\n",
    "result = df_deadlines.select([\n",
    "    pl.col('task'),\n",
    "    pl.col('today'),\n",
    "    pl.col('deadline'),\n",
    "    pl.struct(['today', 'deadline']).map_elements(\n",
    "        business_days_until_deadline,\n",
    "        return_dtype=pl.Int64\n",
    "    ).alias('business_days')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Error Handling in UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF with error handling\n",
    "def safe_divide(row):\n",
    "    \"\"\"Safely divide two numbers\"\"\"\n",
    "    try:\n",
    "        numerator = row['numerator']\n",
    "        denominator = row['denominator']\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return None  # Return null for division by zero\n",
    "        \n",
    "        return numerator / denominator\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "df_division = pl.DataFrame({\n",
    "    'numerator': [10, 20, 30, 40],\n",
    "    'denominator': [2, 0, 5, 0]  # Some zeros!\n",
    "})\n",
    "\n",
    "result = df_division.select([\n",
    "    pl.col('numerator'),\n",
    "    pl.col('denominator'),\n",
    "    pl.struct(['numerator', 'denominator']).map_elements(\n",
    "        safe_divide,\n",
    "        return_dtype=pl.Float64\n",
    "    ).alias('result')\n",
    "])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Lazy Evaluation and UDFs\n",
    "\n",
    "UDFs work with lazy evaluation, but may prevent some query optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF in lazy context\n",
    "def custom_transform(x):\n",
    "    return x * 2 + 10\n",
    "\n",
    "lazy_df = pl.LazyFrame({\n",
    "    'id': range(1, 6),\n",
    "    'value': [10, 20, 30, 40, 50]\n",
    "})\n",
    "\n",
    "lazy_result = (\n",
    "    lazy_df\n",
    "    .filter(pl.col('value') > 20)\n",
    "    .select([\n",
    "        pl.col('id'),\n",
    "        pl.col('value'),\n",
    "        pl.col('value').map_elements(custom_transform, return_dtype=pl.Int64).alias('transformed')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"Lazy query plan:\")\n",
    "print(lazy_result.explain())\n",
    "\n",
    "print(\"\\nCollected result:\")\n",
    "print(lazy_result.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Best Practices & Optimization Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Prefer Native Expressions\n",
    "\n",
    "| Task | ❌ UDF | ✅ Native |\n",
    "|------|--------|----------|\n",
    "| Math | `map_elements(lambda x: x*2)` | `pl.col('x') * 2` |\n",
    "| String | `map_elements(lambda s: s.upper())` | `pl.col('s').str.to_uppercase()` |\n",
    "| Date | `map_elements(lambda d: d.year)` | `pl.col('d').dt.year()` |\n",
    "| Conditional | `map_elements(lambda x: 'high' if x > 10 else 'low')` | `pl.when(...).then(...).otherwise(...)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Use map_batches() When Possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you must use a UDF, prefer map_batches\n",
    "\n",
    "# ❌ Slower: map_elements\n",
    "def transform_element(x):\n",
    "    return (x ** 2 + x ** 0.5) / 2\n",
    "\n",
    "# ✅ Faster: map_batches with NumPy\n",
    "def transform_batch(series):\n",
    "    arr = series.to_numpy()\n",
    "    result = (arr ** 2 + arr ** 0.5) / 2\n",
    "    return pl.Series(result)\n",
    "\n",
    "test_df = pl.DataFrame({'x': range(10000)})\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "result1 = test_df.select(pl.col('x').map_elements(transform_element, return_dtype=pl.Float64))\n",
    "time1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result2 = test_df.select(pl.col('x').map_batches(transform_batch))\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"map_elements: {time1:.4f}s\")\n",
    "print(f\"map_batches:  {time2:.4f}s\")\n",
    "print(f\"\\nmap_batches is {time1/time2:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Specify return_dtype Explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always specify return_dtype for better performance and type safety\n",
    "\n",
    "# ❌ Bad: Type inference can be slow\n",
    "# result = df.select(pl.col('x').map_elements(my_func))\n",
    "\n",
    "# ✅ Good: Explicit dtype\n",
    "def double_value(x):\n",
    "    return x * 2\n",
    "\n",
    "result = df.select(\n",
    "    pl.col('salary').map_elements(double_value, return_dtype=pl.Int64).alias('doubled')\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Cache Expensive Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If UDF is expensive, cache results\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def expensive_computation(value):\n",
    "    \"\"\"Simulate expensive operation\"\"\"\n",
    "    time.sleep(0.001)  # Simulate API call or complex calc\n",
    "    return value ** 2\n",
    "\n",
    "df_cached = pl.DataFrame({\n",
    "    'value': [1, 2, 3, 1, 2, 3, 1, 2, 3]  # Repeated values\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "result = df_cached.select(\n",
    "    pl.col('value').map_elements(expensive_computation, return_dtype=pl.Int64).alias('result')\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(result)\n",
    "print(f\"\\nTime with caching: {elapsed:.4f}s\")\n",
    "print(\"Note: Repeated values are computed only once!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "### 1. **Always Prefer Native Expressions**\n",
    "   - 10-100x faster than UDFs\n",
    "   - Better parallelization\n",
    "   - Query optimization possible\n",
    "\n",
    "### 2. **When to Use UDFs**\n",
    "   - ✅ External API calls\n",
    "   - ✅ Complex business logic\n",
    "   - ✅ Third-party libraries (scipy, sklearn, etc.)\n",
    "   - ✅ Domain-specific calculations\n",
    "   - ❌ Simple math, string, or date operations\n",
    "\n",
    "### 3. **Performance Hierarchy** (fastest to slowest)\n",
    "   1. Native Polars expressions (FASTEST)\n",
    "   2. `map_batches()` with NumPy (FAST)\n",
    "   3. `map_elements()` (SLOW)\n",
    "   4. Python apply/map (SLOWEST - don't use)\n",
    "\n",
    "### 4. **Best Practices**\n",
    "   - Always specify `return_dtype`\n",
    "   - Use `map_batches()` over `map_elements()` when possible\n",
    "   - Leverage NumPy in batch operations\n",
    "   - Add error handling in UDFs\n",
    "   - Cache expensive computations\n",
    "   - Use `struct` for multi-column inputs\n",
    "\n",
    "### 5. **Common Patterns**\n",
    "\n",
    "```python\n",
    "# Single column UDF\n",
    "pl.col('x').map_elements(my_func, return_dtype=pl.Float64)\n",
    "\n",
    "# Multi-column UDF\n",
    "pl.struct(['x', 'y']).map_elements(my_func, return_dtype=pl.Float64)\n",
    "\n",
    "# Vectorized UDF\n",
    "pl.col('x').map_batches(my_batch_func)\n",
    "\n",
    "# Return complex types\n",
    "pl.col('x').map_elements(my_func, return_dtype=pl.List(pl.Int64))\n",
    "pl.col('x').map_elements(my_func, return_dtype=pl.Struct({'a': pl.Int64, 'b': pl.String}))\n",
    "```\n",
    "\n",
    "## Remember:\n",
    "> **UDFs are a last resort. Always try native expressions first!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise data\n",
    "exercise_df = pl.DataFrame({\n",
    "    'product_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "    'price': [1200, 25, 75, 300, 150],\n",
    "    'category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Accessories'],\n",
    "    'description': [\n",
    "        'High performance laptop',\n",
    "        'Wireless mouse with great battery',\n",
    "        'Mechanical keyboard for gaming',\n",
    "        '4K monitor with amazing colors',\n",
    "        'Noise cancelling headphones'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(exercise_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a UDF that classifies products by price range\n",
    "# Budget: < 50, Mid-range: 50-200, Premium: > 200\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a UDF that counts vowels in the product name\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Create a UDF that generates a product code\n",
    "# Format: first 3 letters of category + price rounded to nearest 10\n",
    "# Example: \"Electronics\" + 1200 -> \"ELE1200\"\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Create a batch UDF that normalizes prices (z-score)\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Create a UDF that returns struct with word count and char count\n",
    "# Your code here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
