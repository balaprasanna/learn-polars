{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Notebook 19: Performance Optimization and Debugging\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers performance optimization and debugging techniques in Polars. You'll learn how to profile queries, identify bottlenecks, optimize memory usage, and leverage parallel processing for maximum performance.\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. **Query Profiling and Analysis**\n",
    "   - Using explain() for query plans\n",
    "   - Understanding execution strategies\n",
    "   - Profiling query execution\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Memory profiling techniques\n",
    "   - Streaming for large datasets\n",
    "   - Memory-efficient operations\n",
    "\n",
    "3. **Lazy Evaluation Optimization**\n",
    "   - Query optimization strategies\n",
    "   - Predicate pushdown\n",
    "   - Projection pushdown\n",
    "\n",
    "4. **Parallel Processing**\n",
    "   - Thread pool configuration\n",
    "   - Parallel operations\n",
    "   - Scaling strategies\n",
    "\n",
    "5. **Performance Pitfalls**\n",
    "   - Common anti-patterns\n",
    "   - Expensive operations to avoid\n",
    "   - Best practices\n",
    "\n",
    "6. **Benchmarking**\n",
    "   - Timing comparisons\n",
    "   - Performance testing\n",
    "   - Regression detection\n",
    "\n",
    "7. **Real-World Optimization**\n",
    "   - Complete optimization workflow\n",
    "   - Before/after comparisons\n",
    "   - Production tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import psutil  # For memory profiling\n",
    "import os\n",
    "\n",
    "# Configure Polars for optimal performance\n",
    "pl.Config.set_fmt_str_lengths(50)\n",
    "pl.Config.set_tbl_rows(10)\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-intro",
   "metadata": {},
   "source": [
    "## Part 1: Query Profiling and Analysis\n",
    "\n",
    "### 1.1 Understanding Query Plans with explain()\n",
    "\n",
    "The `explain()` method shows how Polars will execute your query, including optimizations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "df = pl.DataFrame({\n",
    "    'customer_id': range(1, 10001),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 10000),\n",
    "    'product': np.random.choice(['A', 'B', 'C', 'D'], 10000),\n",
    "    'sales': np.random.randint(100, 10000, 10000),\n",
    "    'quantity': np.random.randint(1, 100, 10000),\n",
    "    'date': [datetime(2024, 1, 1) + timedelta(days=i) for i in range(10000)]\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain-lazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lazy query\n",
    "lazy_query = (\n",
    "    df.lazy()\n",
    "    .filter(pl.col('region') == 'North')\n",
    "    .filter(pl.col('sales') > 5000)\n",
    "    .select(['customer_id', 'product', 'sales'])\n",
    "    .group_by('product')\n",
    "    .agg([\n",
    "        pl.col('sales').sum().alias('total_sales'),\n",
    "        pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Show the query plan\n",
    "print(\"Query Plan:\")\n",
    "print(lazy_query.explain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain-optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show optimized query plan\n",
    "print(\"Optimized Query Plan:\")\n",
    "print(lazy_query.explain(optimized=True))\n",
    "\n",
    "# Notice optimizations like:\n",
    "# - Filter pushdown (filters applied early)\n",
    "# - Projection pushdown (only needed columns selected)\n",
    "# - Predicate combination (multiple filters combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "profiling",
   "metadata": {},
   "source": [
    "### 1.2 Profiling Query Execution\n",
    "\n",
    "Use `profile()` to get detailed timing information about query execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profile-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the query execution\n",
    "result, profile = lazy_query.profile()\n",
    "\n",
    "print(\"Query Result:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\nProfile Information:\")\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profile-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile a more complex query\n",
    "complex_query = (\n",
    "    df.lazy()\n",
    "    .with_columns([\n",
    "        (pl.col('sales') / pl.col('quantity')).alias('price_per_unit'),\n",
    "        pl.col('date').dt.month().alias('month')\n",
    "    ])\n",
    "    .filter(pl.col('price_per_unit') > 50)\n",
    "    .group_by(['region', 'month'])\n",
    "    .agg([\n",
    "        pl.col('sales').sum().alias('total_sales'),\n",
    "        pl.col('sales').mean().alias('avg_sales'),\n",
    "        pl.col('customer_id').n_unique().alias('customers'),\n",
    "        pl.col('quantity').sum().alias('total_quantity')\n",
    "    ])\n",
    "    .sort(['region', 'month'])\n",
    ")\n",
    "\n",
    "result, profile = complex_query.profile()\n",
    "\n",
    "print(\"Profile for complex query:\")\n",
    "print(profile)\n",
    "print(f\"\\nResult shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-intro",
   "metadata": {},
   "source": [
    "## Part 2: Memory Management\n",
    "\n",
    "### 2.1 Memory Profiling\n",
    "\n",
    "Understanding memory usage is crucial for handling large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current process memory usage in MB\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def profile_memory(func, *args, **kwargs):\n",
    "    \"\"\"Profile memory usage of a function\"\"\"\n",
    "    mem_before = get_memory_usage()\n",
    "    result = func(*args, **kwargs)\n",
    "    mem_after = get_memory_usage()\n",
    "    mem_used = mem_after - mem_before\n",
    "    \n",
    "    print(f\"Memory before: {mem_before:.2f} MB\")\n",
    "    print(f\"Memory after: {mem_after:.2f} MB\")\n",
    "    print(f\"Memory used: {mem_used:.2f} MB\")\n",
    "    \n",
    "    return result, mem_used\n",
    "\n",
    "# Check DataFrame memory usage\n",
    "print(f\"DataFrame estimated size: {df.estimated_size('mb'):.2f} MB\")\n",
    "print(f\"Current process memory: {get_memory_usage():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage: eager vs lazy\n",
    "\n",
    "# Eager execution\n",
    "print(\"Eager execution:\")\n",
    "def eager_approach():\n",
    "    return (\n",
    "        df\n",
    "        .filter(pl.col('region') == 'North')\n",
    "        .filter(pl.col('sales') > 5000)\n",
    "        .select(['product', 'sales'])\n",
    "        .group_by('product')\n",
    "        .agg(pl.col('sales').sum())\n",
    "    )\n",
    "\n",
    "result_eager, mem_eager = profile_memory(eager_approach)\n",
    "\n",
    "print(\"\\nLazy execution:\")\n",
    "def lazy_approach():\n",
    "    return (\n",
    "        df.lazy()\n",
    "        .filter(pl.col('region') == 'North')\n",
    "        .filter(pl.col('sales') > 5000)\n",
    "        .select(['product', 'sales'])\n",
    "        .group_by('product')\n",
    "        .agg(pl.col('sales').sum())\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "result_lazy, mem_lazy = profile_memory(lazy_approach)\n",
    "\n",
    "print(f\"\\nMemory difference: {abs(mem_eager - mem_lazy):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming",
   "metadata": {},
   "source": [
    "### 2.2 Streaming for Large Datasets\n",
    "\n",
    "Use streaming to process datasets larger than available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for streaming demo\n",
    "large_df = pl.DataFrame({\n",
    "    'id': range(1, 100001),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], 100000),\n",
    "    'value': np.random.randn(100000) * 100,\n",
    "    'timestamp': [datetime(2024, 1, 1) + timedelta(minutes=i) for i in range(100000)]\n",
    "})\n",
    "\n",
    "# Save to disk for streaming\n",
    "large_df.write_parquet('temp_large_data.parquet')\n",
    "\n",
    "print(f\"Large dataset size: {large_df.estimated_size('mb'):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-query",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming query - processes data in chunks\n",
    "print(\"Streaming execution:\")\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "result = (\n",
    "    pl.scan_parquet('temp_large_data.parquet')\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.col('value').mean().alias('avg_value'),\n",
    "        pl.col('value').std().alias('std_value'),\n",
    "        pl.col('id').count().alias('count')\n",
    "    ])\n",
    "    .collect(streaming=True)  # Enable streaming\n",
    ")\n",
    "\n",
    "mem_after = get_memory_usage()\n",
    "print(f\"Memory used with streaming: {mem_after - mem_before:.2f} MB\")\n",
    "print(\"\\nResult:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with non-streaming\n",
    "print(\"Non-streaming execution:\")\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "result = (\n",
    "    pl.scan_parquet('temp_large_data.parquet')\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.col('value').mean().alias('avg_value'),\n",
    "        pl.col('value').std().alias('std_value'),\n",
    "        pl.col('id').count().alias('count')\n",
    "    ])\n",
    "    .collect(streaming=False)\n",
    ")\n",
    "\n",
    "mem_after = get_memory_usage()\n",
    "print(f\"Memory used without streaming: {mem_after - mem_before:.2f} MB\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove('temp_large_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory-efficient",
   "metadata": {},
   "source": [
    "### 2.3 Memory-Efficient Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 1: Use appropriate data types\n",
    "print(\"Impact of data types on memory:\")\n",
    "\n",
    "# Inefficient: using default types\n",
    "df_inefficient = pl.DataFrame({\n",
    "    'small_int': range(10000),  # Will use Int64 by default\n",
    "    'category': ['A'] * 10000,  # Will use String\n",
    "})\n",
    "print(f\"Inefficient size: {df_inefficient.estimated_size('mb'):.2f} MB\")\n",
    "\n",
    "# Efficient: using optimal types\n",
    "df_efficient = pl.DataFrame({\n",
    "    'small_int': pl.Series(range(10000), dtype=pl.UInt16),  # Smaller integer type\n",
    "    'category': pl.Series(['A'] * 10000, dtype=pl.Categorical),  # Categorical for repeating strings\n",
    "})\n",
    "print(f\"Efficient size: {df_efficient.estimated_size('mb'):.2f} MB\")\n",
    "print(f\"Memory saved: {(df_inefficient.estimated_size('mb') - df_efficient.estimated_size('mb')):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory-tips2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip 2: Drop columns you don't need early\n",
    "print(\"\\nMemory optimization with column selection:\")\n",
    "\n",
    "# Bad: keeping all columns\n",
    "def approach_bad():\n",
    "    result = df.filter(pl.col('region') == 'North')\n",
    "    result = result.group_by('product').agg(pl.col('sales').sum())\n",
    "    return result\n",
    "\n",
    "# Good: select only needed columns early\n",
    "def approach_good():\n",
    "    result = df.select(['region', 'product', 'sales'])\n",
    "    result = result.filter(pl.col('region') == 'North')\n",
    "    result = result.group_by('product').agg(pl.col('sales').sum())\n",
    "    return result\n",
    "\n",
    "print(\"Bad approach:\")\n",
    "_, mem_bad = profile_memory(approach_bad)\n",
    "\n",
    "print(\"\\nGood approach:\")\n",
    "_, mem_good = profile_memory(approach_good)\n",
    "\n",
    "print(f\"\\nMemory saved: {mem_bad - mem_good:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-intro",
   "metadata": {},
   "source": [
    "## Part 3: Lazy Evaluation Optimization\n",
    "\n",
    "### 3.1 Understanding Query Optimization\n",
    "\n",
    "Polars automatically optimizes lazy queries through various techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predicate-pushdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicate Pushdown - filters are applied as early as possible\n",
    "print(\"Predicate Pushdown Example:\")\n",
    "\n",
    "query = (\n",
    "    df.lazy()\n",
    "    .select(['customer_id', 'region', 'product', 'sales'])\n",
    "    .filter(pl.col('region') == 'North')  # This filter will be pushed down\n",
    "    .filter(pl.col('sales') > 5000)       # This too\n",
    ")\n",
    "\n",
    "print(\"Unoptimized plan:\")\n",
    "print(query.explain(optimized=False))\n",
    "\n",
    "print(\"\\nOptimized plan:\")\n",
    "print(query.explain(optimized=True))\n",
    "print(\"\\nNotice how filters are combined and pushed early in execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "projection-pushdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection Pushdown - only necessary columns are read\n",
    "print(\"Projection Pushdown Example:\")\n",
    "\n",
    "# Create a temporary parquet file\n",
    "df.write_parquet('temp_data.parquet')\n",
    "\n",
    "query = (\n",
    "    pl.scan_parquet('temp_data.parquet')\n",
    "    .select(['product', 'sales'])  # Only these columns will be read from disk\n",
    "    .filter(pl.col('sales') > 5000)\n",
    "    .group_by('product')\n",
    "    .agg(pl.col('sales').sum())\n",
    ")\n",
    "\n",
    "print(\"Optimized plan shows projection pushdown:\")\n",
    "print(query.explain(optimized=True))\n",
    "\n",
    "result = query.collect()\n",
    "print(\"\\nResult:\")\n",
    "print(result)\n",
    "\n",
    "# Clean up\n",
    "os.remove('temp_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization tips for writing queries\n",
    "\n",
    "# Tip 1: Put filters early\n",
    "# Good\n",
    "good_query = (\n",
    "    df.lazy()\n",
    "    .filter(pl.col('region') == 'North')  # Filter early\n",
    "    .with_columns([\n",
    "        (pl.col('sales') * 1.1).alias('sales_with_tax')\n",
    "    ])\n",
    "    .select(['product', 'sales_with_tax'])\n",
    ")\n",
    "\n",
    "# Tip 2: Combine filters when possible\n",
    "# Instead of multiple filter calls\n",
    "# Better to use one with combined conditions\n",
    "combined_filter = (\n",
    "    df.lazy()\n",
    "    .filter(\n",
    "        (pl.col('region') == 'North') & \n",
    "        (pl.col('sales') > 5000) &\n",
    "        (pl.col('quantity') > 10)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Optimization tip: Combine filters\")\n",
    "print(combined_filter.explain(optimized=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-intro",
   "metadata": {},
   "source": [
    "## Part 4: Parallel Processing\n",
    "\n",
    "### 4.1 Thread Pool Configuration\n",
    "\n",
    "Polars uses a thread pool for parallel operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thread-pool",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and configure thread pool\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")\n",
    "print(f\"Polars thread pool size: {pl.thread_pool_size()}\")\n",
    "\n",
    "# You can set the number of threads\n",
    "# pl.Config.set_thread_pool_size(4)  # Set to 4 threads\n",
    "\n",
    "# For CPU-bound operations, use all cores\n",
    "# For I/O-bound operations, you might want fewer threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-ops",
   "metadata": {},
   "source": [
    "### 4.2 Parallel Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars automatically parallelizes many operations\n",
    "\n",
    "# Create a larger dataset\n",
    "large_df = pl.DataFrame({\n",
    "    'group': np.random.choice(['A', 'B', 'C', 'D'], 100000),\n",
    "    'value1': np.random.randn(100000),\n",
    "    'value2': np.random.randn(100000),\n",
    "    'value3': np.random.randn(100000),\n",
    "})\n",
    "\n",
    "# Time a parallel group_by operation\n",
    "start = time.time()\n",
    "result = (\n",
    "    large_df.lazy()\n",
    "    .group_by('group')\n",
    "    .agg([\n",
    "        pl.col('value1').mean(),\n",
    "        pl.col('value1').std(),\n",
    "        pl.col('value2').sum(),\n",
    "        pl.col('value3').max(),\n",
    "        pl.col('value3').min(),\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Parallel aggregation took: {elapsed:.4f} seconds\")\n",
    "print(\"\\nResult:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-apply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple independent operations are automatically parallelized\n",
    "result = large_df.select([\n",
    "    pl.col('value1').mean().alias('mean1'),\n",
    "    pl.col('value2').mean().alias('mean2'),\n",
    "    pl.col('value3').mean().alias('mean3'),\n",
    "    pl.col('value1').std().alias('std1'),\n",
    "    pl.col('value2').std().alias('std2'),\n",
    "    pl.col('value3').std().alias('std3'),\n",
    "])\n",
    "\n",
    "print(\"Multiple aggregations (parallelized):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-intro",
   "metadata": {},
   "source": [
    "## Part 5: Performance Pitfalls\n",
    "\n",
    "### 5.1 Common Anti-Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 1: Using apply() when native expressions exist\n",
    "\n",
    "print(\"Pitfall 1: Avoid apply() when possible\\n\")\n",
    "\n",
    "# Bad: Using apply with Python function (slow)\n",
    "def custom_calc(x):\n",
    "    return x * 2 + 10\n",
    "\n",
    "start = time.time()\n",
    "result_bad = df.with_columns([\n",
    "    pl.col('sales').map_elements(custom_calc, return_dtype=pl.Float64).alias('calculated')\n",
    "])\n",
    "time_bad = time.time() - start\n",
    "\n",
    "# Good: Using native expressions (fast)\n",
    "start = time.time()\n",
    "result_good = df.with_columns([\n",
    "    (pl.col('sales') * 2 + 10).alias('calculated')\n",
    "])\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"Using apply: {time_bad:.4f} seconds\")\n",
    "print(f\"Using native expressions: {time_good:.4f} seconds\")\n",
    "print(f\"Speedup: {time_bad / time_good:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 2: Collecting too early in lazy queries\n",
    "\n",
    "print(\"Pitfall 2: Don't collect() too early\\n\")\n",
    "\n",
    "# Bad: Multiple collect() calls\n",
    "start = time.time()\n",
    "temp1 = df.lazy().filter(pl.col('region') == 'North').collect()\n",
    "temp2 = temp1.lazy().filter(pl.col('sales') > 5000).collect()\n",
    "result_bad = temp2.lazy().group_by('product').agg(pl.col('sales').sum()).collect()\n",
    "time_bad = time.time() - start\n",
    "\n",
    "# Good: Single collect() at the end\n",
    "start = time.time()\n",
    "result_good = (\n",
    "    df.lazy()\n",
    "    .filter(pl.col('region') == 'North')\n",
    "    .filter(pl.col('sales') > 5000)\n",
    "    .group_by('product')\n",
    "    .agg(pl.col('sales').sum())\n",
    "    .collect()\n",
    ")\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"Multiple collect(): {time_bad:.4f} seconds\")\n",
    "print(f\"Single collect(): {time_good:.4f} seconds\")\n",
    "print(f\"Speedup: {time_bad / time_good:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 3: Row-wise operations instead of columnar\n",
    "\n",
    "print(\"Pitfall 3: Use columnar operations\\n\")\n",
    "\n",
    "# Bad: Row-by-row iteration (very slow)\n",
    "start = time.time()\n",
    "result_list = []\n",
    "for row in df.iter_rows(named=True):\n",
    "    if row['region'] == 'North' and row['sales'] > 5000:\n",
    "        result_list.append(row)\n",
    "result_bad = pl.DataFrame(result_list)\n",
    "time_bad = time.time() - start\n",
    "\n",
    "# Good: Columnar operations (fast)\n",
    "start = time.time()\n",
    "result_good = df.filter(\n",
    "    (pl.col('region') == 'North') & \n",
    "    (pl.col('sales') > 5000)\n",
    ")\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"Row-by-row: {time_bad:.4f} seconds\")\n",
    "print(f\"Columnar: {time_good:.4f} seconds\")\n",
    "print(f\"Speedup: {time_bad / time_good:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pitfall-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pitfall 4: Creating too many intermediate DataFrames\n",
    "\n",
    "print(\"Pitfall 4: Chain operations efficiently\\n\")\n",
    "\n",
    "# Bad: Many intermediate variables\n",
    "start = time.time()\n",
    "df1 = df.filter(pl.col('region') == 'North')\n",
    "df2 = df1.with_columns([(pl.col('sales') * 1.1).alias('sales_tax')])\n",
    "df3 = df2.select(['product', 'sales_tax'])\n",
    "result_bad = df3.group_by('product').agg(pl.col('sales_tax').sum())\n",
    "time_bad = time.time() - start\n",
    "\n",
    "# Good: Chain operations\n",
    "start = time.time()\n",
    "result_good = (\n",
    "    df.filter(pl.col('region') == 'North')\n",
    "    .with_columns([(pl.col('sales') * 1.1).alias('sales_tax')])\n",
    "    .select(['product', 'sales_tax'])\n",
    "    .group_by('product')\n",
    "    .agg(pl.col('sales_tax').sum())\n",
    ")\n",
    "time_good = time.time() - start\n",
    "\n",
    "print(f\"Intermediate variables: {time_bad:.4f} seconds\")\n",
    "print(f\"Chained operations: {time_good:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-ops",
   "metadata": {},
   "source": [
    "### 5.2 Expensive Operations to Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operations that can be expensive:\n",
    "\n",
    "print(\"Expensive operations and alternatives:\\n\")\n",
    "\n",
    "# 1. join() can be expensive with large DataFrames\n",
    "# Tip: Filter data before joining\n",
    "print(\"1. Joins - filter before joining to reduce data\")\n",
    "\n",
    "# 2. sort() on large DataFrames\n",
    "# Tip: Only sort when necessary, use top_k() for top N values\n",
    "print(\"2. Sorts - use top_k() instead of sort().head()\")\n",
    "\n",
    "# Example: Getting top 5 sales\n",
    "# Instead of:\n",
    "# df.sort('sales', descending=True).head(5)\n",
    "# Use:\n",
    "top_sales = df.select(pl.col('sales')).top_k(5, by='sales')\n",
    "print(\"\\nTop 5 sales (using top_k):\")\n",
    "print(top_sales)\n",
    "\n",
    "# 3. unique() on large text columns\n",
    "# Tip: Use categorical type for columns with repeated values\n",
    "print(\"\\n3. Unique - use Categorical type for repeated string columns\")\n",
    "\n",
    "# 4. Multiple aggregations without group_by\n",
    "# Tip: Combine aggregations in a single agg() call\n",
    "print(\"\\n4. Aggregations - combine in single agg() call\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part6-intro",
   "metadata": {},
   "source": [
    "## Part 6: Benchmarking\n",
    "\n",
    "### 6.1 Timing Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable, List, Dict\n",
    "\n",
    "def benchmark_function(func: Callable, name: str, iterations: int = 5) -> Dict:\n",
    "    \"\"\"Benchmark a function over multiple iterations\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        start = time.time()\n",
    "        result = func()\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'mean': np.mean(times),\n",
    "        'std': np.std(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'iterations': iterations\n",
    "    }\n",
    "\n",
    "def compare_approaches(approaches: List[tuple]) -> pl.DataFrame:\n",
    "    \"\"\"Compare multiple approaches and return results\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for func, name in approaches:\n",
    "        result = benchmark_function(func, name)\n",
    "        results.append(result)\n",
    "    \n",
    "    return pl.DataFrame(results).sort('mean')\n",
    "\n",
    "print(\"Benchmark utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different approaches to the same task\n",
    "\n",
    "# Task: Calculate total sales by region for North and South only\n",
    "\n",
    "def approach1_eager():\n",
    "    return (\n",
    "        df\n",
    "        .filter(pl.col('region').is_in(['North', 'South']))\n",
    "        .group_by('region')\n",
    "        .agg(pl.col('sales').sum())\n",
    "    )\n",
    "\n",
    "def approach2_lazy():\n",
    "    return (\n",
    "        df.lazy()\n",
    "        .filter(pl.col('region').is_in(['North', 'South']))\n",
    "        .group_by('region')\n",
    "        .agg(pl.col('sales').sum())\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "def approach3_lazy_streaming():\n",
    "    return (\n",
    "        df.lazy()\n",
    "        .filter(pl.col('region').is_in(['North', 'South']))\n",
    "        .group_by('region')\n",
    "        .agg(pl.col('sales').sum())\n",
    "        .collect(streaming=True)\n",
    "    )\n",
    "\n",
    "approaches = [\n",
    "    (approach1_eager, \"Eager\"),\n",
    "    (approach2_lazy, \"Lazy\"),\n",
    "    (approach3_lazy_streaming, \"Lazy + Streaming\")\n",
    "]\n",
    "\n",
    "results = compare_approaches(approaches)\n",
    "print(\"Benchmark results (sorted by mean time):\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-tips",
   "metadata": {},
   "source": [
    "### 6.2 Best Practices for Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices for accurate benchmarking:\n",
    "\n",
    "print(\"Benchmarking Best Practices:\\n\")\n",
    "\n",
    "print(\"1. Run multiple iterations to account for variance\")\n",
    "print(\"2. Use realistic data sizes for your use case\")\n",
    "print(\"3. Warm up the cache before benchmarking\")\n",
    "print(\"4. Measure both execution time and memory usage\")\n",
    "print(\"5. Test with both small and large datasets\")\n",
    "print(\"6. Consider I/O time separately from computation time\")\n",
    "print(\"7. Profile in an environment similar to production\")\n",
    "\n",
    "# Example: Warming up cache\n",
    "def warm_up_cache():\n",
    "    # Run the operation once to warm up\n",
    "    _ = df.select(['region', 'sales']).group_by('region').agg(pl.col('sales').sum())\n",
    "\n",
    "warm_up_cache()\n",
    "print(\"\\nCache warmed up for accurate benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part7-intro",
   "metadata": {},
   "source": [
    "## Part 7: Real-World Optimization Workflow\n",
    "\n",
    "### 7.1 Complete Optimization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-workflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Analyzing sales data with performance optimization\n",
    "\n",
    "# Create a realistic dataset\n",
    "np.random.seed(42)\n",
    "sales_data = pl.DataFrame({\n",
    "    'order_id': range(1, 50001),\n",
    "    'customer_id': np.random.randint(1, 5000, 50000),\n",
    "    'product_category': np.random.choice(['Electronics', 'Clothing', 'Food', 'Books', 'Toys'], 50000),\n",
    "    'product_name': np.random.choice([f'Product_{i}' for i in range(100)], 50000),\n",
    "    'quantity': np.random.randint(1, 20, 50000),\n",
    "    'unit_price': np.random.uniform(10, 1000, 50000),\n",
    "    'discount_pct': np.random.choice([0, 5, 10, 15, 20], 50000),\n",
    "    'order_date': [datetime(2024, 1, 1) + timedelta(days=np.random.randint(0, 365)) for _ in range(50000)],\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], 50000),\n",
    "    'payment_method': np.random.choice(['Credit', 'Debit', 'Cash', 'PayPal'], 50000)\n",
    "})\n",
    "\n",
    "print(f\"Dataset size: {sales_data.estimated_size('mb'):.2f} MB\")\n",
    "print(f\"Rows: {len(sales_data):,}\")\n",
    "print(\"\\nSample data:\")\n",
    "print(sales_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE OPTIMIZATION: Inefficient approach\n",
    "\n",
    "print(\"BEFORE OPTIMIZATION:\\n\")\n",
    "\n",
    "def unoptimized_analysis():\n",
    "    # Calculate total revenue by category and region\n",
    "    # Using inefficient patterns\n",
    "    \n",
    "    # Step 1: Add calculated columns\n",
    "    df1 = sales_data.with_columns([\n",
    "        (pl.col('quantity') * pl.col('unit_price')).alias('subtotal')\n",
    "    ])\n",
    "    \n",
    "    # Step 2: Apply discount (collecting unnecessarily)\n",
    "    df2 = df1.with_columns([\n",
    "        (pl.col('subtotal') * (1 - pl.col('discount_pct') / 100)).alias('total')\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Filter for specific region\n",
    "    df3 = df2.filter(pl.col('region').is_in(['North', 'South']))\n",
    "    \n",
    "    # Step 4: Filter for date range\n",
    "    df4 = df3.filter(\n",
    "        pl.col('order_date') >= datetime(2024, 6, 1)\n",
    "    )\n",
    "    \n",
    "    # Step 5: Group and aggregate\n",
    "    result = (\n",
    "        df4\n",
    "        .group_by(['product_category', 'region'])\n",
    "        .agg([\n",
    "            pl.col('total').sum().alias('total_revenue'),\n",
    "            pl.col('order_id').count().alias('order_count'),\n",
    "            pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "        ])\n",
    "        .sort('total_revenue', descending=True)\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Time and profile\n",
    "start = time.time()\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "result_before = unoptimized_analysis()\n",
    "\n",
    "time_before = time.time() - start\n",
    "mem_used_before = get_memory_usage() - mem_before\n",
    "\n",
    "print(f\"Execution time: {time_before:.4f} seconds\")\n",
    "print(f\"Memory used: {mem_used_before:.2f} MB\")\n",
    "print(\"\\nResult:\")\n",
    "print(result_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimization-optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER OPTIMIZATION: Efficient approach\n",
    "\n",
    "print(\"AFTER OPTIMIZATION:\\n\")\n",
    "\n",
    "def optimized_analysis():\n",
    "    # Same calculation but with optimizations:\n",
    "    # 1. Use lazy evaluation\n",
    "    # 2. Apply filters early\n",
    "    # 3. Select only needed columns\n",
    "    # 4. Combine operations\n",
    "    # 5. Single collect at the end\n",
    "    \n",
    "    result = (\n",
    "        sales_data.lazy()\n",
    "        # Apply filters FIRST (predicate pushdown)\n",
    "        .filter(\n",
    "            pl.col('region').is_in(['North', 'South']) &\n",
    "            (pl.col('order_date') >= datetime(2024, 6, 1))\n",
    "        )\n",
    "        # Select only needed columns (projection pushdown)\n",
    "        .select([\n",
    "            'product_category',\n",
    "            'region',\n",
    "            'order_id',\n",
    "            'customer_id',\n",
    "            'quantity',\n",
    "            'unit_price',\n",
    "            'discount_pct'\n",
    "        ])\n",
    "        # Calculate in a single step\n",
    "        .with_columns([\n",
    "            (\n",
    "                pl.col('quantity') * \n",
    "                pl.col('unit_price') * \n",
    "                (1 - pl.col('discount_pct') / 100)\n",
    "            ).alias('total')\n",
    "        ])\n",
    "        # Aggregate\n",
    "        .group_by(['product_category', 'region'])\n",
    "        .agg([\n",
    "            pl.col('total').sum().alias('total_revenue'),\n",
    "            pl.col('order_id').count().alias('order_count'),\n",
    "            pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "        ])\n",
    "        .sort('total_revenue', descending=True)\n",
    "        # Single collect at the end\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Time and profile\n",
    "start = time.time()\n",
    "mem_before = get_memory_usage()\n",
    "\n",
    "result_after = optimized_analysis()\n",
    "\n",
    "time_after = time.time() - start\n",
    "mem_used_after = get_memory_usage() - mem_before\n",
    "\n",
    "print(f\"Execution time: {time_after:.4f} seconds\")\n",
    "print(f\"Memory used: {mem_used_after:.2f} MB\")\n",
    "print(\"\\nResult:\")\n",
    "print(result_after)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OPTIMIZATION IMPACT:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Time improvement: {time_before / time_after:.2f}x faster\")\n",
    "print(f\"Memory reduction: {mem_used_before - mem_used_after:.2f} MB saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "production-tips",
   "metadata": {},
   "source": [
    "### 7.2 Production Optimization Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "production-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRODUCTION OPTIMIZATION CHECKLIST:\\n\")\n",
    "print(\"☐ 1. Use lazy evaluation with .lazy() and single .collect()\")\n",
    "print(\"☐ 2. Apply filters as early as possible in the query\")\n",
    "print(\"☐ 3. Select only necessary columns\")\n",
    "print(\"☐ 4. Use appropriate data types (e.g., Categorical for repeated strings)\")\n",
    "print(\"☐ 5. Avoid map_elements() - use native expressions instead\")\n",
    "print(\"☐ 6. Use streaming for very large datasets\")\n",
    "print(\"☐ 7. Profile queries with .explain() and .profile()\")\n",
    "print(\"☐ 8. Combine multiple filters into a single condition\")\n",
    "print(\"☐ 9. Use top_k() instead of sort().head()\")\n",
    "print(\"☐ 10. Monitor memory usage with .estimated_size()\")\n",
    "print(\"☐ 11. Configure thread pool size based on workload\")\n",
    "print(\"☐ 12. Use scan_parquet() for file-based operations\")\n",
    "print(\"☐ 13. Avoid row-wise iterations - think columnar\")\n",
    "print(\"☐ 14. Chain operations instead of creating intermediate DataFrames\")\n",
    "print(\"☐ 15. Benchmark different approaches before deploying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "### Query Profiling\n",
    "- Using `explain()` to understand query execution plans\n",
    "- Using `profile()` to identify performance bottlenecks\n",
    "- Understanding optimization strategies (predicate/projection pushdown)\n",
    "\n",
    "### Memory Management\n",
    "- Profiling memory usage with `estimated_size()`\n",
    "- Using streaming for datasets larger than memory\n",
    "- Memory-efficient operations and data types\n",
    "\n",
    "### Lazy Evaluation\n",
    "- How Polars optimizes lazy queries automatically\n",
    "- Predicate and projection pushdown\n",
    "- Writing queries for optimal performance\n",
    "\n",
    "### Parallel Processing\n",
    "- How Polars leverages multiple CPU cores\n",
    "- Thread pool configuration\n",
    "- Parallel operations are automatic\n",
    "\n",
    "### Performance Pitfalls\n",
    "- Avoid `map_elements()` when native expressions exist\n",
    "- Don't collect() too early in lazy queries\n",
    "- Use columnar operations instead of row-wise iteration\n",
    "- Chain operations efficiently\n",
    "\n",
    "### Benchmarking\n",
    "- Timing different approaches\n",
    "- Best practices for accurate measurements\n",
    "- Comparing eager, lazy, and streaming execution\n",
    "\n",
    "### Real-World Optimization\n",
    "- Complete before/after optimization workflow\n",
    "- Production checklist for high performance\n",
    "- Measuring time and memory improvements\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Use lazy evaluation** - Build your query with `.lazy()` and collect once at the end\n",
    "2. **Filter early** - Apply filters before other operations to reduce data volume\n",
    "3. **Think columnar** - Avoid row-wise operations, use native expressions\n",
    "4. **Profile first** - Use `explain()` and `profile()` to understand what's slow\n",
    "5. **Choose the right data types** - Categorical, smaller integer types save memory\n",
    "6. **Use streaming** - For datasets larger than available memory\n",
    "7. **Benchmark alternatives** - Test different approaches to find the fastest\n",
    "\n",
    "Remember: \"Premature optimization is the root of all evil\" - Profile first, then optimize where it matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Query Optimization\n",
    "You have this inefficient query. Optimize it and measure the improvement:\n",
    "\n",
    "```python\n",
    "result = (\n",
    "    sales_data\n",
    "    .with_columns([(pl.col('quantity') * pl.col('unit_price')).alias('total')])\n",
    "    .group_by('product_category')\n",
    "    .agg(pl.col('total').sum())\n",
    "    .filter(pl.col('total') > 100000)\n",
    ")\n",
    "```\n",
    "\n",
    "Hints:\n",
    "- Use lazy evaluation\n",
    "- Apply filters early\n",
    "- Use `.explain()` to see the difference\n",
    "\n",
    "### Exercise 2: Memory Optimization\n",
    "Create a DataFrame with 1 million rows and optimize its memory usage:\n",
    "- Use appropriate integer types (UInt8, UInt16, etc.)\n",
    "- Use Categorical for repeated string values\n",
    "- Measure memory before and after optimization\n",
    "\n",
    "### Exercise 3: Performance Pitfall Identification\n",
    "Identify and fix the performance issues in this code:\n",
    "\n",
    "```python\n",
    "def slow_analysis():\n",
    "    result = []\n",
    "    for row in sales_data.iter_rows(named=True):\n",
    "        if row['region'] == 'North':\n",
    "            total = row['quantity'] * row['unit_price']\n",
    "            result.append({'product': row['product_name'], 'total': total})\n",
    "    return pl.DataFrame(result)\n",
    "```\n",
    "\n",
    "Rewrite it using columnar operations.\n",
    "\n",
    "### Exercise 4: Streaming Implementation\n",
    "Create a large dataset (10+ million rows), save it as parquet, and:\n",
    "- Query it without streaming\n",
    "- Query it with streaming\n",
    "- Compare memory usage and execution time\n",
    "\n",
    "### Exercise 5: Comprehensive Optimization\n",
    "Given this complex analysis task:\n",
    "- Calculate monthly revenue by category\n",
    "- Filter for orders > $100\n",
    "- Calculate average discount per category\n",
    "- Find top 10 customers by total spend\n",
    "\n",
    "Implement it in two ways:\n",
    "1. Inefficient (eager, multiple intermediates)\n",
    "2. Optimized (lazy, single collect, efficient filters)\n",
    "\n",
    "Benchmark both approaches and compare the results.\n",
    "\n",
    "### Exercise 6: Production Pipeline\n",
    "Create a production-ready data processing pipeline that:\n",
    "- Reads from multiple parquet files\n",
    "- Applies transformations efficiently\n",
    "- Includes error handling\n",
    "- Profiles execution time\n",
    "- Logs memory usage\n",
    "- Uses streaming for large files\n",
    "\n",
    "Include all optimizations from this notebook.\n",
    "\n",
    "### Bonus Challenge: Query Plan Analysis\n",
    "Write a function that:\n",
    "1. Takes a lazy query as input\n",
    "2. Prints the unoptimized and optimized query plans\n",
    "3. Profiles the query execution\n",
    "4. Reports execution time and memory usage\n",
    "5. Provides optimization suggestions\n",
    "\n",
    "Test it on various queries and see how different patterns affect performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
