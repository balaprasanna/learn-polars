{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Interoperability - Comprehensive Guide\n",
    "\n",
    "This notebook covers how Polars integrates with other data tools and libraries.\n",
    "\n",
    "## What You'll Learn:\n",
    "- DuckDB integration (SQL on Polars DataFrames)\n",
    "- Apache Arrow ecosystem and zero-copy operations\n",
    "- Pandas interoperability (to_pandas, from_pandas)\n",
    "- NumPy integration for numerical computing\n",
    "- When to use each tool\n",
    "- Performance implications of conversions\n",
    "- Best practices for multi-tool workflows\n",
    "\n",
    "## Prerequisites:\n",
    "```bash\n",
    "pip install polars duckdb pyarrow pandas numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    HAS_ARROW = True\n",
    "except ImportError:\n",
    "    HAS_ARROW = False\n",
    "    print(\"âš ï¸ PyArrow not installed. Some examples will be skipped.\")\n",
    "\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"DuckDB version: {duckdb.__version__}\")\n",
    "if HAS_ARROW:\n",
    "    print(f\"PyArrow version: {pa.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: DuckDB Integration\n",
    "\n",
    "DuckDB is an embedded SQL database optimized for analytics. It can query Polars DataFrames directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic DuckDB Queries on Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Polars DataFrame\n",
    "df_sales = pl.DataFrame({\n",
    "    'order_id': range(1, 11),\n",
    "    'customer': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', \n",
    "                 'Alice', 'Diana', 'Charlie', 'Bob', 'Diana'],\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop',\n",
    "                'Mouse', 'Keyboard', 'Mouse', 'Monitor', 'Laptop'],\n",
    "    'quantity': [1, 2, 1, 1, 1, 3, 2, 1, 2, 1],\n",
    "    'price': [1200, 25, 75, 300, 1200, 25, 75, 25, 300, 1200],\n",
    "    'date': [date(2024, 1, i) for i in range(1, 11)]\n",
    "})\n",
    "\n",
    "print(\"Polars DataFrame:\")\n",
    "print(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Polars DataFrame using DuckDB SQL\n",
    "# DuckDB can directly reference the DataFrame variable name!\n",
    "\n",
    "result = duckdb.query(\"\"\"\n",
    "    SELECT \n",
    "        customer,\n",
    "        COUNT(*) as num_orders,\n",
    "        SUM(quantity * price) as total_revenue\n",
    "    FROM df_sales\n",
    "    GROUP BY customer\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\").pl()  # .pl() returns Polars DataFrame\n",
    "\n",
    "print(\"DuckDB query result (as Polars):\")\n",
    "print(result)\n",
    "print(f\"Type: {type(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex SQL queries with JOINs\n",
    "df_customers = pl.DataFrame({\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', \n",
    "              'diana@example.com', 'eve@example.com'],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'Houston', 'Phoenix']\n",
    "})\n",
    "\n",
    "result = duckdb.query(\"\"\"\n",
    "    SELECT \n",
    "        c.customer,\n",
    "        c.city,\n",
    "        COUNT(s.order_id) as num_orders,\n",
    "        COALESCE(SUM(s.quantity * s.price), 0) as total_spent\n",
    "    FROM df_customers c\n",
    "    LEFT JOIN df_sales s ON c.customer = s.customer\n",
    "    GROUP BY c.customer, c.city\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").pl()\n",
    "\n",
    "print(\"Complex JOIN query:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DuckDB vs Polars SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars native SQL (using SQLContext)\n",
    "ctx = pl.SQLContext()\n",
    "ctx.register(\"sales\", df_sales)\n",
    "ctx.register(\"customers\", df_customers)\n",
    "\n",
    "result_polars = ctx.execute(\"\"\"\n",
    "    SELECT \n",
    "        customer,\n",
    "        SUM(quantity * price) as total\n",
    "    FROM sales\n",
    "    GROUP BY customer\n",
    "    ORDER BY total DESC\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"Polars SQL result:\")\n",
    "print(result_polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: DuckDB vs Polars SQL vs Polars expressions\n",
    "# Create larger dataset\n",
    "n = 100_000\n",
    "df_large = pl.DataFrame({\n",
    "    'id': range(n),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'value': np.random.randn(n)\n",
    "})\n",
    "\n",
    "# Method 1: DuckDB\n",
    "start = time.time()\n",
    "result1 = duckdb.query(\"\"\"\n",
    "    SELECT category, AVG(value) as avg_value\n",
    "    FROM df_large\n",
    "    GROUP BY category\n",
    "\"\"\").pl()\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Method 2: Polars SQL\n",
    "ctx = pl.SQLContext(data=df_large)\n",
    "start = time.time()\n",
    "result2 = ctx.execute(\"\"\"\n",
    "    SELECT category, AVG(value) as avg_value\n",
    "    FROM data\n",
    "    GROUP BY category\n",
    "\"\"\").collect()\n",
    "time2 = time.time() - start\n",
    "\n",
    "# Method 3: Polars expressions (native)\n",
    "start = time.time()\n",
    "result3 = df_large.group_by('category').agg(\n",
    "    pl.col('value').mean().alias('avg_value')\n",
    ")\n",
    "time3 = time.time() - start\n",
    "\n",
    "print(f\"DuckDB SQL:        {time1:.4f}s\")\n",
    "print(f\"Polars SQL:        {time2:.4f}s\")\n",
    "print(f\"Polars expressions: {time3:.4f}s\")\n",
    "print(\"\\nðŸ’¡ Polars expressions are usually fastest for simple queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 When to Use DuckDB vs Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison guide\n",
    "comparison = pl.DataFrame({\n",
    "    'Scenario': [\n",
    "        'Simple aggregations',\n",
    "        'Complex SQL (CTEs, subqueries)',\n",
    "        'Window functions',\n",
    "        'Multiple joins',\n",
    "        'Integration with SQL databases',\n",
    "        'Memory efficiency',\n",
    "        'Team familiar with SQL',\n",
    "        'Need best performance',\n",
    "        'Type safety important'\n",
    "    ],\n",
    "    'Polars': [\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­',\n",
    "        'â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­â­'\n",
    "    ],\n",
    "    'DuckDB': [\n",
    "        'â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­',\n",
    "        'â­â­â­'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Polars vs DuckDB:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Apache Arrow Integration\n",
    "\n",
    "Arrow is the backbone of Polars. It enables zero-copy data sharing between tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Understanding Apache Arrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_ARROW:\n",
    "    # Polars DataFrame -> Arrow Table (zero-copy)\n",
    "    df_arrow = pl.DataFrame({\n",
    "        'id': [1, 2, 3, 4, 5],\n",
    "        'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "        'value': [10.5, 20.3, 30.1, 40.8, 50.2]\n",
    "    })\n",
    "    \n",
    "    # Convert to Arrow\n",
    "    arrow_table = df_arrow.to_arrow()\n",
    "    \n",
    "    print(\"PyArrow Table:\")\n",
    "    print(arrow_table)\n",
    "    print(f\"\\nType: {type(arrow_table)}\")\n",
    "    print(f\"Schema: {arrow_table.schema}\")\n",
    "else:\n",
    "    print(\"PyArrow not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_ARROW:\n",
    "    # Arrow Table -> Polars DataFrame (zero-copy)\n",
    "    df_from_arrow = pl.from_arrow(arrow_table)\n",
    "    \n",
    "    print(\"Polars DataFrame from Arrow:\")\n",
    "    print(df_from_arrow)\n",
    "    print(f\"Type: {type(df_from_arrow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Zero-Copy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_ARROW:\n",
    "    # Demonstrate zero-copy (no data duplication)\n",
    "    import sys\n",
    "    \n",
    "    df_large = pl.DataFrame({\n",
    "        'values': list(range(1_000_000))\n",
    "    })\n",
    "    \n",
    "    # Memory before conversion\n",
    "    mem_before = df_large.estimated_size('mb')\n",
    "    \n",
    "    # Convert to Arrow (zero-copy)\n",
    "    arrow_large = df_large.to_arrow()\n",
    "    \n",
    "    # Convert back to Polars (zero-copy)\n",
    "    df_back = pl.from_arrow(arrow_large)\n",
    "    \n",
    "    print(f\"Original DF memory: {mem_before:.2f} MB\")\n",
    "    print(f\"Arrow table memory: {sys.getsizeof(arrow_large) / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"Back to Polars memory: {df_back.estimated_size('mb'):.2f} MB\")\n",
    "    print(\"\\nðŸ’¡ Zero-copy: Data is shared, not duplicated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Arrow Ecosystem Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_ARROW:\n",
    "    # Arrow enables efficient data exchange\n",
    "    # Polars -> Arrow -> DuckDB (no copying)\n",
    "    \n",
    "    df = pl.DataFrame({\n",
    "        'a': [1, 2, 3],\n",
    "        'b': [4, 5, 6]\n",
    "    })\n",
    "    \n",
    "    # Via Arrow, DuckDB can query Polars data efficiently\n",
    "    result = duckdb.query(\"\"\"\n",
    "        SELECT a, b, a + b as sum\n",
    "        FROM df\n",
    "    \"\"\").pl()\n",
    "    \n",
    "    print(\"Polars -> Arrow -> DuckDB -> Polars (all zero-copy):\")\n",
    "    print(result)\n",
    "    print(\"\\nâœ… Arrow is the 'universal' format for data tools!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Pandas Interoperability\n",
    "\n",
    "Converting between Polars and Pandas for library compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Polars to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Polars DataFrame\n",
    "df_polars = pl.DataFrame({\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'age': [25, 30, 35, 28, 42],\n",
    "    'salary': [70000, 80000, 90000, 75000, 95000]\n",
    "})\n",
    "\n",
    "# Convert to Pandas\n",
    "df_pandas = df_polars.to_pandas()\n",
    "\n",
    "print(\"Polars DataFrame:\")\n",
    "print(df_polars)\n",
    "print(f\"Type: {type(df_polars)}\")\n",
    "\n",
    "print(\"\\nPandas DataFrame:\")\n",
    "print(df_pandas)\n",
    "print(f\"Type: {type(df_pandas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Pandas to Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrame\n",
    "df_pd = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Mouse', 'Keyboard'],\n",
    "    'price': [1200, 25, 75],\n",
    "    'stock': [15, 100, 50]\n",
    "})\n",
    "\n",
    "# Convert to Polars\n",
    "df_pl = pl.from_pandas(df_pd)\n",
    "\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(df_pd)\n",
    "print(f\"Type: {type(df_pd)}\")\n",
    "\n",
    "print(\"\\nPolars DataFrame:\")\n",
    "print(df_pl)\n",
    "print(f\"Type: {type(df_pl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Type Mapping and Gotchas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type differences between Pandas and Polars\n",
    "df_types = pl.DataFrame({\n",
    "    'int64': pl.Series([1, 2, 3], dtype=pl.Int64),\n",
    "    'float64': pl.Series([1.0, 2.0, 3.0], dtype=pl.Float64),\n",
    "    'string': pl.Series(['a', 'b', 'c'], dtype=pl.Utf8),\n",
    "    'categorical': pl.Series(['A', 'B', 'A'], dtype=pl.Categorical),\n",
    "    'date': pl.Series([date(2024, 1, 1), date(2024, 1, 2), date(2024, 1, 3)], dtype=pl.Date)\n",
    "})\n",
    "\n",
    "print(\"Polars dtypes:\")\n",
    "print(df_types.dtypes)\n",
    "\n",
    "# Convert to Pandas\n",
    "df_pd_converted = df_types.to_pandas()\n",
    "print(\"\\nPandas dtypes:\")\n",
    "print(df_pd_converted.dtypes)\n",
    "\n",
    "print(\"\\nðŸ’¡ Categorical becomes 'category', Date becomes datetime64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Performance: Polars vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for comparison\n",
    "n = 100_000\n",
    "data = {\n",
    "    'id': range(n),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], n),\n",
    "    'value': np.random.randn(n)\n",
    "}\n",
    "\n",
    "# Create both DataFrames\n",
    "df_polars_perf = pl.DataFrame(data)\n",
    "df_pandas_perf = pd.DataFrame(data)\n",
    "\n",
    "# GroupBy aggregation in Polars\n",
    "start = time.time()\n",
    "result_polars = df_polars_perf.group_by('category').agg([\n",
    "    pl.col('value').mean().alias('mean'),\n",
    "    pl.col('value').std().alias('std')\n",
    "])\n",
    "time_polars = time.time() - start\n",
    "\n",
    "# GroupBy aggregation in Pandas\n",
    "start = time.time()\n",
    "result_pandas = df_pandas_perf.groupby('category')['value'].agg(['mean', 'std'])\n",
    "time_pandas = time.time() - start\n",
    "\n",
    "print(f\"Polars: {time_polars:.4f}s\")\n",
    "print(f\"Pandas: {time_pandas:.4f}s\")\n",
    "print(f\"\\nPolars is {time_pandas/time_polars:.1f}x faster for this operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 When to Use Pandas vs Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision matrix\n",
    "decision = pl.DataFrame({\n",
    "    'Use Case': [\n",
    "        'Small data (<100MB)',\n",
    "        'Large data (>1GB)',\n",
    "        'Need specific library',\n",
    "        'Performance critical',\n",
    "        'Memory constrained',\n",
    "        'Team familiarity',\n",
    "        'Time series analysis',\n",
    "        'Machine learning prep',\n",
    "        'Data cleaning pipeline'\n",
    "    ],\n",
    "    'Polars': [\n",
    "        'â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­',\n",
    "        'â­â­â­â­',\n",
    "        'â­â­â­â­',\n",
    "        'â­â­â­â­â­'\n",
    "    ],\n",
    "    'Pandas': [\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­',\n",
    "        'â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­â­â­',\n",
    "        'â­â­â­'\n",
    "    ],\n",
    "    'Recommendation': [\n",
    "        'Either works fine',\n",
    "        'Polars strongly preferred',\n",
    "        'Use Pandas, convert if needed',\n",
    "        'Polars',\n",
    "        'Polars',\n",
    "        'Pandas (but learn Polars!)',\n",
    "        'Pandas has more tools',\n",
    "        'Pandas (scikit-learn, etc.)',\n",
    "        'Polars for speed'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Polars vs Pandas Decision Guide:\")\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: NumPy Integration\n",
    "\n",
    "Converting between Polars and NumPy for numerical computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Polars to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars DataFrame to NumPy array\n",
    "df_numpy = pl.DataFrame({\n",
    "    'a': [1, 2, 3, 4, 5],\n",
    "    'b': [10, 20, 30, 40, 50],\n",
    "    'c': [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "# Convert entire DataFrame to 2D NumPy array\n",
    "arr = df_numpy.to_numpy()\n",
    "\n",
    "print(\"Polars DataFrame:\")\n",
    "print(df_numpy)\n",
    "\n",
    "print(\"\\nNumPy array:\")\n",
    "print(arr)\n",
    "print(f\"Shape: {arr.shape}\")\n",
    "print(f\"Dtype: {arr.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert single column (Series) to NumPy array\n",
    "col_arr = df_numpy['a'].to_numpy()\n",
    "\n",
    "print(\"Single column as NumPy array:\")\n",
    "print(col_arr)\n",
    "print(f\"Shape: {col_arr.shape}\")\n",
    "print(f\"Type: {type(col_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 NumPy to Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array to Polars DataFrame\n",
    "arr_2d = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "# Convert with column names\n",
    "df_from_numpy = pl.DataFrame(arr_2d, schema=['col_a', 'col_b', 'col_c'])\n",
    "\n",
    "print(\"NumPy array:\")\n",
    "print(arr_2d)\n",
    "\n",
    "print(\"\\nPolars DataFrame:\")\n",
    "print(df_from_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array to Polars Series\n",
    "arr_1d = np.array([1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "series = pl.Series('values', arr_1d)\n",
    "\n",
    "print(\"NumPy array:\")\n",
    "print(arr_1d)\n",
    "\n",
    "print(\"\\nPolars Series:\")\n",
    "print(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Using NumPy Functions with Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NumPy functions to Polars data\n",
    "df_math = pl.DataFrame({\n",
    "    'x': [0, 0.5, 1.0, 1.5, 2.0],\n",
    "    'y': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "# Method 1: Convert to NumPy, apply function, convert back\n",
    "x_arr = df_math['x'].to_numpy()\n",
    "sin_values = np.sin(x_arr)\n",
    "df_result = df_math.with_columns(\n",
    "    pl.Series('sin_x', sin_values)\n",
    ")\n",
    "\n",
    "print(\"Apply NumPy sin function:\")\n",
    "print(df_result)\n",
    "\n",
    "# Method 2: Use Polars expressions (preferred when available)\n",
    "df_result2 = df_math.with_columns([\n",
    "    pl.col('x').map_elements(np.sin, return_dtype=pl.Float64).alias('sin_x_map')\n",
    "])\n",
    "\n",
    "print(\"\\nUsing map_elements:\")\n",
    "print(df_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Advanced: Numerical Computing with Polars + NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Linear regression using NumPy on Polars data\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "x = np.linspace(0, 10, n)\n",
    "y = 2.5 * x + 3 + np.random.randn(n) * 2\n",
    "\n",
    "df_regression = pl.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# Extract as NumPy for linear regression\n",
    "X = df_regression['x'].to_numpy()\n",
    "Y = df_regression['y'].to_numpy()\n",
    "\n",
    "# Fit line: y = mx + b\n",
    "# Using numpy.polyfit\n",
    "m, b = np.polyfit(X, Y, 1)\n",
    "print(f\"Linear fit: y = {m:.2f}x + {b:.2f}\")\n",
    "\n",
    "# Add predictions back to Polars DataFrame\n",
    "predictions = m * X + b\n",
    "df_regression = df_regression.with_columns(\n",
    "    pl.Series('y_pred', predictions)\n",
    ")\n",
    "\n",
    "print(\"\\nData with predictions:\")\n",
    "print(df_regression.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Multi-Tool Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Polars + DuckDB + Pandas Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world workflow: Combine strengths of each tool\n",
    "\n",
    "# Step 1: Load and clean with Polars (fast)\n",
    "df_raw = pl.DataFrame({\n",
    "    'id': range(1, 101),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'value': np.random.randn(100) * 10 + 50,\n",
    "    'date': [date(2024, 1, 1) + pd.Timedelta(days=i) for i in range(100)]\n",
    "})\n",
    "\n",
    "print(\"Step 1 - Polars: Load and clean\")\n",
    "df_clean = (\n",
    "    df_raw\n",
    "    .filter(pl.col('value') > 0)\n",
    "    .with_columns(\n",
    "        pl.col('value').round(2).alias('value')\n",
    "    )\n",
    ")\n",
    "print(f\"Cleaned: {len(df_clean)} rows\")\n",
    "\n",
    "# Step 2: Complex aggregation with DuckDB (SQL)\n",
    "print(\"\\nStep 2 - DuckDB: Complex SQL query\")\n",
    "df_agg = duckdb.query(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as count,\n",
    "        AVG(value) as avg_value,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY value) as median_value\n",
    "    FROM df_clean\n",
    "    GROUP BY category\n",
    "    ORDER BY avg_value DESC\n",
    "\"\"\").pl()\n",
    "print(df_agg)\n",
    "\n",
    "# Step 3: Use Pandas for specialized analysis (if needed)\n",
    "print(\"\\nStep 3 - Pandas: Time series resampling\")\n",
    "df_pandas_ts = df_clean.to_pandas()\n",
    "df_pandas_ts = df_pandas_ts.set_index('date')\n",
    "df_resampled = df_pandas_ts['value'].resample('7D').mean()\n",
    "print(df_resampled.head())\n",
    "\n",
    "print(\"\\nâœ… Multi-tool pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Best Practices for Tool Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "best_practices = pl.DataFrame({\n",
    "    'Task': [\n",
    "        'Initial data loading',\n",
    "        'Data cleaning',\n",
    "        'Large aggregations',\n",
    "        'Complex SQL queries',\n",
    "        'Time series analysis',\n",
    "        'Machine learning prep',\n",
    "        'Statistical analysis',\n",
    "        'Visualization prep',\n",
    "        'Export to database'\n",
    "    ],\n",
    "    'Recommended Tool': [\n",
    "        'Polars (scan_parquet, scan_csv)',\n",
    "        'Polars (fast, expressive)',\n",
    "        'Polars (parallel, lazy)',\n",
    "        'DuckDB (full SQL support)',\n",
    "        'Pandas (rich ecosystem)',\n",
    "        'Pandas (scikit-learn integration)',\n",
    "        'NumPy/SciPy via Pandas',\n",
    "        'Convert to Pandas at end',\n",
    "        'DuckDB or Pandas'\n",
    "    ],\n",
    "    'Why': [\n",
    "        'Lazy loading, columnar format',\n",
    "        'Fast operations, good API',\n",
    "        'Automatic parallelization',\n",
    "        'CTEs, window functions',\n",
    "        'Pandas has .resample(), rolling',\n",
    "        'Most ML libs use Pandas',\n",
    "        'Mature statistical libraries',\n",
    "        'Plotting libs prefer Pandas',\n",
    "        'Better ecosystem support'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Tool Selection Best Practices:\")\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Interoperability Overview:\n",
    "\n",
    "### **DuckDB Integration:**\n",
    "- âœ… Query Polars DataFrames with SQL directly\n",
    "- âœ… Zero-copy via Apache Arrow\n",
    "- âœ… Best for complex SQL queries (CTEs, subqueries)\n",
    "- âš ï¸ Slightly slower than native Polars expressions\n",
    "\n",
    "### **Apache Arrow:**\n",
    "- âœ… Enables zero-copy data sharing\n",
    "- âœ… Universal format for data tools\n",
    "- âœ… Polars is built on Arrow\n",
    "- âœ… Use `.to_arrow()` and `pl.from_arrow()`\n",
    "\n",
    "### **Pandas:**\n",
    "- âœ… Convert with `.to_pandas()` and `pl.from_pandas()`\n",
    "- âœ… Use Pandas for specific libraries (scikit-learn, statsmodels)\n",
    "- âš ï¸ Conversion has overhead (but usually fast)\n",
    "- âš ï¸ Pandas uses more memory\n",
    "\n",
    "### **NumPy:**\n",
    "- âœ… Convert with `.to_numpy()` and `pl.Series()`\n",
    "- âœ… Use NumPy for numerical computing\n",
    "- âœ… Good for matrix operations, linear algebra\n",
    "- âš ï¸ Loses column names (2D array)\n",
    "\n",
    "## Decision Framework:\n",
    "\n",
    "| Need | Tool | Convert? |\n",
    "|------|------|----------|\n",
    "| Fast data wrangling | Polars | Native |\n",
    "| Complex SQL | DuckDB | Query directly |\n",
    "| ML with scikit-learn | Pandas | `.to_pandas()` |\n",
    "| Time series | Pandas | `.to_pandas()` |\n",
    "| Linear algebra | NumPy | `.to_numpy()` |\n",
    "| Plotting | Pandas | `.to_pandas()` |\n",
    "\n",
    "## Common Patterns:\n",
    "```python\n",
    "# Polars -> DuckDB\n",
    "result = duckdb.query(\"SELECT * FROM df WHERE x > 10\").pl()\n",
    "\n",
    "# Polars -> Pandas\n",
    "df_pd = df_pl.to_pandas()\n",
    "\n",
    "# Pandas -> Polars\n",
    "df_pl = pl.from_pandas(df_pd)\n",
    "\n",
    "# Polars -> NumPy\n",
    "arr = df_pl.to_numpy()\n",
    "\n",
    "# NumPy -> Polars\n",
    "df = pl.DataFrame(arr, schema=['a', 'b', 'c'])\n",
    "```\n",
    "\n",
    "## Key Principle:\n",
    "**ðŸŽ¯ Use Polars for heavy lifting (cleaning, aggregations), convert only when needed for specialized libraries!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Use DuckDB to write a complex SQL query\n",
    "# TODO: Create sales data, use DuckDB with CTEs to analyze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Build a Polars -> Pandas -> ML pipeline\n",
    "# TODO: Clean data with Polars, convert to Pandas, train simple model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Use NumPy for numerical computation on Polars data\n",
    "# TODO: Calculate moving averages using NumPy convolution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
