{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Data Quality and Validation - Comprehensive Guide\n",
    "\n",
    "This notebook covers data quality assessment and validation techniques in Polars.\n",
    "\n",
    "## What You'll Learn:\n",
    "- Data profiling and statistical summaries\n",
    "- Schema validation and enforcement\n",
    "- Value range checks and constraints\n",
    "- Data type validation\n",
    "- Outlier detection methods\n",
    "- Data quality scoring\n",
    "- Assertions and testing\n",
    "- Building data quality pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Profiling\n",
    "\n",
    "Understanding your data through statistical summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "df = pl.DataFrame({\n",
    "    'customer_id': range(1, n+1),\n",
    "    'age': np.random.randint(18, 80, n),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n),\n",
    "    'purchase_amount': np.random.exponential(100, n),\n",
    "    'satisfaction_score': np.random.randint(1, 11, n),\n",
    "    'is_premium': np.random.choice([True, False], n, p=[0.3, 0.7]),\n",
    "    'signup_date': [date(2024, 1, 1) + timedelta(days=int(np.random.randint(0, 365))) for _ in range(n)]\n",
    "})\n",
    "\n",
    "print(\"Sample dataset:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic describe() - statistical summary\n",
    "print(\"Statistical summary:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comprehensive Data Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataframe(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Generate comprehensive profile of DataFrame.\"\"\"\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_data = df[col]\n",
    "        dtype = col_data.dtype\n",
    "        \n",
    "        profile = {\n",
    "            'column': col,\n",
    "            'dtype': str(dtype),\n",
    "            'count': len(col_data),\n",
    "            'null_count': col_data.null_count(),\n",
    "            'null_pct': round(col_data.null_count() / len(col_data) * 100, 2),\n",
    "            'unique_count': col_data.n_unique(),\n",
    "            'unique_pct': round(col_data.n_unique() / len(col_data) * 100, 2)\n",
    "        }\n",
    "        \n",
    "        # Add statistics for numeric columns\n",
    "        if dtype in [pl.Int64, pl.Int32, pl.Float64, pl.Float32, pl.UInt32, pl.UInt64]:\n",
    "            profile.update({\n",
    "                'mean': round(float(col_data.mean()), 2) if col_data.mean() is not None else None,\n",
    "                'std': round(float(col_data.std()), 2) if col_data.std() is not None else None,\n",
    "                'min': float(col_data.min()) if col_data.min() is not None else None,\n",
    "                'q25': float(col_data.quantile(0.25)) if col_data.quantile(0.25) is not None else None,\n",
    "                'median': float(col_data.median()) if col_data.median() is not None else None,\n",
    "                'q75': float(col_data.quantile(0.75)) if col_data.quantile(0.75) is not None else None,\n",
    "                'max': float(col_data.max()) if col_data.max() is not None else None\n",
    "            })\n",
    "        else:\n",
    "            profile.update({\n",
    "                'mean': None,\n",
    "                'std': None,\n",
    "                'min': None,\n",
    "                'q25': None,\n",
    "                'median': None,\n",
    "                'q75': None,\n",
    "                'max': None\n",
    "            })\n",
    "        \n",
    "        profiles.append(profile)\n",
    "    \n",
    "    return pl.DataFrame(profiles)\n",
    "\n",
    "profile = profile_dataframe(df)\n",
    "print(\"Comprehensive data profile:\")\n",
    "print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical columns\n",
    "print(\"Premium member distribution:\")\n",
    "print(df.group_by('is_premium').agg(pl.count()).sort('is_premium'))\n",
    "\n",
    "print(\"\\nSatisfaction score distribution:\")\n",
    "satisfaction_dist = df.group_by('satisfaction_score').agg(\n",
    "    pl.count().alias('count')\n",
    ").sort('satisfaction_score')\n",
    "print(satisfaction_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile analysis for numeric columns\n",
    "quantile_analysis = df.select([\n",
    "    pl.col('age').quantile(0.01).alias('age_p01'),\n",
    "    pl.col('age').quantile(0.05).alias('age_p05'),\n",
    "    pl.col('age').quantile(0.25).alias('age_p25'),\n",
    "    pl.col('age').quantile(0.50).alias('age_median'),\n",
    "    pl.col('age').quantile(0.75).alias('age_p75'),\n",
    "    pl.col('age').quantile(0.95).alias('age_p95'),\n",
    "    pl.col('age').quantile(0.99).alias('age_p99'),\n",
    "])\n",
    "\n",
    "print(\"Age quantiles:\")\n",
    "print(quantile_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Schema Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Expected Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected schema\n",
    "expected_schema = {\n",
    "    'customer_id': pl.Int64,\n",
    "    'age': pl.Int64,\n",
    "    'income': pl.Float64,\n",
    "    'purchase_amount': pl.Float64,\n",
    "    'satisfaction_score': pl.Int64,\n",
    "    'is_premium': pl.Boolean,\n",
    "    'signup_date': pl.Date\n",
    "}\n",
    "\n",
    "print(\"Expected schema:\")\n",
    "for col, dtype in expected_schema.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_schema(df: pl.DataFrame, expected_schema: Dict[str, pl.DataType]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate DataFrame schema against expected schema.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation results and errors\n",
    "    \"\"\"\n",
    "    actual_schema = df.schema\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check missing columns\n",
    "    missing_cols = set(expected_schema.keys()) - set(actual_schema.keys())\n",
    "    if missing_cols:\n",
    "        errors.append(f\"Missing columns: {missing_cols}\")\n",
    "    \n",
    "    # Check extra columns\n",
    "    extra_cols = set(actual_schema.keys()) - set(expected_schema.keys())\n",
    "    if extra_cols:\n",
    "        warnings.append(f\"Extra columns: {extra_cols}\")\n",
    "    \n",
    "    # Check type mismatches\n",
    "    for col in expected_schema:\n",
    "        if col in actual_schema:\n",
    "            if actual_schema[col] != expected_schema[col]:\n",
    "                errors.append(\n",
    "                    f\"Type mismatch in '{col}': expected {expected_schema[col]}, \"\n",
    "                    f\"got {actual_schema[col]}\"\n",
    "                )\n",
    "    \n",
    "    is_valid = len(errors) == 0\n",
    "    \n",
    "    return {\n",
    "        'is_valid': is_valid,\n",
    "        'errors': errors,\n",
    "        'warnings': warnings\n",
    "    }\n",
    "\n",
    "# Validate\n",
    "validation = validate_schema(df, expected_schema)\n",
    "\n",
    "if validation['is_valid']:\n",
    "    print(\"‚úÖ Schema validation passed!\")\n",
    "else:\n",
    "    print(\"‚ùå Schema validation failed!\")\n",
    "    for error in validation['errors']:\n",
    "        print(f\"  ERROR: {error}\")\n",
    "\n",
    "if validation['warnings']:\n",
    "    for warning in validation['warnings']:\n",
    "        print(f\"  WARNING: {warning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Column Presence Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_required_columns(df: pl.DataFrame, required_cols: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Check that all required columns are present.\"\"\"\n",
    "    missing = set(required_cols) - set(df.columns)\n",
    "    \n",
    "    return {\n",
    "        'is_valid': len(missing) == 0,\n",
    "        'missing_columns': list(missing)\n",
    "    }\n",
    "\n",
    "# Test\n",
    "required = ['customer_id', 'age', 'income', 'email']  # 'email' is missing\n",
    "result = validate_required_columns(df, required)\n",
    "\n",
    "if result['is_valid']:\n",
    "    print(\"‚úÖ All required columns present\")\n",
    "else:\n",
    "    print(f\"‚ùå Missing required columns: {result['missing_columns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Value Range Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Numeric Range Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_numeric_range(df: pl.DataFrame, col: str, min_val: float, max_val: float) -> Dict[str, Any]:\n",
    "    \"\"\"Validate numeric column is within expected range.\"\"\"\n",
    "    out_of_range = df.filter(\n",
    "        (pl.col(col) < min_val) | (pl.col(col) > max_val)\n",
    "    )\n",
    "    \n",
    "    actual_min = float(df[col].min())\n",
    "    actual_max = float(df[col].max())\n",
    "    \n",
    "    return {\n",
    "        'is_valid': len(out_of_range) == 0,\n",
    "        'violations': len(out_of_range),\n",
    "        'expected_range': (min_val, max_val),\n",
    "        'actual_range': (actual_min, actual_max),\n",
    "        'violation_examples': out_of_range.head(5) if len(out_of_range) > 0 else None\n",
    "    }\n",
    "\n",
    "# Validate age range\n",
    "age_validation = validate_numeric_range(df, 'age', 18, 100)\n",
    "\n",
    "print(\"Age range validation:\")\n",
    "if age_validation['is_valid']:\n",
    "    print(\"‚úÖ All ages within valid range\")\n",
    "else:\n",
    "    print(f\"‚ùå {age_validation['violations']} values out of range\")\n",
    "    print(f\"Expected: {age_validation['expected_range']}\")\n",
    "    print(f\"Actual: {age_validation['actual_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate satisfaction score\n",
    "satisfaction_validation = validate_numeric_range(df, 'satisfaction_score', 1, 10)\n",
    "\n",
    "print(\"Satisfaction score validation:\")\n",
    "if satisfaction_validation['is_valid']:\n",
    "    print(\"‚úÖ All scores within valid range (1-10)\")\n",
    "else:\n",
    "    print(f\"‚ùå {satisfaction_validation['violations']} scores out of range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Date Range Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_date_range(df: pl.DataFrame, col: str, min_date: date, max_date: date) -> Dict[str, Any]:\n",
    "    \"\"\"Validate date column is within expected range.\"\"\"\n",
    "    out_of_range = df.filter(\n",
    "        (pl.col(col) < min_date) | (pl.col(col) > max_date)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'is_valid': len(out_of_range) == 0,\n",
    "        'violations': len(out_of_range),\n",
    "        'expected_range': (min_date, max_date),\n",
    "        'violation_examples': out_of_range.head(5) if len(out_of_range) > 0 else None\n",
    "    }\n",
    "\n",
    "# Validate signup_date\n",
    "date_validation = validate_date_range(\n",
    "    df, 'signup_date', \n",
    "    date(2024, 1, 1), \n",
    "    date.today()\n",
    ")\n",
    "\n",
    "print(\"Signup date validation:\")\n",
    "if date_validation['is_valid']:\n",
    "    print(\"‚úÖ All dates within valid range\")\n",
    "else:\n",
    "    print(f\"‚ùå {date_validation['violations']} dates out of range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Categorical Value Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_categorical(df: pl.DataFrame, col: str, valid_values: List[Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Validate categorical column only contains valid values.\"\"\"\n",
    "    invalid = df.filter(~pl.col(col).is_in(valid_values))\n",
    "    unique_actual = df[col].unique().to_list()\n",
    "    \n",
    "    return {\n",
    "        'is_valid': len(invalid) == 0,\n",
    "        'violations': len(invalid),\n",
    "        'valid_values': valid_values,\n",
    "        'actual_values': unique_actual,\n",
    "        'invalid_values': list(set(unique_actual) - set(valid_values)),\n",
    "        'violation_examples': invalid.head(5) if len(invalid) > 0 else None\n",
    "    }\n",
    "\n",
    "# Example: Add status column and validate\n",
    "df_with_status = df.with_columns(\n",
    "    pl.when(pl.col('satisfaction_score') >= 8)\n",
    "      .then(pl.lit('satisfied'))\n",
    "      .when(pl.col('satisfaction_score') >= 5)\n",
    "      .then(pl.lit('neutral'))\n",
    "      .otherwise(pl.lit('unsatisfied'))\n",
    "      .alias('status')\n",
    ")\n",
    "\n",
    "status_validation = validate_categorical(\n",
    "    df_with_status, \n",
    "    'status', \n",
    "    ['satisfied', 'neutral', 'unsatisfied']\n",
    ")\n",
    "\n",
    "print(\"Status validation:\")\n",
    "if status_validation['is_valid']:\n",
    "    print(\"‚úÖ All status values valid\")\n",
    "    print(f\"Values: {status_validation['actual_values']}\")\n",
    "else:\n",
    "    print(f\"‚ùå {status_validation['violations']} invalid values\")\n",
    "    print(f\"Invalid values: {status_validation['invalid_values']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 IQR Method (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df: pl.DataFrame, col: str, multiplier: float = 1.5) -> pl.DataFrame:\n",
    "    \"\"\"Detect outliers using IQR method.\n",
    "    \n",
    "    Outliers are values outside [Q1 - multiplier*IQR, Q3 + multiplier*IQR]\n",
    "    \"\"\"\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    \n",
    "    outliers = df.filter(\n",
    "        (pl.col(col) < lower_bound) | (pl.col(col) > upper_bound)\n",
    "    )\n",
    "    \n",
    "    print(f\"Outlier detection for '{col}' (IQR method):\")\n",
    "    print(f\"  Q1: {q1:.2f}\")\n",
    "    print(f\"  Q3: {q3:.2f}\")\n",
    "    print(f\"  IQR: {iqr:.2f}\")\n",
    "    print(f\"  Lower bound: {lower_bound:.2f}\")\n",
    "    print(f\"  Upper bound: {upper_bound:.2f}\")\n",
    "    print(f\"  Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in income\n",
    "income_outliers = detect_outliers_iqr(df, 'income')\n",
    "print(\"\\nOutlier examples:\")\n",
    "print(income_outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df: pl.DataFrame, col: str, threshold: float = 3.0) -> pl.DataFrame:\n",
    "    \"\"\"Detect outliers using Z-score method.\n",
    "    \n",
    "    Outliers are values with |Z-score| > threshold (typically 3)\n",
    "    \"\"\"\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "    \n",
    "    df_with_zscore = df.with_columns(\n",
    "        ((pl.col(col) - mean) / std).alias('z_score')\n",
    "    )\n",
    "    \n",
    "    outliers = df_with_zscore.filter(\n",
    "        pl.col('z_score').abs() > threshold\n",
    "    )\n",
    "    \n",
    "    print(f\"Outlier detection for '{col}' (Z-score method):\")\n",
    "    print(f\"  Mean: {mean:.2f}\")\n",
    "    print(f\"  Std Dev: {std:.2f}\")\n",
    "    print(f\"  Threshold: {threshold}\")\n",
    "    print(f\"  Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect outliers in purchase_amount\n",
    "purchase_outliers = detect_outliers_zscore(df, 'purchase_amount')\n",
    "print(\"\\nOutlier examples:\")\n",
    "print(purchase_outliers.select(['customer_id', 'purchase_amount', 'z_score']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Percentile Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_percentile(df: pl.DataFrame, col: str, lower_pct: float = 0.01, upper_pct: float = 0.99) -> pl.DataFrame:\n",
    "    \"\"\"Detect outliers using percentile method.\n",
    "    \n",
    "    Outliers are values below lower_pct or above upper_pct\n",
    "    \"\"\"\n",
    "    lower_bound = df[col].quantile(lower_pct)\n",
    "    upper_bound = df[col].quantile(upper_pct)\n",
    "    \n",
    "    outliers = df.filter(\n",
    "        (pl.col(col) < lower_bound) | (pl.col(col) > upper_bound)\n",
    "    )\n",
    "    \n",
    "    print(f\"Outlier detection for '{col}' (Percentile method):\")\n",
    "    print(f\"  {lower_pct*100:.0f}th percentile: {lower_bound:.2f}\")\n",
    "    print(f\"  {upper_pct*100:.0f}th percentile: {upper_bound:.2f}\")\n",
    "    print(f\"  Outliers found: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Detect extreme age values\n",
    "age_outliers = detect_outliers_percentile(df, 'age', 0.01, 0.99)\n",
    "print(\"\\nOutlier examples:\")\n",
    "print(age_outliers.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Data Quality Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_score(df: pl.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"Calculate overall data quality score (0-100).\n",
    "    \n",
    "    Factors:\n",
    "    - Completeness (null percentage)\n",
    "    - Uniqueness (duplicate percentage)\n",
    "    - Consistency (type correctness)\n",
    "    \"\"\"\n",
    "    total_cells = len(df) * len(df.columns)\n",
    "    \n",
    "    # Completeness: Percentage of non-null values\n",
    "    null_cells = df.null_count().sum_horizontal()[0]\n",
    "    completeness = (1 - null_cells / total_cells) * 100\n",
    "    \n",
    "    # Uniqueness: For columns that should be unique (e.g., customer_id)\n",
    "    # Here we check if customer_id has duplicates\n",
    "    if 'customer_id' in df.columns:\n",
    "        duplicates = df['customer_id'].is_duplicated().sum()\n",
    "        uniqueness = (1 - duplicates / len(df)) * 100\n",
    "    else:\n",
    "        uniqueness = 100\n",
    "    \n",
    "    # Consistency: Check if numeric columns are numeric, etc.\n",
    "    # Simplified: assuming schema is correct\n",
    "    consistency = 100\n",
    "    \n",
    "    # Overall score (weighted average)\n",
    "    overall = (completeness * 0.4 + uniqueness * 0.3 + consistency * 0.3)\n",
    "    \n",
    "    return {\n",
    "        'completeness': round(completeness, 2),\n",
    "        'uniqueness': round(uniqueness, 2),\n",
    "        'consistency': round(consistency, 2),\n",
    "        'overall': round(overall, 2)\n",
    "    }\n",
    "\n",
    "quality = calculate_quality_score(df)\n",
    "\n",
    "print(\"Data Quality Score:\")\n",
    "for metric, score in quality.items():\n",
    "    print(f\"  {metric.capitalize()}: {score}/100\")\n",
    "\n",
    "if quality['overall'] >= 90:\n",
    "    print(\"\\n‚úÖ Excellent data quality!\")\n",
    "elif quality['overall'] >= 70:\n",
    "    print(\"\\n‚ö†Ô∏è Good data quality, some improvements needed\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Poor data quality, significant cleanup required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Assertions and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_no_nulls(df: pl.DataFrame, columns: List[str] = None):\n",
    "    \"\"\"Assert that specified columns have no null values.\"\"\"\n",
    "    cols_to_check = columns if columns else df.columns\n",
    "    \n",
    "    for col in cols_to_check:\n",
    "        null_count = df[col].null_count()\n",
    "        assert null_count == 0, f\"Column '{col}' has {null_count} null values\"\n",
    "    \n",
    "    print(f\"‚úÖ No nulls in {cols_to_check}\")\n",
    "\n",
    "def assert_unique(df: pl.DataFrame, column: str):\n",
    "    \"\"\"Assert that column has only unique values.\"\"\"\n",
    "    duplicates = df[column].is_duplicated().sum()\n",
    "    assert duplicates == 0, f\"Column '{column}' has {duplicates} duplicates\"\n",
    "    print(f\"‚úÖ Column '{column}' has only unique values\")\n",
    "\n",
    "def assert_range(df: pl.DataFrame, column: str, min_val, max_val):\n",
    "    \"\"\"Assert that all values are within specified range.\"\"\"\n",
    "    violations = df.filter(\n",
    "        (pl.col(column) < min_val) | (pl.col(column) > max_val)\n",
    "    )\n",
    "    assert len(violations) == 0, f\"Column '{column}' has {len(violations)} values outside [{min_val}, {max_val}]\"\n",
    "    print(f\"‚úÖ All '{column}' values in range [{min_val}, {max_val}]\")\n",
    "\n",
    "# Run assertions\n",
    "try:\n",
    "    assert_no_nulls(df, ['customer_id', 'age'])\n",
    "    assert_unique(df, 'customer_id')\n",
    "    assert_range(df, 'age', 18, 100)\n",
    "    assert_range(df, 'satisfaction_score', 1, 10)\n",
    "    print(\"\\n‚úÖ All assertions passed!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"\\n‚ùå Assertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Data Quality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityTest:\n",
    "    \"\"\"Data quality test suite.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df\n",
    "        self.results = []\n",
    "    \n",
    "    def test_no_duplicates(self, column: str) -> bool:\n",
    "        \"\"\"Test: No duplicates in column.\"\"\"\n",
    "        duplicates = self.df[column].is_duplicated().sum()\n",
    "        passed = duplicates == 0\n",
    "        self.results.append({\n",
    "            'test': f'no_duplicates_{column}',\n",
    "            'passed': passed,\n",
    "            'message': f\"Found {duplicates} duplicates\" if not passed else \"OK\"\n",
    "        })\n",
    "        return passed\n",
    "    \n",
    "    def test_no_nulls(self, column: str) -> bool:\n",
    "        \"\"\"Test: No nulls in column.\"\"\"\n",
    "        nulls = self.df[column].null_count()\n",
    "        passed = nulls == 0\n",
    "        self.results.append({\n",
    "            'test': f'no_nulls_{column}',\n",
    "            'passed': passed,\n",
    "            'message': f\"Found {nulls} nulls\" if not passed else \"OK\"\n",
    "        })\n",
    "        return passed\n",
    "    \n",
    "    def test_value_range(self, column: str, min_val, max_val) -> bool:\n",
    "        \"\"\"Test: Values within expected range.\"\"\"\n",
    "        violations = self.df.filter(\n",
    "            (pl.col(column) < min_val) | (pl.col(column) > max_val)\n",
    "        )\n",
    "        passed = len(violations) == 0\n",
    "        self.results.append({\n",
    "            'test': f'range_{column}',\n",
    "            'passed': passed,\n",
    "            'message': f\"Found {len(violations)} out of range\" if not passed else \"OK\"\n",
    "        })\n",
    "        return passed\n",
    "    \n",
    "    def test_data_types(self, expected_schema: Dict[str, pl.DataType]) -> bool:\n",
    "        \"\"\"Test: Correct data types.\"\"\"\n",
    "        mismatches = []\n",
    "        for col, expected_type in expected_schema.items():\n",
    "            if col in self.df.schema:\n",
    "                if self.df.schema[col] != expected_type:\n",
    "                    mismatches.append(col)\n",
    "        \n",
    "        passed = len(mismatches) == 0\n",
    "        self.results.append({\n",
    "            'test': 'data_types',\n",
    "            'passed': passed,\n",
    "            'message': f\"Type mismatches: {mismatches}\" if not passed else \"OK\"\n",
    "        })\n",
    "        return passed\n",
    "    \n",
    "    def run_all_tests(self) -> pl.DataFrame:\n",
    "        \"\"\"Run all tests and return results.\"\"\"\n",
    "        # Run tests\n",
    "        self.test_no_duplicates('customer_id')\n",
    "        self.test_no_nulls('customer_id')\n",
    "        self.test_no_nulls('age')\n",
    "        self.test_value_range('age', 18, 100)\n",
    "        self.test_value_range('satisfaction_score', 1, 10)\n",
    "        self.test_data_types(expected_schema)\n",
    "        \n",
    "        return pl.DataFrame(self.results)\n",
    "\n",
    "# Run test suite\n",
    "tester = DataQualityTest(df)\n",
    "test_results = tester.run_all_tests()\n",
    "\n",
    "print(\"Data Quality Test Results:\")\n",
    "print(test_results)\n",
    "\n",
    "# Summary\n",
    "total_tests = len(test_results)\n",
    "passed_tests = test_results.filter(pl.col('passed')).height\n",
    "print(f\"\\n{passed_tests}/{total_tests} tests passed\")\n",
    "\n",
    "if passed_tests == total_tests:\n",
    "    print(\"‚úÖ All tests passed!\")\n",
    "else:\n",
    "    print(\"‚ùå Some tests failed\")\n",
    "    print(\"\\nFailed tests:\")\n",
    "    print(test_results.filter(~pl.col('passed')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Complete Data Quality Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_data_quality_report(df: pl.DataFrame, \n",
    "                                       expected_schema: Dict[str, pl.DataType] = None) -> Dict:\n",
    "    \"\"\"Generate comprehensive data quality report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'dataset_info': {\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'total_cells': len(df) * len(df.columns)\n",
    "        },\n",
    "        'profile': profile_dataframe(df),\n",
    "        'quality_score': calculate_quality_score(df),\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check for issues\n",
    "    # 1. Null values\n",
    "    for col in df.columns:\n",
    "        null_count = df[col].null_count()\n",
    "        if null_count > 0:\n",
    "            report['issues'].append({\n",
    "                'type': 'nulls',\n",
    "                'column': col,\n",
    "                'count': null_count,\n",
    "                'percentage': round(null_count / len(df) * 100, 2)\n",
    "            })\n",
    "    \n",
    "    # 2. Duplicates in key columns\n",
    "    if 'customer_id' in df.columns:\n",
    "        dup_count = df['customer_id'].is_duplicated().sum()\n",
    "        if dup_count > 0:\n",
    "            report['issues'].append({\n",
    "                'type': 'duplicates',\n",
    "                'column': 'customer_id',\n",
    "                'count': dup_count\n",
    "            })\n",
    "    \n",
    "    # 3. Schema validation (if expected schema provided)\n",
    "    if expected_schema:\n",
    "        schema_result = validate_schema(df, expected_schema)\n",
    "        if not schema_result['is_valid']:\n",
    "            for error in schema_result['errors']:\n",
    "                report['issues'].append({\n",
    "                    'type': 'schema',\n",
    "                    'message': error\n",
    "                })\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate comprehensive report\n",
    "report = comprehensive_data_quality_report(df, expected_schema)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä Dataset Info:\")\n",
    "for key, value in report['dataset_info'].items():\n",
    "    print(f\"  {key}: {value:,}\")\n",
    "\n",
    "print(\"\\nüéØ Quality Score:\")\n",
    "for metric, score in report['quality_score'].items():\n",
    "    print(f\"  {metric.capitalize()}: {score}/100\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Issues Found:\")\n",
    "if report['issues']:\n",
    "    for i, issue in enumerate(report['issues'], 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No issues found!\")\n",
    "\n",
    "print(\"\\nüìà Column Profile:\")\n",
    "print(report['profile'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Data Quality Dimensions:\n",
    "\n",
    "| Dimension | What to Check | Methods |\n",
    "|-----------|---------------|----------|\n",
    "| **Completeness** | Missing values | null_count(), fill strategies |\n",
    "| **Uniqueness** | Duplicates | is_duplicated(), unique() |\n",
    "| **Validity** | Value ranges, types | Range checks, schema validation |\n",
    "| **Consistency** | Data type correctness | Type validation, format checks |\n",
    "| **Accuracy** | Outliers, anomalies | IQR, Z-score, percentile methods |\n",
    "\n",
    "## Best Practices:\n",
    "\n",
    "1. ‚úÖ **Profile early** - Understand data before processing\n",
    "2. ‚úÖ **Define expectations** - Schema, ranges, constraints\n",
    "3. ‚úÖ **Automate validation** - Test suite for data pipelines\n",
    "4. ‚úÖ **Score quality** - Track quality over time\n",
    "5. ‚úÖ **Document issues** - Log all data quality problems\n",
    "6. ‚úÖ **Use assertions** - Fail fast on bad data\n",
    "7. ‚úÖ **Monitor outliers** - Multiple detection methods\n",
    "8. ‚úÖ **Validate schema** - Type safety prevents errors\n",
    "\n",
    "## Common Patterns:\n",
    "```python\n",
    "# Profile data\n",
    "df.describe()\n",
    "df.null_count()\n",
    "\n",
    "# Validate schema\n",
    "assert df.schema == expected_schema\n",
    "\n",
    "# Check ranges\n",
    "assert df.filter((pl.col('age') < 0) | (pl.col('age') > 120)).height == 0\n",
    "\n",
    "# Detect outliers\n",
    "q1, q3 = df['value'].quantile([0.25, 0.75])\n",
    "iqr = q3 - q1\n",
    "outliers = df.filter((pl.col('value') < q1 - 1.5*iqr) | (pl.col('value') > q3 + 1.5*iqr))\n",
    "```\n",
    "\n",
    "## Data Quality Pipeline:\n",
    "1. **Profile** - Understand data distribution\n",
    "2. **Validate** - Check schema and constraints\n",
    "3. **Clean** - Fix issues (nulls, duplicates)\n",
    "4. **Monitor** - Track quality metrics\n",
    "5. **Alert** - Notify on quality degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a custom validation function\n",
    "# TODO: Write a function to validate email format using regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Build a data quality dashboard\n",
    "# TODO: Create a summary showing all quality metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement custom outlier detection\n",
    "# TODO: Combine IQR and Z-score methods for robust detection\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
