{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Time-Series Analysis - group_by_dynamic & Advanced Features\n",
    "\n",
    "Deep dive into Polars' powerful time-series capabilities.\n",
    "\n",
    "## Topics:\n",
    "- `group_by_dynamic` - Dynamic time-based grouping\n",
    "- `group_by_rolling` - Rolling window aggregations\n",
    "- Time-based resampling (upsampling/downsampling)\n",
    "- Sliding windows and offset configurations\n",
    "- Time-based joins and asof joins\n",
    "- Handling irregular time series\n",
    "- Business calendar operations\n",
    "- Real-world financial and IoT time-series examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime, date, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding group_by_dynamic\n",
    "\n",
    "`group_by_dynamic` is one of Polars' most powerful features for time-series analysis. It allows you to:\n",
    "- Group data by time-based windows\n",
    "- Handle irregular time series\n",
    "- Create custom rolling windows\n",
    "- Resample data to different frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic group_by_dynamic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with irregular timestamps\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pl.DataFrame({\n",
    "    'timestamp': [\n",
    "        datetime(2024, 1, 1, 0, 5),\n",
    "        datetime(2024, 1, 1, 0, 17),\n",
    "        datetime(2024, 1, 1, 0, 28),\n",
    "        datetime(2024, 1, 1, 0, 45),\n",
    "        datetime(2024, 1, 1, 1, 3),\n",
    "        datetime(2024, 1, 1, 1, 22),\n",
    "        datetime(2024, 1, 1, 1, 47),\n",
    "        datetime(2024, 1, 1, 2, 15),\n",
    "    ],\n",
    "    'value': [10, 15, 8, 12, 20, 18, 14, 22]\n",
    "})\n",
    "\n",
    "print(\"Original irregular data:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 30-minute windows\n",
    "result_30min = df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='30m'\n",
    ").agg([\n",
    "    pl.col('value').sum().alias('sum'),\n",
    "    pl.col('value').mean().alias('mean'),\n",
    "    pl.col('value').count().alias('count')\n",
    "])\n",
    "\n",
    "print(\"\\nGrouped by 30-minute windows:\")\n",
    "print(result_30min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'every' Parameter - Time Window Sizes\n",
    "\n",
    "The `every` parameter supports various time units:\n",
    "- `ns` - nanoseconds\n",
    "- `us` - microseconds  \n",
    "- `ms` - milliseconds\n",
    "- `s` - seconds\n",
    "- `m` - minutes\n",
    "- `h` - hours\n",
    "- `d` - days\n",
    "- `w` - weeks\n",
    "- `mo` - months\n",
    "- `q` - quarters\n",
    "- `y` - years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different window sizes\n",
    "print(\"1-hour windows:\")\n",
    "print(df.group_by_dynamic('timestamp', every='1h').agg(pl.col('value').sum()))\n",
    "\n",
    "print(\"\\n15-minute windows:\")\n",
    "print(df.group_by_dynamic('timestamp', every='15m').agg(pl.col('value').sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'period' Parameter - Window Duration\n",
    "\n",
    "While `every` determines window start points, `period` determines the window duration.\n",
    "This enables overlapping or non-overlapping windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hourly data for clearer demonstration\n",
    "hourly_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1, 0, 0),\n",
    "        datetime(2024, 1, 1, 12, 0),\n",
    "        '1h',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': range(13)\n",
    "})\n",
    "\n",
    "print(\"Original hourly data:\")\n",
    "print(hourly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every=3h, period=3h (non-overlapping 3-hour windows)\n",
    "non_overlapping = hourly_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='3h',\n",
    "    period='3h'\n",
    ").agg([\n",
    "    pl.col('value').sum().alias('sum'),\n",
    "    pl.col('value').count().alias('count')\n",
    "])\n",
    "\n",
    "print(\"\\nNon-overlapping 3-hour windows:\")\n",
    "print(non_overlapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every=1h, period=3h (overlapping windows - sliding window)\n",
    "overlapping = hourly_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='1h',\n",
    "    period='3h'\n",
    ").agg([\n",
    "    pl.col('value').sum().alias('sum'),\n",
    "    pl.col('value').count().alias('count')\n",
    "])\n",
    "\n",
    "print(\"\\nOverlapping 3-hour windows (sliding every 1 hour):\")\n",
    "print(overlapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'offset' Parameter - Shifting Window Boundaries\n",
    "\n",
    "The `offset` parameter shifts when windows start, useful for aligning to specific times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default: windows start at midnight\n",
    "default_windows = hourly_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='4h'\n",
    ").agg(pl.col('value').sum())\n",
    "\n",
    "print(\"Windows starting at midnight (default):\")\n",
    "print(default_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset by 2 hours: windows start at 02:00, 06:00, 10:00, etc.\n",
    "offset_windows = hourly_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='4h',\n",
    "    offset='2h'\n",
    ").agg(pl.col('value').sum())\n",
    "\n",
    "print(\"\\nWindows offset by 2 hours:\")\n",
    "print(offset_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'truncate' Parameter - Window Alignment\n",
    "\n",
    "Controls whether the window boundaries are truncated to the time unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data starting at an odd time\n",
    "odd_start_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1, 0, 17),  # Starts at 00:17\n",
    "        datetime(2024, 1, 1, 5, 17),\n",
    "        '1h',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': range(6)\n",
    "})\n",
    "\n",
    "print(\"Data starting at 00:17:\")\n",
    "print(odd_start_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate=True (default): windows align to clean boundaries\n",
    "truncated = odd_start_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='2h',\n",
    "    truncate=True\n",
    ").agg(pl.col('value').sum())\n",
    "\n",
    "print(\"\\nWith truncate=True (aligned to hour boundaries):\")\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate=False: windows start from first data point\n",
    "not_truncated = odd_start_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='2h',\n",
    "    truncate=False\n",
    ").agg(pl.col('value').sum())\n",
    "\n",
    "print(\"\\nWith truncate=False (windows from first data point):\")\n",
    "print(not_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'include_boundaries' Parameter\n",
    "\n",
    "Adds explicit boundary columns showing window start/end times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_boundaries = hourly_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='3h',\n",
    "    include_boundaries=True\n",
    ").agg([\n",
    "    pl.col('value').sum().alias('sum')\n",
    "])\n",
    "\n",
    "print(\"With window boundaries:\")\n",
    "print(with_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: group_by_rolling - Rolling Window Aggregations\n",
    "\n",
    "`group_by_rolling` creates rolling (moving) windows based on the data itself rather than fixed time boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample time series data\n",
    "ts_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 10),\n",
    "        '1d',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': [10, 12, 8, 15, 11, 14, 9, 13, 16, 10]\n",
    "})\n",
    "\n",
    "print(\"Daily time series data:\")\n",
    "print(ts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling 3-day window\n",
    "rolling_3d = ts_df.group_by_rolling(\n",
    "    'timestamp',\n",
    "    period='3d'\n",
    ").agg([\n",
    "    pl.col('value').mean().alias('rolling_mean'),\n",
    "    pl.col('value').sum().alias('rolling_sum'),\n",
    "    pl.col('value').std().alias('rolling_std')\n",
    "])\n",
    "\n",
    "print(\"\\n3-day rolling window statistics:\")\n",
    "print(rolling_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed Parameter - Window Boundary Inclusion\n",
    "\n",
    "Controls which boundaries are included in the window:\n",
    "- `'right'` (default): includes right boundary, excludes left\n",
    "- `'left'`: includes left boundary, excludes right\n",
    "- `'both'`: includes both boundaries\n",
    "- `'none'`: excludes both boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different closed options\n",
    "simple_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 5),\n",
    "        '1d',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': [1, 2, 3, 4, 5]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(simple_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-day rolling with different closed options\n",
    "for closed in ['right', 'left', 'both', 'none']:\n",
    "    result = simple_df.group_by_rolling(\n",
    "        'timestamp',\n",
    "        period='2d',\n",
    "        closed=closed\n",
    "    ).agg([\n",
    "        pl.col('value').sum().alias(f'sum_{closed}')\n",
    "    ])\n",
    "    print(f\"\\n2-day rolling sum with closed='{closed}':\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offset in group_by_rolling\n",
    "\n",
    "The offset parameter in rolling windows shifts the window backwards or forwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window with negative offset (look backward)\n",
    "backward_rolling = simple_df.group_by_rolling(\n",
    "    'timestamp',\n",
    "    period='2d',\n",
    "    offset='-1d'\n",
    ").agg([\n",
    "    pl.col('value').sum().alias('sum')\n",
    "])\n",
    "\n",
    "print(\"Rolling window offset backward by 1 day:\")\n",
    "print(backward_rolling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced Time-Series Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling - Reducing Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-frequency data (every minute)\n",
    "np.random.seed(42)\n",
    "high_freq = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1, 0, 0),\n",
    "        datetime(2024, 1, 1, 2, 0),\n",
    "        '1m',\n",
    "        eager=True\n",
    "    ),\n",
    "    'temperature': np.random.normal(20, 2, 121),\n",
    "    'humidity': np.random.normal(60, 5, 121)\n",
    "})\n",
    "\n",
    "print(f\"High-frequency data: {len(high_freq)} records (1-minute intervals)\")\n",
    "print(high_freq.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample to 15-minute intervals\n",
    "downsampled_15m = high_freq.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='15m'\n",
    ").agg([\n",
    "    pl.col('temperature').mean().alias('temp_mean'),\n",
    "    pl.col('temperature').min().alias('temp_min'),\n",
    "    pl.col('temperature').max().alias('temp_max'),\n",
    "    pl.col('humidity').mean().alias('humidity_mean')\n",
    "])\n",
    "\n",
    "print(f\"\\nDownsampled to 15-minute intervals: {len(downsampled_15m)} records\")\n",
    "print(downsampled_15m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling - Increasing Frequency\n",
    "\n",
    "For upsampling, we create the desired time range and join with original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse data (hourly)\n",
    "sparse_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1, 0, 0),\n",
    "        datetime(2024, 1, 1, 5, 0),\n",
    "        '1h',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': [10, 15, 12, 18, 14, 20]\n",
    "})\n",
    "\n",
    "print(\"Sparse hourly data:\")\n",
    "print(sparse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 15-minute grid\n",
    "upsampled_grid = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1, 0, 0),\n",
    "        datetime(2024, 1, 1, 5, 0),\n",
    "        '15m',\n",
    "        eager=True\n",
    "    )\n",
    "})\n",
    "\n",
    "# Forward fill (use last known value)\n",
    "upsampled = upsampled_grid.join_asof(\n",
    "    sparse_df,\n",
    "    on='timestamp',\n",
    "    strategy='forward'\n",
    ")\n",
    "\n",
    "print(\"\\nUpsampled to 15-minute intervals (forward fill):\")\n",
    "print(upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation for upsampling\n",
    "upsampled_interp = upsampled_grid.join(\n",
    "    sparse_df,\n",
    "    on='timestamp',\n",
    "    how='left'\n",
    ").with_columns([\n",
    "    pl.col('value').interpolate().alias('value_interpolated')\n",
    "])\n",
    "\n",
    "print(\"\\nUpsampled with linear interpolation:\")\n",
    "print(upsampled_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Column and Grouped Dynamic Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group_by_dynamic with Multiple Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-sensor data\n",
    "np.random.seed(42)\n",
    "sensors_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1, 0, 0),\n",
    "        datetime(2024, 1, 1, 6, 0),\n",
    "        '30m',\n",
    "        eager=True\n",
    "    ).repeat_by(3).explode(),\n",
    "    'sensor_id': ['sensor_A', 'sensor_B', 'sensor_C'] * 13,\n",
    "    'reading': np.random.uniform(10, 30, 39)\n",
    "}).sort(['sensor_id', 'timestamp'])\n",
    "\n",
    "print(\"Multi-sensor readings:\")\n",
    "print(sensors_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by sensor AND time window\n",
    "sensor_hourly = sensors_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='2h',\n",
    "    by='sensor_id'  # Additional grouping column\n",
    ").agg([\n",
    "    pl.col('reading').mean().alias('avg_reading'),\n",
    "    pl.col('reading').std().alias('std_reading'),\n",
    "    pl.col('reading').count().alias('num_readings')\n",
    "]).sort(['sensor_id', 'timestamp'])\n",
    "\n",
    "print(\"\\n2-hour aggregations per sensor:\")\n",
    "print(sensor_hourly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ASOF Joins - Time-Series Joins\n",
    "\n",
    "ASOF (as-of) joins are crucial for time-series data where exact timestamp matches are rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock prices (sampled every few minutes)\n",
    "stock_prices = pl.DataFrame({\n",
    "    'timestamp': [\n",
    "        datetime(2024, 1, 1, 9, 0),\n",
    "        datetime(2024, 1, 1, 9, 5),\n",
    "        datetime(2024, 1, 1, 9, 12),\n",
    "        datetime(2024, 1, 1, 9, 18),\n",
    "        datetime(2024, 1, 1, 9, 25),\n",
    "    ],\n",
    "    'price': [100.0, 101.5, 99.8, 102.3, 103.1]\n",
    "})\n",
    "\n",
    "# Trade events (irregular timing)\n",
    "trades = pl.DataFrame({\n",
    "    'timestamp': [\n",
    "        datetime(2024, 1, 1, 9, 3),\n",
    "        datetime(2024, 1, 1, 9, 8),\n",
    "        datetime(2024, 1, 1, 9, 15),\n",
    "        datetime(2024, 1, 1, 9, 23),\n",
    "    ],\n",
    "    'quantity': [100, 200, 150, 300]\n",
    "})\n",
    "\n",
    "print(\"Stock prices:\")\n",
    "print(stock_prices)\n",
    "print(\"\\nTrades:\")\n",
    "print(trades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join trades with most recent price (backward strategy)\n",
    "trades_with_price = trades.join_asof(\n",
    "    stock_prices,\n",
    "    on='timestamp',\n",
    "    strategy='backward'  # Use most recent price before trade\n",
    ")\n",
    "\n",
    "print(\"\\nTrades matched with most recent price (backward):\")\n",
    "print(trades_with_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward strategy - use next available price\n",
    "trades_forward = trades.join_asof(\n",
    "    stock_prices,\n",
    "    on='timestamp',\n",
    "    strategy='forward'\n",
    ")\n",
    "\n",
    "print(\"\\nTrades matched with next available price (forward):\")\n",
    "print(trades_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest strategy - use closest price in time\n",
    "trades_nearest = trades.join_asof(\n",
    "    stock_prices,\n",
    "    on='timestamp',\n",
    "    strategy='nearest'\n",
    ")\n",
    "\n",
    "print(\"\\nTrades matched with nearest price:\")\n",
    "print(trades_nearest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASOF Join with Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only match if price is within 3 minutes of trade\n",
    "trades_with_tolerance = trades.join_asof(\n",
    "    stock_prices,\n",
    "    on='timestamp',\n",
    "    strategy='backward',\n",
    "    tolerance='3m'\n",
    ")\n",
    "\n",
    "print(\"\\nTrades with price (max 3-minute tolerance):\")\n",
    "print(trades_with_tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASOF Join with Multiple Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple stocks\n",
    "multi_stock_prices = pl.DataFrame({\n",
    "    'timestamp': [\n",
    "        datetime(2024, 1, 1, 9, 0),\n",
    "        datetime(2024, 1, 1, 9, 0),\n",
    "        datetime(2024, 1, 1, 9, 10),\n",
    "        datetime(2024, 1, 1, 9, 10),\n",
    "        datetime(2024, 1, 1, 9, 20),\n",
    "        datetime(2024, 1, 1, 9, 20),\n",
    "    ],\n",
    "    'symbol': ['AAPL', 'GOOGL', 'AAPL', 'GOOGL', 'AAPL', 'GOOGL'],\n",
    "    'price': [150.0, 2800.0, 151.5, 2795.0, 149.8, 2810.0]\n",
    "})\n",
    "\n",
    "multi_trades = pl.DataFrame({\n",
    "    'timestamp': [\n",
    "        datetime(2024, 1, 1, 9, 5),\n",
    "        datetime(2024, 1, 1, 9, 12),\n",
    "        datetime(2024, 1, 1, 9, 15),\n",
    "    ],\n",
    "    'symbol': ['AAPL', 'GOOGL', 'AAPL'],\n",
    "    'quantity': [100, 50, 200]\n",
    "})\n",
    "\n",
    "# ASOF join on both timestamp AND symbol\n",
    "matched_trades = multi_trades.join_asof(\n",
    "    multi_stock_prices,\n",
    "    on='timestamp',\n",
    "    by='symbol',\n",
    "    strategy='backward'\n",
    ")\n",
    "\n",
    "print(\"\\nTrades matched by symbol and time:\")\n",
    "print(matched_trades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Real-World Example 1 - Financial Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate tick data (high-frequency stock prices)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate irregular timestamps (realistic trading scenario)\n",
    "base_time = datetime(2024, 1, 1, 9, 30)  # Market open\n",
    "num_ticks = 1000\n",
    "\n",
    "# Random millisecond intervals between ticks\n",
    "intervals_ms = np.random.exponential(500, num_ticks).astype(int)\n",
    "cumulative_ms = np.cumsum(intervals_ms)\n",
    "\n",
    "tick_data = pl.DataFrame({\n",
    "    'timestamp': [base_time + timedelta(milliseconds=int(ms)) for ms in cumulative_ms],\n",
    "    'price': 100 + np.cumsum(np.random.normal(0, 0.1, num_ticks)),\n",
    "    'volume': np.random.randint(100, 1000, num_ticks)\n",
    "})\n",
    "\n",
    "print(f\"Tick data: {len(tick_data)} records\")\n",
    "print(tick_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OHLC (Open-High-Low-Close) Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-minute OHLC bars\n",
    "ohlc_1m = tick_data.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='1m'\n",
    ").agg([\n",
    "    pl.col('price').first().alias('open'),\n",
    "    pl.col('price').max().alias('high'),\n",
    "    pl.col('price').min().alias('low'),\n",
    "    pl.col('price').last().alias('close'),\n",
    "    pl.col('volume').sum().alias('volume'),\n",
    "    pl.col('price').count().alias('num_ticks')\n",
    "])\n",
    "\n",
    "print(\"\\n1-minute OHLC bars:\")\n",
    "print(ohlc_1m.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate common technical indicators using rolling windows\n",
    "technical_indicators = ohlc_1m.with_columns([\n",
    "    # Simple Moving Average (SMA)\n",
    "    pl.col('close').rolling_mean(window_size=5).alias('sma_5'),\n",
    "    pl.col('close').rolling_mean(window_size=20).alias('sma_20'),\n",
    "    \n",
    "    # Exponential Moving Average (EMA) - using built-in ewm_mean\n",
    "    pl.col('close').ewm_mean(span=5).alias('ema_5'),\n",
    "    pl.col('close').ewm_mean(span=20).alias('ema_20'),\n",
    "    \n",
    "    # Bollinger Bands (20-period)\n",
    "    pl.col('close').rolling_std(window_size=20).alias('std_20'),\n",
    "]).with_columns([\n",
    "    # Calculate Bollinger Bands\n",
    "    (pl.col('sma_20') + 2 * pl.col('std_20')).alias('bb_upper'),\n",
    "    (pl.col('sma_20') - 2 * pl.col('std_20')).alias('bb_lower'),\n",
    "    \n",
    "    # Calculate returns\n",
    "    (pl.col('close') / pl.col('close').shift(1) - 1).alias('return_1m'),\n",
    "]).with_columns([\n",
    "    # Volatility (rolling standard deviation of returns)\n",
    "    pl.col('return_1m').rolling_std(window_size=20).alias('volatility_20')\n",
    "])\n",
    "\n",
    "print(\"\\nTechnical indicators:\")\n",
    "print(technical_indicators.select([\n",
    "    'timestamp', 'close', 'sma_5', 'sma_20', 'ema_5', 'ema_20', \n",
    "    'bb_upper', 'bb_lower', 'return_1m', 'volatility_20'\n",
    "]).tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume-Weighted Average Price (VWAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VWAP for each 5-minute period\n",
    "vwap_5m = tick_data.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='5m'\n",
    ").agg([\n",
    "    (pl.col('price') * pl.col('volume')).sum().alias('price_volume'),\n",
    "    pl.col('volume').sum().alias('total_volume')\n",
    "]).with_columns([\n",
    "    (pl.col('price_volume') / pl.col('total_volume')).alias('vwap')\n",
    "]).select(['timestamp', 'vwap', 'total_volume'])\n",
    "\n",
    "print(\"\\n5-minute VWAP:\")\n",
    "print(vwap_5m.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Real-World Example 2 - IoT Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate IoT sensor data with missing readings and irregular intervals\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 1 week of data with some gaps\n",
    "all_timestamps = pl.datetime_range(\n",
    "    datetime(2024, 1, 1, 0, 0),\n",
    "    datetime(2024, 1, 7, 23, 59),\n",
    "    '5m',\n",
    "    eager=True\n",
    ")\n",
    "\n",
    "# Randomly drop 20% of readings (simulate sensor failures)\n",
    "keep_indices = np.random.choice(len(all_timestamps), int(len(all_timestamps) * 0.8), replace=False)\n",
    "keep_indices.sort()\n",
    "\n",
    "iot_data = pl.DataFrame({\n",
    "    'timestamp': [all_timestamps[i] for i in keep_indices],\n",
    "    'temperature': np.random.normal(20, 3, len(keep_indices)),\n",
    "    'humidity': np.random.normal(60, 10, len(keep_indices)),\n",
    "    'pressure': np.random.normal(1013, 5, len(keep_indices))\n",
    "})\n",
    "\n",
    "print(f\"IoT sensor data: {len(iot_data)} readings (20% missing)\")\n",
    "print(iot_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Data with Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complete 5-minute grid\n",
    "complete_grid = pl.DataFrame({\n",
    "    'timestamp': all_timestamps\n",
    "})\n",
    "\n",
    "# Join and interpolate missing values\n",
    "complete_iot = complete_grid.join(\n",
    "    iot_data,\n",
    "    on='timestamp',\n",
    "    how='left'\n",
    ").with_columns([\n",
    "    pl.col('temperature').interpolate().alias('temperature'),\n",
    "    pl.col('humidity').interpolate().alias('humidity'),\n",
    "    pl.col('pressure').interpolate().alias('pressure')\n",
    "])\n",
    "\n",
    "print(f\"\\nComplete data with interpolation: {len(complete_iot)} readings\")\n",
    "print(complete_iot.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Anomalies Using Rolling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some anomalies\n",
    "anomaly_iot = complete_iot.clone()\n",
    "\n",
    "# Insert temperature spikes at random positions\n",
    "spike_indices = np.random.choice(len(anomaly_iot), 10, replace=False)\n",
    "for idx in spike_indices:\n",
    "    anomaly_iot[idx, 'temperature'] = anomaly_iot[idx, 'temperature'] + np.random.choice([15, -15])\n",
    "\n",
    "# Calculate rolling mean and std\n",
    "with_stats = anomaly_iot.with_columns([\n",
    "    pl.col('temperature').rolling_mean(window_size=12).alias('temp_mean_1h'),  # 12 * 5min = 1 hour\n",
    "    pl.col('temperature').rolling_std(window_size=12).alias('temp_std_1h')\n",
    "]).with_columns([\n",
    "    # Flag anomalies (> 3 standard deviations from mean)\n",
    "    (pl.col('temperature') - pl.col('temp_mean_1h')).abs() > (3 * pl.col('temp_std_1h')).alias('is_anomaly')\n",
    "])\n",
    "\n",
    "# Show anomalies\n",
    "anomalies = with_stats.filter(pl.col('is_anomaly'))\n",
    "print(f\"\\nDetected {len(anomalies)} anomalies:\")\n",
    "print(anomalies.select(['timestamp', 'temperature', 'temp_mean_1h', 'temp_std_1h']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Resolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze at different time scales\n",
    "resolutions = [\n",
    "    ('15m', '15 minutes'),\n",
    "    ('1h', '1 hour'),\n",
    "    ('6h', '6 hours'),\n",
    "    ('1d', '1 day')\n",
    "]\n",
    "\n",
    "for interval, label in resolutions:\n",
    "    aggregated = complete_iot.group_by_dynamic(\n",
    "        'timestamp',\n",
    "        every=interval\n",
    "    ).agg([\n",
    "        pl.col('temperature').mean().alias('temp_mean'),\n",
    "        pl.col('temperature').std().alias('temp_std'),\n",
    "        pl.col('humidity').mean().alias('humidity_mean')\n",
    "    ])\n",
    "    \n",
    "    print(f\"\\n{label} resolution: {len(aggregated)} records\")\n",
    "    print(aggregated.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Business Calendar Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create business data with timestamps\n",
    "business_df = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 31),\n",
    "        '1d',\n",
    "        eager=True\n",
    "    ),\n",
    "    'sales': np.random.uniform(1000, 5000, 31)\n",
    "})\n",
    "\n",
    "print(\"Daily business data:\")\n",
    "print(business_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Business Days Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only weekdays (Monday=0 to Friday=4)\n",
    "business_days_only = business_df.filter(\n",
    "    pl.col('timestamp').dt.weekday() < 5\n",
    ")\n",
    "\n",
    "print(f\"\\nBusiness days only: {len(business_days_only)} records\")\n",
    "print(business_days_only.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week-over-Week Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by week and calculate WoW growth\n",
    "weekly = business_df.group_by_dynamic(\n",
    "    'timestamp',\n",
    "    every='1w',\n",
    "    offset='1d'  # Start week on Monday\n",
    ").agg([\n",
    "    pl.col('sales').sum().alias('weekly_sales')\n",
    "]).with_columns([\n",
    "    # Week-over-week growth\n",
    "    ((pl.col('weekly_sales') / pl.col('weekly_sales').shift(1)) - 1).alias('wow_growth')\n",
    "])\n",
    "\n",
    "print(\"\\nWeekly sales with WoW growth:\")\n",
    "print(weekly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Month-to-Date Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative month-to-date sales\n",
    "mtd = business_df.with_columns([\n",
    "    pl.col('timestamp').dt.year().alias('year'),\n",
    "    pl.col('timestamp').dt.month().alias('month')\n",
    "]).with_columns([\n",
    "    pl.col('sales').cum_sum().over(['year', 'month']).alias('mtd_sales')\n",
    "])\n",
    "\n",
    "print(\"\\nMonth-to-date sales:\")\n",
    "print(mtd.select(['timestamp', 'sales', 'mtd_sales']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Performance Tips for Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Ensure Data is Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_by_dynamic and rolling operations assume sorted data\n",
    "# Always sort by timestamp first for best performance\n",
    "\n",
    "unsorted_df = pl.DataFrame({\n",
    "    'timestamp': [\n",
    "        datetime(2024, 1, 1, 12, 0),\n",
    "        datetime(2024, 1, 1, 8, 0),\n",
    "        datetime(2024, 1, 1, 10, 0),\n",
    "    ],\n",
    "    'value': [1, 2, 3]\n",
    "})\n",
    "\n",
    "# Sort before time-series operations\n",
    "sorted_df = unsorted_df.sort('timestamp')\n",
    "\n",
    "print(\"Always sort by timestamp first:\")\n",
    "print(sorted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use Lazy Evaluation for Large Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large time-series in lazy mode\n",
    "large_ts = pl.LazyFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 12, 31),\n",
    "        '1m',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': range(525600)  # Minutes in a year\n",
    "})\n",
    "\n",
    "# Chain operations in lazy mode\n",
    "result = (large_ts\n",
    "    .group_by_dynamic('timestamp', every='1d')\n",
    "    .agg([\n",
    "        pl.col('value').mean().alias('daily_mean')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        pl.col('daily_mean').rolling_mean(window_size=7).alias('weekly_ma')\n",
    "    ])\n",
    "    .collect()  # Execute all at once\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(result)} days of data efficiently\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Choose Appropriate Time Granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't use higher resolution than needed\n",
    "# If you need daily stats, aggregate to daily first, then calculate\n",
    "\n",
    "minute_data = pl.DataFrame({\n",
    "    'timestamp': pl.datetime_range(\n",
    "        datetime(2024, 1, 1),\n",
    "        datetime(2024, 1, 7),\n",
    "        '1m',\n",
    "        eager=True\n",
    "    ),\n",
    "    'value': range(10080)\n",
    "})\n",
    "\n",
    "# Aggregate to daily first for better performance\n",
    "daily_first = (\n",
    "    minute_data\n",
    "    .group_by_dynamic('timestamp', every='1d')\n",
    "    .agg(pl.col('value').mean().alias('daily_mean'))\n",
    "    # Then do further analysis on daily data\n",
    "    .with_columns([\n",
    "        pl.col('daily_mean').rolling_mean(window_size=3).alias('3d_ma')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"Efficient aggregation:\")\n",
    "print(daily_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Time-Series Features:\n",
    "\n",
    "**group_by_dynamic**:\n",
    "- `every`: Time window size\n",
    "- `period`: Window duration (can differ from `every` for overlapping windows)\n",
    "- `offset`: Shift window boundaries\n",
    "- `truncate`: Align to time boundaries\n",
    "- `include_boundaries`: Show window start/end\n",
    "- `by`: Additional grouping columns\n",
    "\n",
    "**group_by_rolling**:\n",
    "- `period`: Rolling window size\n",
    "- `offset`: Shift window position\n",
    "- `closed`: Window boundary inclusion ('left', 'right', 'both', 'none')\n",
    "- `by`: Additional grouping columns\n",
    "\n",
    "**ASOF Joins**:\n",
    "- `strategy`: 'backward', 'forward', or 'nearest'\n",
    "- `tolerance`: Maximum time difference allowed\n",
    "- `by`: Join on multiple keys\n",
    "\n",
    "### Best Practices:\n",
    "1. Always sort data by timestamp before time-series operations\n",
    "2. Use lazy evaluation for large datasets\n",
    "3. Choose appropriate time granularity\n",
    "4. Use ASOF joins for aligning time series with different timestamps\n",
    "5. Consider `period` vs `every` for overlapping windows\n",
    "6. Use interpolation for missing data points\n",
    "7. Apply rolling statistics for anomaly detection\n",
    "8. Leverage `by` parameter for multi-series analysis\n",
    "\n",
    "### Common Use Cases:\n",
    "- **Financial**: OHLC bars, technical indicators, VWAP\n",
    "- **IoT**: Sensor data aggregation, anomaly detection\n",
    "- **Business**: Sales analysis, WoW/MoM growth, business calendar operations\n",
    "- **Resampling**: Downsampling (reduce frequency), upsampling (increase frequency)\n",
    "- **Multi-resolution**: Analyze data at multiple time scales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
