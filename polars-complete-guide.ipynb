{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Guide to Polars for Pandas/PySpark Users\n",
    "\n",
    "This notebook provides a comprehensive introduction to Polars, covering everything from basics to advanced topics.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction & Setup](#1-introduction--setup)\n",
    "2. [Basic Data Structures](#2-basic-data-structures)\n",
    "3. [Creating DataFrames](#3-creating-dataframes)\n",
    "4. [Reading & Writing Data](#4-reading--writing-data)\n",
    "5. [Data Selection & Filtering](#5-data-selection--filtering)\n",
    "6. [Expressions - The Heart of Polars](#6-expressions---the-heart-of-polars)\n",
    "7. [Transformations & Column Operations](#7-transformations--column-operations)\n",
    "8. [Aggregations & GroupBy](#8-aggregations--groupby)\n",
    "9. [Joins & Concatenations](#9-joins--concatenations)\n",
    "10. [Lazy vs Eager Evaluation](#10-lazy-vs-eager-evaluation)\n",
    "11. [Time Series Operations](#11-time-series-operations)\n",
    "12. [String Operations](#12-string-operations)\n",
    "13. [Window Functions](#13-window-functions)\n",
    "14. [Performance Optimization](#14-performance-optimization)\n",
    "15. [Advanced Features](#15-advanced-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Setup\n",
    "\n",
    "### What is Polars?\n",
    "- **Fast**: Written in Rust, optimized for performance\n",
    "- **Efficient**: Uses Apache Arrow columnar format\n",
    "- **Expressive**: Rich expression API\n",
    "- **Lazy**: Built-in query optimization\n",
    "\n",
    "### Key Differences from Pandas/PySpark\n",
    "| Feature | Pandas | PySpark | Polars |\n",
    "|---------|--------|---------|--------|\n",
    "| Speed | Moderate | Fast (distributed) | Very Fast (single node) |\n",
    "| Memory | Copies data often | Distributed | Zero-copy views |\n",
    "| API Style | Method chaining | SQL-like | Expression-based |\n",
    "| Lazy Evaluation | No | Yes | Yes |\n",
    "| Parallelization | Limited | Distributed | Multi-threaded |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Polars (run this if not already installed)\n",
    "# !pip install polars\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Check version\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Data Structures\n",
    "\n",
    "Polars has two main data structures:\n",
    "- **Series**: 1D array (like pandas Series)\n",
    "- **DataFrame**: 2D table (like pandas DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Series\n",
    "s = pl.Series(\"numbers\", [1, 2, 3, 4, 5])\n",
    "print(\"Series:\")\n",
    "print(s)\n",
    "print(f\"\\nDtype: {s.dtype}\")\n",
    "print(f\"Length: {len(s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series with different dtypes\n",
    "int_series = pl.Series(\"integers\", [1, 2, 3], dtype=pl.Int64)\n",
    "float_series = pl.Series(\"floats\", [1.0, 2.5, 3.7], dtype=pl.Float64)\n",
    "str_series = pl.Series(\"strings\", [\"a\", \"b\", \"c\"], dtype=pl.Utf8)\n",
    "bool_series = pl.Series(\"booleans\", [True, False, True], dtype=pl.Boolean)\n",
    "\n",
    "print(\"Int Series:\", int_series.to_list())\n",
    "print(\"Float Series:\", float_series.to_list())\n",
    "print(\"String Series:\", str_series.to_list())\n",
    "print(\"Boolean Series:\", bool_series.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating DataFrames\n",
    "\n",
    "Multiple ways to create DataFrames in Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From dictionary\n",
    "df = pl.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [25, 30, 35, 40, 28],\n",
    "    \"city\": [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Berlin\"],\n",
    "    \"salary\": [70000, 80000, 90000, 95000, 75000]\n",
    "})\n",
    "\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: From list of dictionaries (row-oriented)\n",
    "data = [\n",
    "    {\"product\": \"A\", \"quantity\": 10, \"price\": 100},\n",
    "    {\"product\": \"B\", \"quantity\": 20, \"price\": 200},\n",
    "    {\"product\": \"C\", \"quantity\": 15, \"price\": 150},\n",
    "]\n",
    "\n",
    "df2 = pl.DataFrame(data)\n",
    "print(\"DataFrame from list of dicts:\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: From NumPy array\n",
    "arr = np.random.randn(5, 3)\n",
    "df3 = pl.DataFrame(arr, schema=[\"col1\", \"col2\", \"col3\"])\n",
    "print(\"DataFrame from NumPy:\")\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame info (similar to pandas)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumn names:\", df.columns)\n",
    "print(\"\\nDtypes:\", df.dtypes)\n",
    "print(\"\\nSchema:\")\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick statistics\n",
    "print(\"Describe:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading & Writing Data\n",
    "\n",
    "Polars supports multiple file formats with excellent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for I/O examples\n",
    "sample_df = pl.DataFrame({\n",
    "    \"id\": range(1, 1001),\n",
    "    \"name\": [f\"User_{i}\" for i in range(1, 1001)],\n",
    "    \"score\": np.random.randint(0, 100, 1000),\n",
    "    \"timestamp\": [datetime.now() - timedelta(days=i) for i in range(1000)]\n",
    "})\n",
    "\n",
    "print(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to CSV\n",
    "sample_df.write_csv(\"data.csv\")\n",
    "print(\"Written to CSV\")\n",
    "\n",
    "# Reading from CSV\n",
    "df_csv = pl.read_csv(\"data.csv\")\n",
    "print(\"\\nRead from CSV:\")\n",
    "print(df_csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet (recommended for performance)\n",
    "sample_df.write_parquet(\"data.parquet\")\n",
    "df_parquet = pl.read_parquet(\"data.parquet\")\n",
    "print(\"Read from Parquet:\")\n",
    "print(df_parquet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "sample_df.head(5).write_json(\"data.json\")\n",
    "df_json = pl.read_json(\"data.json\")\n",
    "print(\"Read from JSON:\")\n",
    "print(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy reading (for large files) - reads only when needed\n",
    "lazy_df = pl.scan_csv(\"data.csv\")\n",
    "print(\"Lazy DataFrame (not yet loaded):\")\n",
    "print(lazy_df)\n",
    "\n",
    "# Collect to execute\n",
    "result = lazy_df.head(3).collect()\n",
    "print(\"\\nCollected result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Selection & Filtering\n",
    "\n",
    "Polars uses expressions for powerful and efficient data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "df = pl.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\"],\n",
    "    \"age\": [25, 30, 35, 40, 28, 45],\n",
    "    \"city\": [\"New York\", \"London\", \"Paris\", \"Tokyo\", \"Berlin\", \"Sydney\"],\n",
    "    \"salary\": [70000, 80000, 90000, 95000, 75000, 100000],\n",
    "    \"department\": [\"IT\", \"HR\", \"IT\", \"Finance\", \"HR\", \"IT\"]\n",
    "})\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "print(\"Select single column:\")\n",
    "print(df.select(\"name\"))\n",
    "\n",
    "print(\"\\nSelect multiple columns:\")\n",
    "print(df.select([\"name\", \"age\", \"salary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select using expressions (pl.col)\n",
    "print(\"Select with expressions:\")\n",
    "print(df.select([\n",
    "    pl.col(\"name\"),\n",
    "    pl.col(\"age\"),\n",
    "    pl.col(\"salary\")\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by dtype\n",
    "print(\"Select numeric columns:\")\n",
    "print(df.select(pl.col(pl.Int64)))\n",
    "\n",
    "print(\"\\nSelect string columns:\")\n",
    "print(df.select(pl.col(pl.Utf8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows (similar to pandas query or SQL WHERE)\n",
    "print(\"Filter age > 30:\")\n",
    "print(df.filter(pl.col(\"age\") > 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions with & (and) | (or)\n",
    "print(\"Filter with multiple conditions (age > 30 AND salary > 80000):\")\n",
    "print(df.filter(\n",
    "    (pl.col(\"age\") > 30) & (pl.col(\"salary\") > 80000)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String filtering\n",
    "print(\"Filter department == 'IT':\")\n",
    "print(df.filter(pl.col(\"department\") == \"IT\"))\n",
    "\n",
    "print(\"\\nFilter city contains 'o':\")\n",
    "print(df.filter(pl.col(\"city\").str.contains(\"o\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isin (similar to pandas)\n",
    "print(\"Filter names in list:\")\n",
    "print(df.filter(pl.col(\"name\").is_in([\"Alice\", \"Bob\", \"Charlie\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head, tail, sample\n",
    "print(\"First 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\nLast 2 rows:\")\n",
    "print(df.tail(2))\n",
    "\n",
    "print(\"\\nRandom sample (2 rows):\")\n",
    "print(df.sample(n=2, seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Expressions - The Heart of Polars\n",
    "\n",
    "Expressions are what make Polars powerful and fast. They are:\n",
    "- **Composable**: Can be chained together\n",
    "- **Parallelizable**: Automatically run in parallel\n",
    "- **Optimizable**: Query optimizer improves performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic expression\n",
    "print(\"Double the salary:\")\n",
    "print(df.select([\n",
    "    pl.col(\"name\"),\n",
    "    (pl.col(\"salary\") * 2).alias(\"doubled_salary\")\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple operations in one select\n",
    "print(\"Multiple expressions:\")\n",
    "print(df.select([\n",
    "    pl.col(\"name\"),\n",
    "    pl.col(\"age\"),\n",
    "    (pl.col(\"salary\") / 1000).alias(\"salary_k\"),\n",
    "    (pl.col(\"age\") > 30).alias(\"is_senior\")\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_columns (add/modify columns without selecting)\n",
    "print(\"Add new columns:\")\n",
    "result = df.with_columns([\n",
    "    (pl.col(\"salary\") * 1.1).alias(\"salary_after_raise\"),\n",
    "    (pl.col(\"age\") + 1).alias(\"age_next_year\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional expressions (when-then-otherwise)\n",
    "print(\"Conditional column:\")\n",
    "result = df.with_columns([\n",
    "    pl.when(pl.col(\"age\") < 30)\n",
    "      .then(pl.lit(\"Young\"))\n",
    "      .when(pl.col(\"age\") < 40)\n",
    "      .then(pl.lit(\"Middle\"))\n",
    "      .otherwise(pl.lit(\"Senior\"))\n",
    "      .alias(\"age_group\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression aliases and chaining\n",
    "print(\"Chained expressions:\")\n",
    "result = df.select([\n",
    "    pl.col(\"name\").str.to_uppercase().alias(\"name_upper\"),\n",
    "    pl.col(\"salary\").log10().round(2).alias(\"log_salary\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformations & Column Operations\n",
    "\n",
    "Common data transformation operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "print(\"Sort by age (descending):\")\n",
    "print(df.sort(\"age\", descending=True))\n",
    "\n",
    "print(\"\\nSort by multiple columns:\")\n",
    "print(df.sort([\"department\", \"salary\"], descending=[False, True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "print(\"Rename columns:\")\n",
    "renamed = df.rename({\"name\": \"employee_name\", \"salary\": \"annual_salary\"})\n",
    "print(renamed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "print(\"Drop columns:\")\n",
    "print(df.drop([\"city\", \"department\"]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast dtypes\n",
    "print(\"Cast age to float:\")\n",
    "result = df.with_columns(pl.col(\"age\").cast(pl.Float64))\n",
    "print(result.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null handling\n",
    "df_with_nulls = pl.DataFrame({\n",
    "    \"a\": [1, 2, None, 4, None],\n",
    "    \"b\": [\"x\", None, \"y\", \"z\", None]\n",
    "})\n",
    "\n",
    "print(\"DataFrame with nulls:\")\n",
    "print(df_with_nulls)\n",
    "\n",
    "print(\"\\nFill nulls:\")\n",
    "print(df_with_nulls.fill_null(strategy=\"forward\"))\n",
    "\n",
    "print(\"\\nFill with specific value:\")\n",
    "print(df_with_nulls.fill_null(0))\n",
    "\n",
    "print(\"\\nDrop nulls:\")\n",
    "print(df_with_nulls.drop_nulls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique and duplicates\n",
    "print(\"Unique values in department:\")\n",
    "print(df.select(pl.col(\"department\").unique()))\n",
    "\n",
    "print(\"\\nCount unique values:\")\n",
    "print(df.select(pl.col(\"department\").n_unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Aggregations & GroupBy\n",
    "\n",
    "Powerful aggregation capabilities, similar to pandas groupby but more expressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregations\n",
    "print(\"Mean salary:\")\n",
    "print(df.select(pl.col(\"salary\").mean()))\n",
    "\n",
    "print(\"\\nMultiple aggregations:\")\n",
    "print(df.select([\n",
    "    pl.col(\"salary\").mean().alias(\"mean_salary\"),\n",
    "    pl.col(\"salary\").median().alias(\"median_salary\"),\n",
    "    pl.col(\"salary\").std().alias(\"std_salary\"),\n",
    "    pl.col(\"age\").min().alias(\"min_age\"),\n",
    "    pl.col(\"age\").max().alias(\"max_age\")\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy - basic\n",
    "print(\"Group by department:\")\n",
    "print(df.group_by(\"department\").agg([\n",
    "    pl.col(\"salary\").mean().alias(\"avg_salary\"),\n",
    "    pl.col(\"age\").mean().alias(\"avg_age\"),\n",
    "    pl.count().alias(\"count\")\n",
    "]).sort(\"department\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy - multiple aggregations per column\n",
    "print(\"Multiple aggregations:\")\n",
    "print(df.group_by(\"department\").agg([\n",
    "    pl.col(\"salary\").min().alias(\"min_salary\"),\n",
    "    pl.col(\"salary\").max().alias(\"max_salary\"),\n",
    "    pl.col(\"salary\").mean().alias(\"avg_salary\"),\n",
    "    pl.col(\"name\").count().alias(\"employee_count\")\n",
    "]).sort(\"department\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy with multiple keys\n",
    "df_extended = df.with_columns(\n",
    "    pl.when(pl.col(\"age\") < 35)\n",
    "      .then(pl.lit(\"Young\"))\n",
    "      .otherwise(pl.lit(\"Senior\"))\n",
    "      .alias(\"age_category\")\n",
    ")\n",
    "\n",
    "print(\"Group by multiple columns:\")\n",
    "print(df_extended.group_by([\"department\", \"age_category\"]).agg([\n",
    "    pl.col(\"salary\").mean().alias(\"avg_salary\"),\n",
    "    pl.count().alias(\"count\")\n",
    "]).sort([\"department\", \"age_category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced aggregations\n",
    "print(\"List aggregation (collect names per department):\")\n",
    "print(df.group_by(\"department\").agg([\n",
    "    pl.col(\"name\").alias(\"employees\"),\n",
    "    pl.col(\"salary\").sum().alias(\"total_salary\")\n",
    "]).sort(\"department\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiles and percentiles\n",
    "print(\"Salary percentiles:\")\n",
    "print(df.select([\n",
    "    pl.col(\"salary\").quantile(0.25).alias(\"p25\"),\n",
    "    pl.col(\"salary\").quantile(0.50).alias(\"p50\"),\n",
    "    pl.col(\"salary\").quantile(0.75).alias(\"p75\"),\n",
    "    pl.col(\"salary\").quantile(0.90).alias(\"p90\")\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Joins & Concatenations\n",
    "\n",
    "Combining DataFrames - similar to SQL joins and pandas merge/concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames for joining\n",
    "employees = pl.DataFrame({\n",
    "    \"emp_id\": [1, 2, 3, 4, 5],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"dept_id\": [10, 20, 10, 30, 20]\n",
    "})\n",
    "\n",
    "departments = pl.DataFrame({\n",
    "    \"dept_id\": [10, 20, 30, 40],\n",
    "    \"dept_name\": [\"IT\", \"HR\", \"Finance\", \"Marketing\"]\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "print(\"Inner join:\")\n",
    "print(employees.join(departments, on=\"dept_id\", how=\"inner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join\n",
    "print(\"Left join:\")\n",
    "print(employees.join(departments, on=\"dept_id\", how=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join\n",
    "print(\"Outer join:\")\n",
    "print(employees.join(departments, on=\"dept_id\", how=\"outer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with different column names\n",
    "salaries = pl.DataFrame({\n",
    "    \"employee_id\": [1, 2, 3, 4, 5],\n",
    "    \"salary\": [70000, 80000, 90000, 95000, 75000]\n",
    "})\n",
    "\n",
    "print(\"Join on different column names:\")\n",
    "print(employees.join(salaries, left_on=\"emp_id\", right_on=\"employee_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation - vertical (like SQL UNION or pandas concat axis=0)\n",
    "df1 = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n",
    "df2 = pl.DataFrame({\"a\": [5, 6], \"b\": [7, 8]})\n",
    "\n",
    "print(\"Vertical concatenation:\")\n",
    "print(pl.concat([df1, df2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation - horizontal (like pandas concat axis=1)\n",
    "df3 = pl.DataFrame({\"c\": [9, 10]})\n",
    "\n",
    "print(\"Horizontal concatenation:\")\n",
    "print(pl.concat([df1, df3], how=\"horizontal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Lazy vs Eager Evaluation\n",
    "\n",
    "One of Polars' most powerful features - lazy evaluation allows query optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager execution (default)\n",
    "print(\"Eager execution:\")\n",
    "result_eager = (\n",
    "    df.filter(pl.col(\"age\") > 30)\n",
    "      .select([\"name\", \"salary\"])\n",
    "      .sort(\"salary\", descending=True)\n",
    ")\n",
    "print(result_eager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy execution - convert to lazy\n",
    "print(\"Lazy execution (not yet computed):\")\n",
    "lazy_query = (\n",
    "    df.lazy()\n",
    "      .filter(pl.col(\"age\") > 30)\n",
    "      .select([\"name\", \"salary\"])\n",
    "      .sort(\"salary\", descending=True)\n",
    ")\n",
    "print(lazy_query)\n",
    "print(\"\\nQuery plan:\")\n",
    "print(lazy_query.explain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute lazy query with collect()\n",
    "print(\"Collected result:\")\n",
    "result_lazy = lazy_query.collect()\n",
    "print(result_lazy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing optimization benefits\n",
    "# Polars will optimize this to only read necessary columns\n",
    "lazy_optimized = (\n",
    "    pl.scan_csv(\"data.csv\")\n",
    "      .select([\"name\", \"score\"])  # Only these columns will be read from CSV\n",
    "      .filter(pl.col(\"score\") > 50)\n",
    "      .head(10)\n",
    ")\n",
    "\n",
    "print(\"Optimized query plan:\")\n",
    "print(lazy_optimized.explain())\n",
    "\n",
    "# Execute\n",
    "print(\"\\nResult:\")\n",
    "print(lazy_optimized.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Time Series Operations\n",
    "\n",
    "Working with dates and times in Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series data\n",
    "from datetime import date\n",
    "\n",
    "ts_df = pl.DataFrame({\n",
    "    \"date\": pl.date_range(\n",
    "        date(2024, 1, 1),\n",
    "        date(2024, 12, 31),\n",
    "        interval=\"1d\",\n",
    "        eager=True\n",
    "    ),\n",
    "    \"value\": np.random.randn(366).cumsum()\n",
    "})\n",
    "\n",
    "print(\"Time series data:\")\n",
    "print(ts_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "print(\"Extract date components:\")\n",
    "result = ts_df.with_columns([\n",
    "    pl.col(\"date\").dt.year().alias(\"year\"),\n",
    "    pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "    pl.col(\"date\").dt.day().alias(\"day\"),\n",
    "    pl.col(\"date\").dt.weekday().alias(\"weekday\"),\n",
    "    pl.col(\"date\").dt.quarter().alias(\"quarter\")\n",
    "]).head(10)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime arithmetic\n",
    "print(\"Add days to date:\")\n",
    "result = ts_df.with_columns(\n",
    "    (pl.col(\"date\") + pl.duration(days=7)).alias(\"date_plus_week\")\n",
    ").head(5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample and aggregate (like pandas resample)\n",
    "print(\"Monthly aggregation:\")\n",
    "monthly = (\n",
    "    ts_df.group_by_dynamic(\"date\", every=\"1mo\")\n",
    "         .agg([\n",
    "             pl.col(\"value\").mean().alias(\"avg_value\"),\n",
    "             pl.col(\"value\").min().alias(\"min_value\"),\n",
    "             pl.col(\"value\").max().alias(\"max_value\")\n",
    "         ])\n",
    ")\n",
    "print(monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window operations\n",
    "print(\"7-day rolling average:\")\n",
    "result = ts_df.with_columns(\n",
    "    pl.col(\"value\").rolling_mean(window_size=7).alias(\"rolling_avg_7d\")\n",
    ").head(20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. String Operations\n",
    "\n",
    "String manipulation in Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create string data\n",
    "str_df = pl.DataFrame({\n",
    "    \"text\": [\n",
    "        \"hello world\",\n",
    "        \"POLARS is FAST\",\n",
    "        \"  pandas  \",\n",
    "        \"data-science-2024\",\n",
    "        \"user@example.com\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"String data:\")\n",
    "print(str_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String methods\n",
    "print(\"String transformations:\")\n",
    "result = str_df.with_columns([\n",
    "    pl.col(\"text\").str.to_uppercase().alias(\"upper\"),\n",
    "    pl.col(\"text\").str.to_lowercase().alias(\"lower\"),\n",
    "    pl.col(\"text\").str.strip_chars().alias(\"stripped\"),\n",
    "    pl.col(\"text\").str.len_chars().alias(\"length\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String contains, starts_with, ends_with\n",
    "print(\"String matching:\")\n",
    "result = str_df.with_columns([\n",
    "    pl.col(\"text\").str.contains(\"a\").alias(\"contains_a\"),\n",
    "    pl.col(\"text\").str.starts_with(\"h\").alias(\"starts_h\"),\n",
    "    pl.col(\"text\").str.ends_with(\"m\").alias(\"ends_m\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String replace and split\n",
    "print(\"Replace:\")\n",
    "print(str_df.with_columns(\n",
    "    pl.col(\"text\").str.replace(\"-\", \"_\").alias(\"replaced\")\n",
    "))\n",
    "\n",
    "print(\"\\nSplit:\")\n",
    "print(str_df.with_columns(\n",
    "    pl.col(\"text\").str.split(\"-\").alias(\"split\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract with regex\n",
    "print(\"Extract email domain:\")\n",
    "email_df = pl.DataFrame({\"email\": [\"user@example.com\", \"test@domain.org\"]})\n",
    "result = email_df.with_columns(\n",
    "    pl.col(\"email\").str.extract(r\"@(.+)\", group_index=1).alias(\"domain\")\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Window Functions\n",
    "\n",
    "Powerful window operations (like SQL window functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for window functions\n",
    "sales_df = pl.DataFrame({\n",
    "    \"date\": pl.date_range(date(2024, 1, 1), date(2024, 1, 10), interval=\"1d\", eager=True),\n",
    "    \"product\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\", \"A\", \"B\"],\n",
    "    \"sales\": [100, 150, 120, 160, 110, 140, 130, 170, 115, 155]\n",
    "})\n",
    "\n",
    "print(\"Sales data:\")\n",
    "print(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window aggregation with over()\n",
    "print(\"Average sales per product (window function):\")\n",
    "result = sales_df.with_columns([\n",
    "    pl.col(\"sales\").mean().over(\"product\").alias(\"avg_sales_per_product\"),\n",
    "    pl.col(\"sales\").sum().over(\"product\").alias(\"total_sales_per_product\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking within groups\n",
    "print(\"Rank sales within each product:\")\n",
    "result = sales_df.with_columns([\n",
    "    pl.col(\"sales\").rank(method=\"ordinal\").over(\"product\").alias(\"rank\")\n",
    "]).sort([\"product\", \"rank\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum within groups\n",
    "print(\"Cumulative sales per product:\")\n",
    "result = sales_df.with_columns(\n",
    "    pl.col(\"sales\").cum_sum().over(\"product\").alias(\"cumulative_sales\")\n",
    ").sort([\"product\", \"date\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift/lag operations\n",
    "print(\"Previous day sales (lag):\")\n",
    "result = sales_df.with_columns([\n",
    "    pl.col(\"sales\").shift(1).over(\"product\").alias(\"prev_sales\"),\n",
    "    (pl.col(\"sales\") - pl.col(\"sales\").shift(1).over(\"product\")).alias(\"sales_change\")\n",
    "]).sort([\"product\", \"date\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Performance Optimization\n",
    "\n",
    "Tips and tricks for maximizing Polars performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use lazy evaluation for large datasets\n",
    "print(\"Use scan_* methods for lazy reading:\")\n",
    "lazy_query = (\n",
    "    pl.scan_csv(\"data.csv\")\n",
    "      .filter(pl.col(\"score\") > 50)\n",
    "      .select([\"name\", \"score\"])\n",
    "      .head(5)\n",
    ")\n",
    "print(lazy_query.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use appropriate data types (smaller = faster)\n",
    "print(\"Downcast to smaller dtypes when possible:\")\n",
    "df_optimized = pl.DataFrame({\n",
    "    \"id\": pl.Series([1, 2, 3], dtype=pl.UInt32),  # Instead of Int64\n",
    "    \"value\": pl.Series([1.0, 2.0, 3.0], dtype=pl.Float32)  # Instead of Float64\n",
    "})\n",
    "print(df_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prefer Parquet over CSV for I/O\n",
    "import time\n",
    "\n",
    "# Write\n",
    "start = time.time()\n",
    "sample_df.write_parquet(\"test.parquet\")\n",
    "parquet_write_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "sample_df.write_csv(\"test.csv\")\n",
    "csv_write_time = time.time() - start\n",
    "\n",
    "print(f\"Parquet write time: {parquet_write_time:.4f}s\")\n",
    "print(f\"CSV write time: {csv_write_time:.4f}s\")\n",
    "\n",
    "# Read\n",
    "start = time.time()\n",
    "_ = pl.read_parquet(\"test.parquet\")\n",
    "parquet_read_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = pl.read_csv(\"test.csv\")\n",
    "csv_read_time = time.time() - start\n",
    "\n",
    "print(f\"Parquet read time: {parquet_read_time:.4f}s\")\n",
    "print(f\"CSV read time: {csv_read_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Use expression chaining instead of multiple operations\n",
    "print(\"Chain operations efficiently:\")\n",
    "\n",
    "# Less efficient: multiple passes\n",
    "result1 = df.with_columns((pl.col(\"salary\") * 1.1).alias(\"new_salary\"))\n",
    "result1 = result1.with_columns((pl.col(\"age\") + 1).alias(\"new_age\"))\n",
    "\n",
    "# More efficient: single pass\n",
    "result2 = df.with_columns([\n",
    "    (pl.col(\"salary\") * 1.1).alias(\"new_salary\"),\n",
    "    (pl.col(\"age\") + 1).alias(\"new_age\")\n",
    "])\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Use streaming for very large datasets\n",
    "print(\"Streaming execution for large data:\")\n",
    "lazy_query = (\n",
    "    pl.scan_csv(\"data.csv\")\n",
    "      .filter(pl.col(\"score\") > 50)\n",
    "      .group_by(\"name\")\n",
    "      .agg(pl.col(\"score\").mean())\n",
    ")\n",
    "\n",
    "# Collect with streaming (processes data in chunks)\n",
    "result = lazy_query.collect(streaming=True)\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Advanced Features\n",
    "\n",
    "Advanced Polars capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Explode (unnest lists)\n",
    "df_lists = pl.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\"],\n",
    "    \"scores\": [[85, 90, 88], [92, 87, 95]]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_lists)\n",
    "\n",
    "print(\"\\nExploded:\")\n",
    "print(df_lists.explode(\"scores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pivot (wide format)\n",
    "pivot_df = pl.DataFrame({\n",
    "    \"date\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-02\", \"2024-01-02\"],\n",
    "    \"product\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"sales\": [100, 150, 120, 160]\n",
    "})\n",
    "\n",
    "print(\"Original:\")\n",
    "print(pivot_df)\n",
    "\n",
    "print(\"\\nPivoted:\")\n",
    "print(pivot_df.pivot(values=\"sales\", index=\"date\", columns=\"product\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Melt (long format)\n",
    "wide_df = pl.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\"],\n",
    "    \"math\": [85, 90],\n",
    "    \"science\": [88, 92],\n",
    "    \"english\": [90, 87]\n",
    "})\n",
    "\n",
    "print(\"Wide format:\")\n",
    "print(wide_df)\n",
    "\n",
    "print(\"\\nMelted (long format):\")\n",
    "print(wide_df.melt(id_vars=\"name\", variable_name=\"subject\", value_name=\"score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Apply custom functions with map_elements (use sparingly - slower than expressions)\n",
    "def custom_function(x):\n",
    "    return x * 2 + 10\n",
    "\n",
    "print(\"Apply custom function:\")\n",
    "result = df.select([\n",
    "    pl.col(\"name\"),\n",
    "    pl.col(\"age\").map_elements(custom_function, return_dtype=pl.Int64).alias(\"custom\")\n",
    "])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. SQL interface\n",
    "# Register DataFrame in SQL context\n",
    "ctx = pl.SQLContext()\n",
    "ctx.register(\"employees\", df)\n",
    "\n",
    "print(\"Query with SQL:\")\n",
    "result = ctx.execute(\"\"\"\n",
    "    SELECT name, salary, department\n",
    "    FROM employees\n",
    "    WHERE salary > 80000\n",
    "    ORDER BY salary DESC\n",
    "\"\"\").collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Categorical data for memory efficiency\n",
    "cat_df = pl.DataFrame({\n",
    "    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\"] * 1000\n",
    "})\n",
    "\n",
    "print(\"String dtype memory:\")\n",
    "print(f\"{cat_df.estimated_size('mb'):.4f} MB\")\n",
    "\n",
    "# Convert to categorical\n",
    "cat_df_opt = cat_df.with_columns(\n",
    "    pl.col(\"category\").cast(pl.Categorical)\n",
    ")\n",
    "\n",
    "print(\"\\nCategorical dtype memory:\")\n",
    "print(f\"{cat_df_opt.estimated_size('mb'):.4f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Struct columns (nested data)\n",
    "struct_df = pl.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"address\": [\n",
    "        {\"city\": \"NYC\", \"zip\": \"10001\"},\n",
    "        {\"city\": \"LA\", \"zip\": \"90001\"},\n",
    "        {\"city\": \"SF\", \"zip\": \"94101\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Struct column:\")\n",
    "print(struct_df)\n",
    "\n",
    "print(\"\\nAccess struct fields:\")\n",
    "print(struct_df.with_columns([\n",
    "    pl.col(\"address\").struct.field(\"city\").alias(\"city\"),\n",
    "    pl.col(\"address\").struct.field(\"zip\").alias(\"zip\")\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### When to Use Polars vs Pandas/PySpark:\n",
    "\n",
    "**Use Polars when:**\n",
    "- You need maximum performance on a single machine\n",
    "- Your data fits in memory (or can be streamed)\n",
    "- You want better memory efficiency\n",
    "- You need lazy evaluation and query optimization\n",
    "\n",
    "**Use Pandas when:**\n",
    "- You need maximum ecosystem compatibility\n",
    "- Your data is small and performance isn't critical\n",
    "- You're working with legacy code\n",
    "\n",
    "**Use PySpark when:**\n",
    "- Your data is too large for a single machine\n",
    "- You need distributed computing\n",
    "- You already have a Spark cluster\n",
    "\n",
    "### Key Polars Concepts:\n",
    "1. **Expressions**: The core abstraction for data manipulation\n",
    "2. **Lazy Evaluation**: Use `.lazy()` and `scan_*` for query optimization\n",
    "3. **Arrow Backend**: Zero-copy operations for speed\n",
    "4. **Parallelization**: Automatic multi-threading\n",
    "5. **Type System**: Strong typing helps catch errors early\n",
    "\n",
    "### Performance Tips:\n",
    "- Use Parquet for storage\n",
    "- Chain operations in a single expression\n",
    "- Use lazy evaluation for large datasets\n",
    "- Prefer expressions over custom functions\n",
    "- Use appropriate dtypes (smaller when possible)\n",
    "- Use streaming for very large data\n",
    "\n",
    "### Migration from Pandas:\n",
    "- `df[df['col'] > 5]` → `df.filter(pl.col('col') > 5)`\n",
    "- `df['new'] = df['old'] * 2` → `df.with_columns((pl.col('old') * 2).alias('new'))`\n",
    "- `df.groupby('col').agg({'x': 'mean'})` → `df.group_by('col').agg(pl.col('x').mean())`\n",
    "- `df.merge(other)` → `df.join(other)`\n",
    "\n",
    "Happy data wrangling with Polars!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. Load the data.csv file and find all records where score > 75\n",
    "2. Calculate the average score per day of the week\n",
    "3. Create a new column that categorizes scores: Low (0-33), Medium (34-66), High (67-100)\n",
    "4. Find the top 10 users by score using window functions\n",
    "5. Write a lazy query that filters, groups, and aggregates the data, then optimize it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
